{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parsing ETCSL\n",
    "## 1. Introduction\n",
    "\n",
    "The Electronic Text Corpus of Sumerian Literature ([ETCSL]; 1998-2006) provides editions and translations of 394 Sumerian literary compositions. Goal of this Notebook is to format the [ETCSL] data in such a way that the (lemmatized) texts are made available for computational text analysis. The [ETCSL] lemmatizations are made compatible with [ORACC] standards (see [ePSD2](http://oracc.org/epsd2/sux)), so that [ETCSL] and [ORACC] data can be mixed and matched for text analysis purposes.\n",
    "\n",
    "The output of this notebook is a `csv` file of lemmatized words (one record is one word) with information about composition, line number, etc.\n",
    "\n",
    "The original [ETCSL] files in `TEI XML` may be downloaded from the [Oxford Text Archive](http://ota.ox.ac.uk/desc/2518) under a Creative Commons Attribution non-Commercial Share-Alike ([BY-NC-SA 3.0](http://creativecommons.org/licenses/by-nc-sa/3.0/)) license.\n",
    "\n",
    "The editors and copyright holders of [ETCSL] are: Jeremy Black, Graham Cunningham, Jarle Ebeling, Esther Flückiger-Hawker, Eleanor Robson, Jon Taylor, and Gábor Zólyomi.\n",
    "\n",
    "The [manual](http://etcsl.orinst.ox.ac.uk/edition2/etcslmanual.php) of the [ETCSL] project explains in full detail the editorial principles and technical details. \n",
    "\n",
    "[ETCSL]: http://etcsl.orinst.ox.ac.uk\n",
    "[ORACC]: http://oracc.org\n",
    "[COMPASS]: http://build-oracc.museum.upenn.edu/compass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Authorship and License\n",
    "This notebook was written by Niek Veldhuis in the context of the [Computational Assyriology](http://oracc.org/compass) project. Fuller explanation of the structure of this notebook and the techniques employed may be found there (Chapter 2.2: Data Acquision: ETCSL).\n",
    "\n",
    "This notebook and the associated files may be freely copied; the user is encouraged to adapt the code to suit her research goals (Creative Commons 0). \n",
    "<p xmlns:dct=\"http://purl.org/dc/terms/\" xmlns:vcard=\"http://www.w3.org/2001/vcard-rdf/3.0#\">\n",
    "  <a rel=\"license\"\n",
    "     href=\"http://creativecommons.org/publicdomain/zero/1.0/\">\n",
    "    <img src=\"http://i.creativecommons.org/p/zero/1.0/88x31.png\" style=\"border-style: none;\" alt=\"CC0\" />\n",
    "  </a>\n",
    " </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 XML, Xpath, and lxml\n",
    "The [ETCSL] files as distributed by the [Oxford Text Archive](http://ota.ox.ac.uk/desc/2518) are encoded in the `TEI` (Text Encoding Initiative) dialect of `XML` (Extensible Markup Language). \n",
    "\n",
    "The `XML`tree is parsed in `Xpath`, a language that defines ways in which one can move through an `XML` file and identify particular nodes, sub-nodes, attributes, and elements. The module `etree`of the library [`lxml`](https://lxml.de/) provides a full implementation of `Xpath 1.0` and is largely compatible with more widely-used libraries such as `ElementTree` and its counterpart `cElementTree`.\n",
    "\n",
    "[ETCSL]: http://etcsl.orinst.ox.ac.uk\n",
    "[ORACC]: http://oracc.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Input and Output\n",
    "\n",
    "This scraper expects the following files and directories:\n",
    "\n",
    "1. Directory `etcsl/transliterations/`  \n",
    "   This directory should contain the [ETCSL] `TEI XML` transliteration files.  \n",
    "2. Directory `Equivalencies`  \n",
    "   `equivalencies.json`: a set of equivalence dictionaries used at various places in the parser.  \n",
    "\n",
    "The output is saved in the `Output` directory as a single `.csv` file.\n",
    "\n",
    "[ETCSL]: http://etcsl.orinst.ox.ac.uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up\n",
    "### 2.1 Load Libraries\n",
    "Before running this cell you may need to install the packages `lxml` (for parsing `XML`) and  `tqdm` (for displaying a progress bar). For proper installation of libraries for a Jupyter Notebook see [install_packages.ipynb](../1_preliminaries/install_packages.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from lxml import etree\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load Equivalencies \n",
    "The file `equivalencies.json` contains a number of dictionaries that will be used to search and replace at various places in this notebook. The dictionaries are:\n",
    "- `suxwords`: Sumerian words (Citation Form, GuideWord, and Part of Speech) in [ETCSL] format and their [ORACC] counterparts.\n",
    "- `emesalwords`: idem for Emesal words\n",
    "- `propernouns`: idem for proper nouns\n",
    "- `ampersands`: HTML entities (such as `&aacute;`) and their Unicode counterparts (`á`; see section 3).\n",
    "- `versions`: [ETCSL] version names and (abbreviated) equivalents\n",
    "\n",
    "The `equivalencies.json` file is loaded with the `json` library. The dictionaries `suxwords`, `emesalwords` and `propernouns` (which, together, contain the entire [ETCSL] vocabulary) are concatenated into a single dictionary.\n",
    "\n",
    "[ETCSL]: http://etcsl.orinst.ox.ac.uk\n",
    "[ORACC]: http://oracc.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"equivalencies/equivalencies.json\", encoding=\"utf-8\") as f:\n",
    "    eq = json.load(f)\n",
    "equiv = eq[\"suxwords\"]\n",
    "equiv.update(eq[\"emesalwords\"])\n",
    "equiv.update(eq[\"propernouns\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing: HTML-entities\n",
    "The [ETCSL] `TEI XML` files are written in ASCII and represent special characters (such as š or ī) by a sequence of characters that begins with & and ends with ; (e.g. `&c;` represents `š`). The `lxml` library cannot deal with these entities and thus we have to replace them with the actual (Unicode) character that they represent before feeding the data to `etree` module.\n",
    "\n",
    "The function `ampersands()` uses the dictionary `ampersands` for a search-replace action. The dictionary `ampersands` is included in the file `equivalencies.json`, which was loaded above (section 2).\n",
    "\n",
    "The function `ampersands()` is called in `parsetext()` (see section 11) before the `etree` is built. The regular expression `amp` captures all so-called HTML entities (beginning with a '&' and ending with a ';'). The regex is compiled in the main process.\n",
    "\n",
    "[ETCSL]: http://etcsl.orinst.ox.ac.uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ampersands(x):    \n",
    "    x = re.sub(amp, lambda m: \n",
    "               eq[\"ampersands\"].get(m.group(0), m.group(0)), x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Marking 'Secondary Text' and/or 'Additional Text'\n",
    "\n",
    "The [ETCSL] web pages include variants, indicated as '(1 ms. has instead: )', with the variant text enclosed in curly brackets. Two types of variants are distinguished: 'additional text' and 'secondary text'. 'Additional text' refers to a line that appears in a minority of sources (often in only one) in addition to the majority text. 'Secondary text' refers to variant words or variant lines that are found in a minority of sources, replacing the text of the majority sources. The function `mark_extra()` marks each word of 'secondary text' or 'additional text' by adding the attribute `status` with the value \"additional\" or \"secondary\". \n",
    "\n",
    "The function `mark_extra()` is called twice by the function `parsetext()` (see below, section 11), once for \"additional\" and once for \"secondary\" text, indicated by the `which` argument. \n",
    "\n",
    "[ETCSL]: http://etcsl.orinst.ox.ac.uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_extra(tree, which):\n",
    "    extra = tree.xpath(f'//w[preceding::addSpan[@type=\"{which}\"]/@to = following::anchor/@id]')\n",
    "    for word in extra:\n",
    "        word.attrib[\"status\"] = which\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transliteration Conventions\n",
    "\n",
    "Transliteration of Sumerian text in [ETCSL] `TEI XML` files uses **c** for **š**, **j** for **ŋ** and regular numbers for index numbers. The function `tounicode()` replaces each of those. For example **cag4** is replaced by **šag₄**. This function is called in the function `getword()` to format `Citation Forms` and `Forms` (transliteration). The function `tounicode()` uses the translation tables `transind` (for index numbers) and `transcj` (for c and j), defined in the main process. The `translate()` function replaces individual characters from a string, according to the table.\n",
    "\n",
    "In order to replace regular numbers with index numbers the function uses a [regular expression](https://www.regular-expressions.info/) to select only those single or double digit numbers that are preceded a letter (leaving alone the \"7\" in 7-ta-am3). The regex `ind` is compiled in the main process.\n",
    "\n",
    "[ETCSL]: http://etcsl.orinst.ox.ac.uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tounicode(x):\n",
    "    x = re.sub(ind, lambda m: m.group().translate(transind), x)\n",
    "    x = x.translate(transcj)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Replace [ETCSL] by [ORACC] Lemmatization\n",
    "For every word, once `cf` (Citation Form), `gw` (Guide Word), and `pos` (Part of Speech) have been pulled out of the [ETCSL] `XML` file, they are combined into a lemma and run through the etcsl/oracc equivalence lists to match it with the [ORACC]/[ePSD2](http://oracc.org/epsd2) standards. The equivalence lists are stored in the file `equivalencies.json`, which was loaded above (section 2).\n",
    "\n",
    "The function `etcsl_to_oracc()` is called by the function `getword()`.\n",
    "\n",
    "\n",
    "[ETCSL]: http://etcsl.orinst.ox.ac.uk\n",
    "[ORACC]: http://oracc.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etcsl_to_oracc(word):\n",
    "    lemma = f\"{word['cf']}[{word['gw']}]{word['pos']}\"\n",
    "    if lemma in equiv:\n",
    "        word['cf'] = equiv[lemma][\"cf\"]\n",
    "        word[\"gw\"] = equiv[lemma][\"gw\"]\n",
    "        word[\"pos\"] = equiv[lemma][\"pos\"]\n",
    "        alltexts.append(word)\n",
    "        if \"cf2\" in equiv[lemma]: # if an ETCSL word is replaced by two ORACC words\n",
    "            word2 = word.copy()\n",
    "            word2[\"cf\"] = equiv[lemma][\"cf2\"]\n",
    "            word2[\"gw\"] = equiv[lemma][\"gw2\"]\n",
    "            word2[\"pos\"] = equiv[lemma][\"pos2\"]\n",
    "            alltexts.append(word2)\n",
    "    else: # word not found in equiv\n",
    "        alltexts.append(word)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Formatting Words\n",
    "\n",
    "A word in the [ETCSL] files is represented by a `<w>` node in the `XML` tree with a number of attributes that identify the `form` (transliteration), `citation form`, `guide word`, `part of speech`, etc. The function `getword()` formats the word as closely as possible to the [ORACC] conventions. Three different types of words are treated in three different ways: Proper Nouns, Sumerian words and Emesal words.\n",
    "\n",
    "In [ETCSL] **proper nouns** are nouns (`pos` = \"N\"), which are qualified by an additional attribute `type` (Divine Name, Personal Name, Geographical Name, etc.; abbreviated as DN, PN, GN, etc.). In [ORACC] a word has a single `pos`; for proper nouns this is DN, PN, GN, etc. - so what is `type` in [ETCSL] becomes `pos` in [ORACC]. [ORACC] proper nouns usually do not have a guide word (only a number to enable disambiguation of namesakes). The [ETCSL] guide words (`label`) for names come pretty close to ORACC citation forms. Proper nouns are therefore formatted differently from other nouns.\n",
    "\n",
    "**Sumerian words** are essentially treated in the same way in [ETCSL] and [ORACC], but the `citation forms` and `guide words` are often different. Transformation of citation forms and guide words to [ORACC]/[epsd2] standards takes place in the function `etcsl_to_oracc()` (see above, section 6).\n",
    "\n",
    "**Emesal words** in [ETCSL] use their Sumerian equivalents as `citation form` (attribute `lemma`), adding a separate attribute (`emesal`) for the Emesal form proper. This Emesal form is the one that is used as `citation form` in the output.\n",
    "\n",
    "The function `getword()` uses the dictionary `meta_d` which has collected all the meta-data (text ID, composition name, version, line number, etc.) of this particular word It produces the dictionary `word` which is sent to the function `etcsl_to_oracc()`\n",
    "\n",
    "[ETCSL]: http://etcsl.orinst.ox.ac.uk\n",
    "[ORACC]: http://oracc.org\n",
    "[epsd2]: http://oracc.org/epsd2/sux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getword(node):\n",
    "    word = {key:meta_d[key] for key in meta_d} # copy all meta data from metad_d into the word dictionary\n",
    "    if node.tag == 'gloss': # these are Akkadian glosses which are not lemmatized\n",
    "        form = node.xpath('string(.)')\n",
    "        form = form.replace('\\n', ' ').strip() # occasionally an Akkadian gloss may consist of multiple lines\n",
    "        word[\"form\"] = tounicode(form) # check - is this needed?\n",
    "        word[\"lang\"] = node.xpath(\"string(@lang)\")\n",
    "        alltexts.append(word)\n",
    "        return\n",
    "    \n",
    "    word[\"cf\"] = node.xpath('string(@lemma)') # xpath('@lemma') returns a list. The string\n",
    "    word[\"cf\"] = word[\"cf\"].replace('Xbr', '(X)')  # function turns it into a single string\n",
    "    word[\"gw\"] = node.xpath('string(@label)')\n",
    "\n",
    "    if len(node.xpath('@pos')) > 0:\n",
    "        word[\"pos\"] = node.xpath('string(@pos)')\n",
    "    else:         # if a word is not lemmatized (because it is broken or unknown) add pos = NA and gw = NA\n",
    "        word[\"pos\"] = 'NA'\n",
    "        word[\"gw\"] = 'NA'\n",
    "\n",
    "    form = node.xpath('string(@form)')\n",
    "    word[\"form\"] = form.replace('Xbr', '(X)')\n",
    "    \n",
    "    if len(node.xpath('@emesal')) > 0:\n",
    "        word[\"cf\"] = node.xpath('string(@emesal)')\n",
    "        word[\"lang\"] = \"sux-x-emesal\"\n",
    "    else:\n",
    "        word[\"lang\"] = \"sux\"\n",
    "\n",
    "    exception = [\"unclear\", \"Mountain-of-cedar-felling\", \"Six-headed Wild Ram\", \n",
    "                     \"The-enemy-cannot-escape\", \"Field constellation\", \n",
    "                     \"White Substance\", \"Chariot constellation\", \n",
    "                 \"Crushes-a-myriad\", \"Copper\"]\n",
    "    \n",
    "    if len(node.xpath('@type')) > 0 and word[\"pos\"] == 'N': # special case: Proper Nouns\n",
    "        if node.xpath('string(@type)') != 'ideophone':  # special case in the special case: skip ideophones\n",
    "            word[\"pos\"] = node.xpath('string(@type)')\n",
    "            word[\"gw\"] = '1'\n",
    "            if node.xpath('string(@label)') not in exception:\n",
    "                word[\"cf\"] = node.xpath('string(@label)')\n",
    "    if len(node.xpath('@status')) > 0:\n",
    "        word['status'] = node.xpath('string(@status)')\n",
    "    \n",
    "    word[\"cf\"] = tounicode(word[\"cf\"])\n",
    "    word[\"form\"] = tounicode(word[\"form\"])\n",
    "    etcsl_to_oracc(word)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Formatting Lines\n",
    "\n",
    "A line may either be an actual line (in Sumerian and/or Akkadian) or a gap (a portion of text lost). Both receive a line reference. A line reference is an integer that is used to keep lines (and gaps) in their proper order.\n",
    "\n",
    "The function `getline()` is called by `getsection()`. If the argument of `getline()` is an actual line (not a gap) it calls `getword()` for every individual word in that line.\n",
    "\n",
    "[ETCSL]: http://etcsl.orinst.ox.ac.uk\n",
    "[ORACC]: http://oracc.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getline(lnode):\n",
    "    meta_d[\"id_line\"] += 1\n",
    "    if lnode.tag == 'gap':\n",
    "        line = {key:meta_d[key] for key in [\"id_text\", \"text_name\", \"version\", \"id_line\"]}\n",
    "        line[\"extent\"] = lnode.xpath(\"string(@extent)\")\n",
    "        alltexts.append(line)\n",
    "        return\n",
    "    \n",
    "    for node in lnode.xpath('.//w|.//gloss[@lang=\"akk\"]'):\n",
    "                        # get <w> nodes and <gloss> nodes, but only Akkadian glosses\n",
    "        getword(node)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sections\n",
    "\n",
    "Some [ETCSL] compositions are divided into **sections**. That is the case, in particular, when a composition has gaps of unknown length. \n",
    "\n",
    "The function `getsection()` is called by `getversion()` and receives one argument: `tree` (the `etree` object representing one version of the composition). The function updates `meta_d`, a dictionary of meta data. The function `getsection()` checks to see whether a sub-division into sections is present. If so, it iterates over these sections. Each section (or, if there are no sections, the composition/version as a whole) consists of series of lines and/or gaps. The function `getline()` is called to process each line or gap. \n",
    "\n",
    "[ETCSL]: http://etcsl.orinst.ox.ac.uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getsection(tree):\n",
    "    sections = tree.xpath('.//div1')\n",
    "    \n",
    "    if len(sections) > 0: # if the text is not divided into sections - skip to else:\n",
    "        for snode in sections:\n",
    "            section = snode.xpath('string(@n)')\n",
    "            for lnode in snode.xpath('.//l|.//gap'):\n",
    "                if lnode.tag == 'l':\n",
    "                    line = section + lnode.xpath('string(@n)')\n",
    "                    meta_d[\"label\"] = line   # \"label\" is the human-legible \n",
    "                getline(lnode)\n",
    "\n",
    "    else:\n",
    "        for lnode in tree.xpath('.//l|.//gap'):\n",
    "            if lnode.tag == 'l':\n",
    "                line_no = lnode.xpath('string(@n)')\n",
    "                meta_d[\"label\"] = line_no\n",
    "            getline(lnode)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Versions\n",
    "\n",
    "In some cases an [ETCSL] file contains different versions of the same composition. The versions may be distinguished as 'Version A' vs. 'Version B' or may indicate the provenance of the version ('A version from Urim' vs. 'A version from Nibru'). In the edition of the proverbs the same mechanism is used to distinguish between numerous tablets (often lentils) that contain just one proverb, or a few, and are collected in the files \"Proverbs from Susa,\" \"Proverbs from Nibru,\" etc. ([ETCSL] c.6.2.1 - c.6.2.5).\n",
    "\n",
    "The function `getversion()` is called by the function `parsetext()` and receives one argument: `tree` (the `etree` object). The function updates`meta_d`, a dictionary of meta-data. The function checks to see if versions are available in the file that is being parsed. If so, the function iterates over these versions while adding the version name to the `meta_d` dictionary. If there are no versions, the version name is left empty. The parsing process is continued by calling `getsection()` to see if the composition/version is further divided into sections.\n",
    "\n",
    "[ETCSL]: http://etcsl.orinst.ox.ac.uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getversion(tree):\n",
    "    versions = tree.xpath('.//body[child::head]')\n",
    "\n",
    "    if len(versions) > 0: # if the text is not divided into versions - skip 'getversion()':\n",
    "        for vnode in versions:\n",
    "            version = vnode.xpath('string(head)')\n",
    "            version = eq[\"versions\"][version]\n",
    "            meta_d[\"version\"] = version\n",
    "            getsection(vnode)\n",
    "\n",
    "    else:\n",
    "        meta_d[\"version\"] = ''\n",
    "        getsection(tree)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Parse a Text\n",
    "\n",
    "The function `parsetext()` takes one xml file (a composition in [ETCSL]) and parses it, calling a variety of functions defined above. \n",
    "\n",
    "The parsing is done by the `etree` package in the `lxml` library. Before the file can be parsed properly the so-called HTML entities need to be replaced by their Unicode equivalents. This is done by calling the `ampersands()` function (see above, section 3: Preprocessing).\n",
    "\n",
    "[ETCSL]: http://etcsl.orinst.ox.ac.uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsetext(file):\n",
    "    with open(f'etcsl/transliterations/{file}') as f:\n",
    "        xmltext = f.read()\n",
    "    xmltext = ampersands(xmltext)          #replace HTML entities by Unicode equivalents\n",
    "    \n",
    "    tree = etree.fromstring(xmltext)\n",
    "    \n",
    "    tree = mark_extra(tree, \"additional\") # mark additional words with attribute status = 'additional'\n",
    "    tree = mark_extra(tree, \"secondary\")  # mark secondary words with attribute status = 'secondary'\n",
    "    name = tree.xpath('string(//title)')\n",
    "    name = name.replace(' -- a composite transliteration', '')\n",
    "    name = name.replace(',', '')\n",
    "    meta_d[\"id_text\"] =  file[:-4]\n",
    "    meta_d[\"text_name\"] = name\n",
    "    meta_d[\"id_line\"] = 0\n",
    "    getversion(tree)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Main Process\n",
    "\n",
    "The list `alltexts` is created as an empty list. It will be filled with dictionaries, each dictionary representing one word form.\n",
    "\n",
    "The variable `textlist` is a list of all the `XML` files with [ETCSL] compositions in the directory `etcsl/transliterations`. Each file  is sent as an argument to the function `parsetext()`. \n",
    "\n",
    "The dictionary `meta_d` is created as an empty dictionary. On each level of analysis the dictionary is updated with meta-data, such as text ID, version name, line number, etc.\n",
    "\n",
    "The list is transformed into a `pandas` DataFrame. All missing values (`NaN`) are replaced by empty strings. \n",
    "\n",
    "[ETCSL]: http://etcsl.orinst.ox.ac.uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textlist = os.listdir('etcsl/transliterations')\n",
    "textlist.sort()\n",
    "if not os.path.exists('Output'):\n",
    "    os.mkdir('Output')\n",
    "\n",
    "amp = re.compile(r'&[^;]+;') #regex for HTML entities, used in ampersands()\n",
    "\n",
    "asccj, unicj = 'cjCJ', 'šŋŠŊ'\n",
    "transcj = str.maketrans(asccj, unicj) # translation table for c > š and j > ŋ\n",
    "\n",
    "ind = re.compile(r'[a-zŋḫṣšṭA-ZŊḪṢŠṬ][0-9x]{1,2}') #regex for sign index nos preceded by a letter\n",
    "ascind, uniind = '0123456789x', '₀₁₂₃₄₅₆₇₈₉ₓ'\n",
    "transind = str.maketrans(ascind, uniind) # translation table for index numbers\n",
    "# regex ind and the translation tables transind and transcj are used in tounicode()\n",
    "\n",
    "alltexts = []\n",
    "files = tqdm(textlist)\n",
    "for file in files:\n",
    "    files.set_description(f'ETCSL {file[2:-4]}')\n",
    "    meta_d = {}\n",
    "    parsetext(file)\n",
    "\n",
    "df = pd.DataFrame(alltexts).fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14 Save as CSV\n",
    "The DataFrame is saved as a `CSV` file named `alltexts.csv` in the directory `output`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/alltexts.csv', 'w', encoding=\"utf-8\") as w:\n",
    "    df.to_csv(w, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
