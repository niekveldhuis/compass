{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Treasure And Shoe Archive from Drehem\n",
    "\n",
    "The Treasure and Shoe archive from Drehem (ancient Puzriš-Dagan) is a relatively small archive (< 300 documents) of administrative texts that date to the 21st century BCE and deal with valuable objects (made of metals and stone) and leather objects (shoes and belts, etc.). The archive was studied in detail by Paola Paoletti in her book *Der König und sein Kreis* (2012). The present notebook essentially takes the texts edited and studied by Paoletti, assigning roles to personal names in each text. This data is then used to study the network of persons involved in this archive. The network is drawn in an interactive plot.\n",
    "\n",
    "This notebook goes through several steps: \n",
    "* Data acquisition\n",
    "* Data cleaning\n",
    "* Defining Nodes\n",
    "* Assigning roles\n",
    "* Defining edges\n",
    "* Creating a Network (using networkx)\n",
    "* Plotting the network (using bokeh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the Necessary Modules\n",
    "Here we import the modules necessary for data acquisition and data cleaning and for assignment of roles and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # this suppresses a warning about pandas from tqdmimport pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout\n",
    "import webbrowser    # for opening web pages\n",
    "import networkx as nx\n",
    "import networkx.algorithms.community as nxcom\n",
    "%matplotlib inline\n",
    "from IPython.display import display, clear_output\n",
    "import re\n",
    "import pickle\n",
    "util_dir = os.path.abspath('../utils')\n",
    "sys.path.append(util_dir)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Corpus\n",
    "\n",
    "A list of P numbers (text ID numbers) of texts that belong to the Treasure/Shoe archive is available in the directory `csv`, kindly made available by Paolo Paoletti. This list contains some additional information, for instance where a text was published, and to which sub-archive it belongs. The latter information may be used later on to separate the Treasure texts from the leather texts (or to color those differently in the plot).\n",
    "\n",
    "One of the texts studied in her book still remains unpublished and is therefore not included (this is U.30117). Several additional Treasury texts were published by [Nawalla Al-Mutawalli and Walther Sallaberger 2018](https://doi-org.libproxy.berkeley.edu/10.1515/za-2017-0101) and one additional leather text is found in [RSO 83 (2010), 345 text 14](http://oracc.org/epsd2/admin/ur3/P433582). Another leather text was identified among the photographs of texts from the Louvre ([AO 11393](http://oracc.org/epsd2/admin/ur3/P493329)) made available on [CDLI](http://cli.ucla.edu/P493329).\n",
    "\n",
    "The corpus used here is, therefore, not exactly identical to what was used in Paoletti's book - but it is very close. At the moment of writing the corpus that is being used includes 288 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'csv/treasury_shoes2.txt'\n",
    "tr_df = pd.read_csv(file, encoding='utf8')\n",
    "ids = list(tr_df['id_text'])\n",
    "ids.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the Corpus\n",
    "When running this notebook for the first time (or after a major update of the Ur III corpus on [epsd2])(http://oracc.org/epsd2/admin/ur3) it is necessary to download the entire Ur III corpus in JSON format. If you already have the file `epsd2-admin-ur3.zip` in the `jsonzip` directory you may comment out the command that does the download, by placing a hashmark (#) in front of it as follows: \n",
    "\n",
    "``` python\n",
    "#oracc_download([project])\n",
    "```\n",
    "\n",
    "Remove the hashmark and run the cell again to download a fresh copy (this is a large file and may take between one and ten minutes, depending on the speed of your computer and your connection).\n",
    "\n",
    "# Note\n",
    "Instead of the full Ur III corpus, we are using the (temporary) ORACC project [treasury](http://build-oracc.museum.upenn.edu/treasury/pager), which contains all of the documents that belong to our corpus. In order to maintain compatability with other usages of this script with other corpora, the code that selects the relevant documents is maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'treasury'\n",
    "#project = 'epsd2/admin/ur3'\n",
    "oracc_download([project]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Parsejson function\n",
    "\n",
    "The `parsejson()` function used here differs in one way from the ones demonstrated in Chapter 2.1. The code looks for a `key` called `ftype` (field type) among the lemmatization data. The only value that `ftype` may take in this corpus is `yn`, indicating that a word is part of a year name. The field `ftype` enables us to easily exclude year names from our analysis. We will, of course, consider the dating of each texts (that is part of the metadata). The vocabulary of the year names, however, is irrelevant for understanding the transactions and connections described in this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsejson(text):\n",
    "    for JSONobject in text[\"cdl\"]:\n",
    "        if \"cdl\" in JSONobject: \n",
    "            parsejson(JSONobject)\n",
    "        if \"label\" in JSONobject:\n",
    "            meta_d[\"label\"] = JSONobject['label']\n",
    "        if \"f\" in JSONobject:\n",
    "            lemma = JSONobject[\"f\"]\n",
    "            if \"ftype\" in JSONobject:\n",
    "                lemma['ftype'] = JSONobject['ftype'] # this picks up YN for year name\n",
    "            lemma[\"id_word\"] = JSONobject[\"ref\"]\n",
    "            lemma['label'] = meta_d[\"label\"]\n",
    "            lemma[\"id_text\"] = meta_d[\"id_text\"]\n",
    "            lemm_l.append(lemma)\n",
    "        if \"strict\" in JSONobject and JSONobject[\"strict\"] == \"1\":\n",
    "            lemma = {key: JSONobject[key] for key in dollar_keys}\n",
    "            lemma[\"id_word\"] = JSONobject[\"ref\"]\n",
    "            lemma[\"id_text\"] = meta_d[\"id_text\"]\n",
    "            lemm_l.append(lemma)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call the Parser on Treasury Texts\n",
    "\n",
    "The variable `ids`, created above, holds all the P numbers of the documents we are interested in. This list is used to define the list of files to be extracted from the `zip` file and to be sent to the parser. Otherwise, the code below is identical to the code in Chapter 2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_l = []\n",
    "meta_d = {\"label\": None, \"id_text\": None}\n",
    "dollar_keys = [\"extent\", \"scope\", \"state\"]\n",
    "file = f\"jsonzip/{project.replace('/', '-')}.zip\"\n",
    "try:\n",
    "    z = zipfile.ZipFile(file) \n",
    "except:\n",
    "    print(f\"{file} does not exist or is not a proper ZIP file\")\n",
    "files = z.namelist() # list of all the files in the ZIP file\n",
    "files = [name for name in files if name[-12:-5] in ids] # select only those of the treasury/leather archive\n",
    "for filename in tqdm(files, desc = project):\n",
    "    id_text = project + filename[-13:-5] \n",
    "    meta_d[\"id_text\"] = id_text\n",
    "    try:\n",
    "        st = z.read(filename).decode('utf-8')\n",
    "        data_json = json.loads(st)           \n",
    "        parsejson(data_json)\n",
    "    except:\n",
    "        print(f'{id_text} is not available or not complete')\n",
    "z.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame\n",
    "\n",
    "The `parsejson()` function has filled the list of lists `lemm_l` with data. This list  is read into a DataFrame for further manipulation. We remove commas and spaces from Guide Words and simplify the field `id_text` to read \"P433582\" instead of \"epsd2/admin/ur3/P433582\" (all documents derive from [epsd2/admin/ur3](http://oracc.org/epsd2/admin/ur3)).\n",
    "\n",
    "Finally the code adds a field `id_line` (integer) in order to keep track of the lines in a document. Since lines are meaningful units in Sumerian administrative texts, this will be an important tool. \n",
    "\n",
    "All of these steps are identical (or very similar) to steps explained in Chapter 2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame(lemm_l).fillna('')\n",
    "#keep = ['extent', 'scope', 'state', 'id_word', 'id_text', 'form', 'cf', 'gw', 'pos', 'ftype', 'label']\n",
    "#words = words[keep]\n",
    "words['gw'] = words['gw'].replace([' ', ','], ['', ''], regex=True)\n",
    "words['id_text'] = [i[-7:] for i in words['id_text']]\n",
    "words['id_line'] = [int(i.split('.')[1]) for i in words['id_word']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Lemma Field\n",
    "\n",
    "The `lemma` field strings Citation Form, Guide Word, and Part of Speech together and is dealt with in the same way as in Chapter 2.1 Words that remain unlemmatized (e.g. because of illegible or broken signs) are represented by their `form` and numbers receive the POS 'NU'. \n",
    "\n",
    "In the DataFrame (physical) breaks and text divisions (marked by horizontal rulings on the tablet, blank lines, and the like) are preserved in the field `state`. Such demarcations have their own row in the DataFrame and receive their own line ID, but they do not have data in fields such as `form`. In order to keep track of breaks we will define two different types: logical and physical breaks. The keywords break_physical and break_logical are included in the `lemma` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_break = ['illegible', 'traces', 'missing', 'effaced']\n",
    "logical_break = ['other', 'blank', 'ruling']\n",
    "words['lemma'] = words[\"cf\"] + '[' + words[\"gw\"] + ']' + words[\"pos\"]\n",
    "words.loc[words[\"cf\"] == \"\" , 'lemma'] = words['form'] + '[NA]NA'\n",
    "words.loc[words[\"pos\"] == \"n\" , 'lemma'] = words['form'] + '[]NU'\n",
    "words.loc[words[\"state\"].isin(logical_break), 'lemma'] = \"break_logical\"\n",
    "words.loc[words[\"state\"].isin(physical_break), 'lemma'] = \"break_physical\"\n",
    "words.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proper Nouns in the Ur III Corpus\n",
    "## The Name Authority\n",
    "\n",
    "In order to build a network of interactions between individuals in the Treasury corpus we need to be able to extract proper nouns. The lemmatization of proper nouns in this corpus is still in early stages, so that we cannot fully rely on the lemmatized data as represented in our DataFrame. A list of all proper nouns from Drehem texts was put together by John Carnahan and Niek Veldhuis, based on the Drehem texts in [BDTNS](http://bdtns.filol.csic.es/), December 2016. This list includes the form of each name (in transliteration) plus a normalization of that name in [epsd2](http://oracc.org/epsd2) style. The list includes spelling variants as well as names with morphology. Thus, **Ur-{d}en-lil₂**, **Ur-{d}en-lil₂-la₂**, and **Ur-{d}en-lil₂-la₂-ta** are all listed seperately, and are all associated with the same name, as follows: \n",
    "```csv\n",
    "Ur-{d}En-lil₂,UrEnlilak[]PN,\n",
    "Ur-{d}En-lil₂-la₂,UrEnlilak[]PN,\n",
    "Ur-{d}En-lil₂-la₂-še₃,UrEnlilak[]PN,\n",
    "Ur-{d}En-lil₂-la₂-ka-še₃,UrEnlilak[]PN,\n",
    "Ur-{d}En-lil₂-la₂-ka-ta,UrEnlilak[]PN,\n",
    "Ur-{d}En-lil₂-la₂-ke₄,UrEnlilak[]PN,\n",
    "Ur-{d}En-lil₂-la₂-ta,UrEnlilak[]PN,\n",
    "Ur-{d}En-lil₂-ta,UrEnlilak[]PN,\n",
    "```\n",
    "Similarly, name instances that have different spellings but belong to the same person are associated with one normalized form of that name, as in: \n",
    "```csv\n",
    "{d}Šul-gi-si-im-ti,Šulgisimti[]PN, \n",
    "{d}Šul-gi-si-im-tum,Šulgisimti[]PN,\n",
    "```\n",
    "\n",
    "The list includes all name forms from Drehem (in 2016), currently almost 6,000 entries. \n",
    "\n",
    "We will use this list (`drehem_norm_names.csv`, available in the `Normalized` directory) to find proper nouns (Personal Names, Divine Names, Geographical Names, etc.) in our DataFrame and to create more reliable lemmatizations of names than [epsd2](http://oracc.org/epsd2) can offer today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normdf = pd.read_csv('Normalized/drehem_names2.csv', encoding='utf8')\n",
    "normdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce Names to Sign Sequences\n",
    "\n",
    "Many Proper Nouns may validly be transcribed in different ways. Sometimes that is the case because the name is not fully understood or of foreign origin. The city name **a-dam-DUN{ki}** is read variously **a-dam-šah₂{ki}**, **a-dam-dun{ki}**, or **a-dam-DUN{ki}** by different scholars. For a network, the correct reading is of less importance than a consistent reading. All these readings reflect a sign sequence **A DAM DUN KI**, a sequence of signs that cannot plausibly be anything else than a reference to this city.\n",
    "\n",
    "We will, therefore, use the ORACC Global Sign List ([ogsl](http://oracc.org/ogsl)) to transform each name form in our data set into a sequence of signs. The name authority already has a column `sign_names`; we can  use this column to associate sign sequences with normalized names.\n",
    "\n",
    "Much of the code in the following cells is equivalent to code in Chapter 2.4, where we developed a tool for searching signs in [bdtns](http://bdtns.filol.csic.es/) data.\n",
    "\n",
    "# Download OGSL\n",
    "\n",
    "Using code developed in Chapter 2, download the file `ogsl.zip`, which contains all of the [OGSL](http://oracc.org/ogsl) data (sign names and sign readings) in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracc_download(['ogsl']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign Equivalencies\n",
    "\n",
    "In the Ur III period, some signs that are differentiated in [OGSL](http://oracc.org/ogsl) have fallen together. For instance, **DUR₂** and **KU**, which are separate in the Fara period, are represented by the same sign in Ur III. Such signs are listed in the following dictionary. This list may not be complete - it can be extended when additional equivalencies are identified. The compiled [regular expression](https://docs.python.org/3/howto/regex.html) is used to replace sign names where appropriate while parsing the [OGSL](http://oracc.org/ogsl) JSON in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equiv = {'ANŠE' : 'GIR₃', \n",
    "        'DUR₂' : 'KU', \n",
    "        'NAM₂' : 'TUG₂', \n",
    "        'TIL' : 'BAD', \n",
    "        'NI₂' : 'IM',\n",
    "        'ŠAR₂' : 'HI', \n",
    "         'DUH' : 'GABA'\n",
    "        }\n",
    "w = re.compile(r'\\w+') # replace whole words only - do not replace TILLA with BADLA.\n",
    "           # but do replace |SAL.ANŠE| with |SAL.GIR₃|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse OGSL\n",
    "The parse function uses the `equiv` dictionary and the compiled regular expression to fill a dictionary, associating each valid sign reading with a sign name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseogsljson(data_json):\n",
    "    for key, value in data_json[\"signs\"].items():\n",
    "        key = re.sub(w, lambda m: equiv.get(m.group(), m.group()), key)\n",
    "        if \"values\" in value:\n",
    "            for n in value[\"values\"]:\n",
    "                d2[n] = key\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call the OGSL parser\n",
    "Calling  the function in the previous cell, a dictionary is formed where each `key` is a sign value and each `value` a sign name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = {}  # this empty dictionary is filled by the parsejson() function, called in this cell.\n",
    "file = \"jsonzip/ogsl.zip\"\n",
    "z = zipfile.ZipFile(file) \n",
    "filename = \"ogsl/ogsl-sl.json\"\n",
    "signlist = z.read(filename).decode('utf-8')\n",
    "data_json = json.loads(signlist)                # make it into a json object (essentially a dictionary)\n",
    "parseogsljson(data_json)  \n",
    "with open('output/ogsl_dict.p', 'wb') as p:\n",
    "    pickle.dump(d2, p)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separators = ['{', '}', '-']\n",
    "separators2 = ['.', '+', '|']  # used in compound signs\n",
    "#operators = ['&', '%', '@', '×']\n",
    "flags = \"][?<>⸢⸣⌈⌉*/\" # note that ! is omitted from flags, because it is dealt with separately\n",
    "table = str.maketrans(dict.fromkeys(flags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signnames(translit):  \n",
    "    \"\"\"This function takes a string of transliterated cuneiform text and translates that string into a string of\n",
    "    sign names, separated by spaces. In order to work it needs the variables separators, separators2, flags, and table defined above. The variable table\n",
    "    is used by the translate() method to translate all flags (except for !) to None. The function also needs a dictionary, called d2, that has as\n",
    "    keys sign readings and sign names as corresponding values. In case a key is not found, the sign reading is replaced by itself.\"\"\"\n",
    "    sign_sequence = []\n",
    "    translit = translit.translate(table).lower()  # remove flags, half brackets, square brackets.\n",
    "    translit = translit.replace('...', 'x')\n",
    "    for s in separators: # split transliterated form into signs   \n",
    "        translit = translit.replace(s, ' ').strip()\n",
    "    s_l = translit.split() # s_l is a list that contains the sequence of transliterated signs without separators or flags\n",
    "    # Now take care of some special situations: signs with qualifiers, compound signs.\n",
    "    for sign in s_l:\n",
    "        if '!' in sign: # corrected sign, as in ka!(SAG), get only the corrected reading.\n",
    "            sign = sign.split('!(')[0]\n",
    "            sign = sign.replace('!', '') # remove remaining exclamation marks\n",
    "        elif sign[-1] == ')' and '(' in sign: # qualified sign, as in ziₓ(SIG₇) - get only the qualifier\n",
    "            sign = sign.split('(')[1][:-1]\n",
    "        sign_sequence.append(sign)\n",
    "    sign_sequence = [d2.get(sign, sign).upper() for sign in sign_sequence] # replace each transliterated sign with its sign name.\n",
    "    signnames = ''\n",
    "    for s in sign_sequence:\n",
    "        if '.' in s or '+' in s: # DIRI signs are analyzed only when they represented a simple sign sequence\n",
    "            if not '×' in s: # compounds like |UD×(U.U.U)| are not further analyzed.\n",
    "                for sep in separators2:\n",
    "                    s = s.replace(sep, ' ').strip() \n",
    "        signnames = f'{signnames} {s}'.strip()\n",
    "    return signnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Column Sign_Names\n",
    "Use the function in the cell above to add a new column column, called `sign_names` to the `words` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normdf[\"sign_names\"] = normdf[\"transliteration\"].progress_map(signnames)\n",
    "words['sign_names'] = words['form'].progress_map(signnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proper Noun Dictionary\n",
    "Transform the Name Authority in a dictionary with `sign_names` as key and `normalization` as value and use this dictionary to transform words marked as Proper Nouns or 'X' (unknown) in the Part of Speech (`pos`) column into normalized names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proper_nouns = ['RN', 'PN', 'DN', 'AN', 'WN', 'ON', 'TN', 'CN', 'GN', 'SN']\n",
    "normd2 = dict(zip(normdf['sign_names'], normdf['normalization']))\n",
    "words.loc[words.pos.isin(proper_nouns + ['X']), 'lemma'] = words.progress_apply(lambda x: normd2.get(x['sign_names'], x['lemma']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Corrections\n",
    "The Ur III data sets as represented in [epsd2/admin/ur3](http://oracc.org/epsd2/admin/ur3) may still contain errors, which will  surface later in the process. For instance, a name does not show up in the analysis, because it has not been lemmatized as a name. Or, the other way around, a word may have been falsely lemmatized as a name. When found, such errors can be corrected (temporarily, until they get corrected in the source files) in the file `corrections.csv` in the directory `Normalized`, associating forms with correct lemmatizations. The final line of code in this cell will correct the column `pos` when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corrections = pd.read_csv('Normalized/corrections.csv', encoding='utf8')\n",
    "#corr_d = dict(zip(corrections['form'], corrections['corr']))\n",
    "#words.loc[words.pos.isin(proper_nouns + ['X']), 'lemma'] = words.progress_apply(lambda x: corr_d.get(x['form'], x['lemma']), axis =1)\n",
    "#words['pos'] = [w.split(']')[-1] if ']' in w else '' for w in words['lemma']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Typology\n",
    "Different types of documents will be treated differently. In the Treasure Archive the main document types are Intake, Expenditure, and Transfer.\n",
    "\n",
    "|  type           | keyword         | lemma(s)              | description |\n",
    "|----------------|------------------|-----------------------|-------------|\n",
    "| Intake      | mu-kuₓ(DU)      | mu.DU\\[delivery\\]N      | materials delivered to the administration |\n",
    "| Expenditure | ba-zi or zi-ga  | zig[rise]V/i          | materials expended by the administration |\n",
    "| Transfer    | šu ba-ti        | šu\\[hand\\]N teŋ\\[near\\]V/i | materials transfered from one office to another |\n",
    "\n",
    "In the great majority of cases they can be identified simply by looking  for the keyword lemmas.\n",
    "\n",
    "This typology is discussed in much detail in Paoletti's *Der König und sein Kreis* (2012), Chapter 2.1 and 3.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texttype = []\n",
    "for i in ids:\n",
    "    ttype = ''\n",
    "    text = list(words.loc[words.id_text == i, 'lemma'])\n",
    "    if 'mu.DU[delivery]N' in text:\n",
    "        ttype = 'intake'\n",
    "    elif 'zig[rise]V/i' in text:\n",
    "        ttype = 'expenditure'\n",
    "    elif 'teŋ[near]V/i' in text: \n",
    "        ttype = 'transfer'\n",
    "    texttype.append(ttype)\n",
    "treasure = dict(zip(ids, texttype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvest Nodes\n",
    "\n",
    "For the node list, select Personal Names (PN) and Royal Names (RN) but skip those that appear in Year Names or Seals. Add lugal (king) sukkalmah (prime minister) and nin (queen), because these actors may be mentioned without personal name. Seals and year names are excluded here because the text does not directly reflect the transaction that is recorded in the tablet. Seals do include important information about the identity of the sealer (a father's name, a profession and/or loyalty to a king or another high official). Such information could be harvested for node attributes (but that is not done here). Year names include essential chronological information (and that is taken care of elsewhere) - but the proper nouns in year names (the name of the king, or the name of a priest being elected) are not part of the transaction proper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnamed = {'lugal[king]N', 'nin[queen]N', 'sukkalmah[official]N'}\n",
    "names = set(words.loc[words.pos.isin(['PN', 'RN', 'DN']), 'lemma'])\n",
    "actors = unnamed | names\n",
    "seal = list(words.loc[words.label.str.contains('seal'), 'id_word'])\n",
    "yn = list(words.loc[words.ftype == 'yn', 'id_word'])\n",
    "exclude = seal + yn\n",
    "PNs = words.loc[(words.lemma.isin(actors)) & (~words.id_word.isin(exclude))].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define keywords and the corresponding roles\n",
    "The accounts use key words to indicate the particular role of each person in a document. Some of these key words (or key phrases, in some instances) follow the Proper Noun, others precede it. Thus when the word **ŋiri\\[foot\\]N** immediately precedes a Personal Name, that person is an intermediary (or conveyor). A Personal Name followed by **šu\\[hand\\]N teŋ\\[near\\]V/i** marks the recipient of goods.\n",
    "\n",
    "The keywords are associated with a role in two separate dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_post = {'mu.DU[delivery]N': 'deliverer', 'maškim[administrator]N' : 'representative', \n",
    "               'zig[rise]V/i' : 'expender',\n",
    "               'šu[hand]N teŋ[near]V/i': 'recipient' , 'šu[hand]N us[follow]V/t': 'sender', \n",
    "           'la[hang]V/t' : 'weigher'\n",
    "            }\n",
    "key_pre =  {'ŋiri[foot]N' : 'intermediary', \n",
    "            'arua[offering]N' : 'offerer', 'kišib[seal]N' : 'sealer', \n",
    "            'gabari[copy]N kišib[seal]N' : 'sealer', 'mu.DU[delivery]N' : 'deliverer',\n",
    "            'mu[name]N' : 'reason', 'ki[place]N' : 'source', 'kiŋ[work]N ak[do]V/t' : 'producer', \n",
    "            'niŋgur[property]N' :'owner', 'šag[heart]N la[hang]V/t' : 'weigher'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine the Role of each PN\n",
    "The role of a personal name in an administrative texts is determined by keywords before of after the name. The keywords and their corresponding roles are defined in the dictionaries `key_post` and `key_pre` in the preceding cell.\n",
    "\n",
    "The DataFrame `PNs`, created above, is an extract from the DataFrame `words` and therefore uses the same indexes. If the index of a Personal Name in `PNs` is `i`, then the word before that name is `words.loc[i-1]`.\n",
    "\n",
    "We iterate over the index numbers in `PNs` to determine which keywords are found before or after. Before doing so we determine the extent of a document by finding the lowest and highest index number with the same `id_text` as the name instance we are dealing with. Similarly, we can define the extent of the line on which the name is found by finding the lowest and highest index number of the words within the document that share the same `id_line`. With these index numbers we can easily see whether a name is in first position in a line (its index equals the lowest index of that line) or what the last word is in the line in which the name instance appears. We can also check to make sure that when we inspect the next line, this line still belongs to the same document. Working with indexes this way is very fast and flexible - we do not have to look up the P number (in `id_text`) every time, or check the `id_line` field.\n",
    "\n",
    "Because of the structure of Sumerian grammar, keywords that come *after* the name may be separated from their name instance by one or more positions, as in:\n",
    "> \tARAD₂-{d}nanna sukkal maškim\t\t\tAradNanna the minister was representative.\" \n",
    "\n",
    "In practice the keyword is always the last word on the line, or the first on the next line, as in:\n",
    "> \tlu₂-{d}nanna šagina nag-su{ki}-ke₄ \t\tLuNanna the military leader of Nagsu\n",
    "> \tšu ba-ti \t\t\t\t\t\t\t\treceived\n",
    "\n",
    "Therefore, if the PN is the first word on the line, we can check  to see if the last word on the line is a keyword and, if so, assign a role to the PN. We can also look at the first word (or two words) of the next line, to see if there is a keyword in that position. Any word following the PN that is not a keyword is considered an attribute (a father's name, a profession, etc.).\n",
    "\n",
    "If the PN is not the first word on the line (its index number is higher than the lowest index number of that line), one possibility is that it is preceded by a keyword, as in:\n",
    "> \tki lu₂-dingir-ra-ta\t\t\t\t\tfrom Ludingira\n",
    "\n",
    "If there is a preceding keyword, we use the `key_pre` dictionary to assign a role. But a PN later in the line may also be the name of a parent (PN1 dumu PN2) or some other relative, or, occasionally, the name of an employer, as in:\n",
    "> \tgiri₃ a-bu-du₁₀ lu₂ a-bu-ni-ka\t\tTransporter: Abuṭāb the man of Abuni\n",
    "\n",
    "In the case of Abuni, his name is not preceded by a keyword, but there is another PN earlier in the line. Abuni does not have a role in the document directly, but he is involved indirectly by his relation of authority to Abuṭāb. In such cases the role is defined as 'relation'. \n",
    "\n",
    "If none of these methods results in a role assignment, we fall back to default roles. Default roles are different for different document types. Transfer texts (keywords šu ba-ti) and Intake (mu.DU\\[delivery\\]N) may record deliveries by multiple people who are not specifically marked with a keyword. The default in those texts is \"source\". Expenditure texts (zi-ga or ba-zi) may record expenditures to various people, not marked specifically. Here the default is \"recipient\".\n",
    "\n",
    "The code takes care of a few special cases. If a name is preceded by u\\[and\\]CNJ the role of that name is the same as the role of the preceding name. The expression ki PN-še₃ (\"to the place of\") appears a few times in this archive, but is otherwise very rare in Ur III. It uses the same keyword ki\\[place\\]N as the very common expression ki PN-ta (\"from the place of\") but has the opposite meaning. Since the morphology (-ta vs. -še₃) is not available in the `lemma` column we have to use the `form` column to check for such instances. The expression kin ak, which precedes the name of a craftsman (finished work of ...) may be found on the line preceding the name.\n",
    "\n",
    "Breaks in the text and horizontal rulings have their own entries in the DataFrame `words`, with separate index numbers and a separate `id_line`. This means that while looking for keywords in a preceding or following line the code will never \"jump\" over such a break. If, for instance, a document has a name instance right before a break and the line right after the break has šu ba-ti, this will not result in the role \"recipient\" for the name instance because the break is the next line. It is possible that a name instance right before or right after a break does not receive a role because key words are missing - and is then assigned a default role. This is a possible source of errors.\n",
    "\n",
    "Much of the code here might be used in the same way for other Ur III archives, but it is likely that there will be other keywords and other special cases that one would need to take care of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = []\n",
    "attribute = []\n",
    "for i in PNs.index:                           # the index of PNs is identical to the one of words\n",
    "    Pno = PNs.loc[i]['id_text']                    # the text ID (P number)\n",
    "    lineno = PNs.loc[i]['id_line']                 # the line number in which the name instance appears\n",
    "    text = words.loc[words.id_text == Pno]                # the entire text\n",
    "    line = text.loc[text.id_line == lineno] #  the line with the PN\n",
    "    mnw = min(line.index)                 # index no. of first word in line\n",
    "    mxw = max(line.index)                   # index no. of last word in line\n",
    "    mn = min(text.index)                    # index no. of first word in text\n",
    "    mx = max(text.index)                    # index no. of last word in text\n",
    "    mxl = text.loc[mx]['id_line']          # highest line number in text\n",
    "    if lineno < mxl:                       # check that next line is still in the same text\n",
    "        nl = text.loc[mxw + 1, 'id_line']  # line id of the word after \n",
    "                                           # the last word of the line\n",
    "        nextline = text.loc[text.id_line == nl] # words in nextline\n",
    "    else:\n",
    "        nextline = pd.DataFrame(columns = words.columns) #empty dataframe\n",
    "    attr = []\n",
    "    r = ''\n",
    "    # first get attributes\n",
    "    if i < mxw:  # PN is not the last word of the line\n",
    "        attr = list(line.loc[i+1:]['lemma'])   # all words following a PN are attributes\n",
    "        attr = [w for w in attr if not w in key_post]  # except for keywords\n",
    "        if len(attr) > 1: \n",
    "            if ' '.join(attr[-2:]) in key_post: # or multi-word keywords\n",
    "                attr = attr[:-2]\n",
    "                \n",
    "    # now assign roles\n",
    "    if mnw == i:               # PN in first position\n",
    "        r = key_post.get(line.loc[mxw]['lemma'], r)    # keyword appears in same line in last position\n",
    "        r = key_post.get(words.loc[mxw+1]['lemma'], r) # or in the next line in first position\n",
    "        if len(line) > 1: \n",
    "            lastwords = ' '.join(list(line[-2:]['lemma'])) # two-word keywords (as in šu ba-ti)\n",
    "            r = key_post.get(lastwords, r)\n",
    "        if len(nextline) > 1:               # two-word keyword appearing in first position in next line\n",
    "            firstwords = ' '.join(list(nextline[:2]['lemma']))\n",
    "            r = key_post.get(firstwords, r) \n",
    "        if i > mn + 1:  # special case: if kin ak or ša₃ i₃-la₂ is found on preceding line \n",
    "            preceding2words = ' '.join(list(words.loc[i-2:i-1]['lemma']))\n",
    "            r = key_pre.get(preceding2words, r)\n",
    "\n",
    "                \n",
    "    elif i > mnw:             # PN appears further in the line with keyword(s) preceding\n",
    "        firstwords = ' '.join(list(line.loc[mnw:i-1]['lemma']))  # join all words before PN\n",
    "        r = key_pre.get(firstwords, r)        # ŋiri₃ PN; ki PN-ta; kin ak PN; etc\n",
    "        if line.loc[mnw]['lemma'] == 'ki[place]N' and line.loc[mxw]['form'].endswith('-še₃'): \n",
    "            r = 'destination'              # special case: ki PN-še₃         \n",
    "        if r == '':                # role has not been filled yet\n",
    "            PN = [w for w in line.loc[mnw:i-2]['lemma'] if w in list(PNs['lemma'])] # is there another PN previously in the line?\n",
    "            if PN:                                                                  \n",
    "                if words.loc[i-1]['lemma'] == 'u[and]CNJ':   # PN1 u PN2\n",
    "                    if role:\n",
    "                        r = role[-1]               # same role as previous PN\n",
    "                else:\n",
    "                    r = 'relation'             # as in PN dumu PN\n",
    "            elif treasure.get(Pno) == 'expenditure': # commodities + PN: recipient\n",
    "                r = 'recipient'\n",
    "            else:                              # if there is no preceding PN: drop the name\n",
    "                r = 'none'                     # this may  need refinement\n",
    "        if PNs.loc[i]['lemma'] in unnamed and words.loc[i-1]['lemma'] in list(PNs['lemma']):  \n",
    "            r = 'none'       # 'unnamed' person (e.g. sukkalmah) is preceded by name\n",
    "    if r == '' :\n",
    "        if treasure.get(Pno) == 'expenditure': # default role for zi-ga/ba-zi texts\n",
    "            r = 'recipient'\n",
    "        elif treasure.get(Pno) in ['intake', 'transfer']:    # default role for mu-DU and šu ba-ti texts\n",
    "            r = 'source'\n",
    "    role.append(r)\n",
    "    attribute.append(' '.join(attr))\n",
    "PNs['role'] = role\n",
    "PNs['attribute'] = attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show results in a table with links for checking\n",
    "The code in the following two cells does not add or change anything, but allows to check the results. In the second cell one may enter a condition to restrict the output to a particular text, a particular kind of role, or lemma, for instance\n",
    "```python\n",
    "return PNs2.loc[PNs2.role == \"producer\", ['id_word', 'form', 'pos', 'lemma', 'role', 'attribute']][:rows].style\n",
    "```\n",
    "to see only the name instances that have received the role 'producer'. Or\n",
    "```python\n",
    "return PNs2.loc[PNs2.id_text == \"P102169\", ['id_word', 'form', 'pos', 'lemma', 'role', 'attribute']][:rows].style\n",
    "```\n",
    "to see the roles assigned to the name instances in text P102169.\n",
    "\n",
    "The Pandas `style` method creates links out of properly formated HTML anchor (\\<a\\>) tags. The links are made by adding the `id_word` field to the URL for the ur3 database. This will highlight the word in question. Note that `id_word` is fairly stable, but not absolutely so. If the text has been edited in such a way that there are more (or less) lines on the object or there are more (or less) words in the line, the highlighting may be off.\n",
    "\n",
    "Move the slide to see more (or less) results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = '<a href=\"http://build-oracc.museum.upenn.edu/treasury/{}\", target=\"_blank\">{}</a>'\n",
    "PNs2 = PNs.copy()\n",
    "PNs2['id_word'] = [anchor.format(val,val) for val in PNs['id_word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(rows = (1, len(PNs2), 1))\n",
    "def showpns(rows = 25): \n",
    "    return PNs2.loc[PNs2.id_text == 'P103121', ['id_word', 'form', 'pos', 'lemma', 'role', 'attribute']][:rows].style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Edges\n",
    "In order to create edges we need to decide what the direction is of the movement. For instance, goods go from a 'source' to an 'intermediary' and from there to a 'recipient'. This example creates two edges: \n",
    "> source => intermediary </br>\n",
    "> intermediary => recipient\n",
    "\n",
    "Each edge is a separate row in a new dataframe, whith four columns: `source`, `target`, `id_text`, and `edgetype`.\n",
    "\n",
    "In order to create an edge we find a name, check its role, and then look for a name with an appropriate role further on in the text. In an Intake document, if we find an `intermediary` (keyword ŋiri₃) we want to find a matching `recipient` (šu ba-ti) - this will be the first PN with role `recipient` after our `intermediary`. If we find a `recipient` we want to find a matching `expender` later in the text. In an Intake document we will not find one, because the `recipient`is found at the end with the keywords šu ba-ti (that edge has already been made when the `source` or the `intermediary` was encountered earlier in the text). In Expenditure documents, however, a list of recipients comes first, each one potentially followed by a `repesentative` or `intermediary` with an `expender` at the end. By  searching only forwards we avoid making double edges.\n",
    "\n",
    "This way we can take care of multiple transaction texts, where we may have multiple pairs of \"source\" and \"intermediary\" (or \"representative\") and one \"recipient\" at the end, or multiple \"recipients\" followed by one \"expender\".\n",
    "\n",
    "The one exception is for the role \"relation\" in a line like: \n",
    "> \tgiri₃ a-bu-du₁₀ lu₂ a-bu-ni-ka\t\tTransporter: Abuṭāb the man of Abuni\n",
    "\n",
    "Here, Abuni has the role 'relation', which always points backwards in the document. The \"relation\" (here: Abuni) is the Source, and the first name encountered before this one is the Target (here Abuṭāb), whatever its role. Since, in this case, Abuṭāb has the role \"intermediary\" he will have three edges ([Trouvaille 86](http://oracc.org/epsd2/admin/ur3/P134759)):\n",
    "\n",
    " | source | target |\n",
    " |--------|-----------|\n",
    " | Abuni  | Abuṭāb |\n",
    " |Šu-Enlil | Abuṭāb |\n",
    " |Abuṭāb | PuzurErra |\n",
    " \n",
    " Šu-Enlil, who sends his goods through Abuṭāb to PuzurErra, is qualified himself as \"son\"of the king\", so that he, in turn, will appear in  another edge in which the king is Source, and Šu-Enlil Target.\n",
    " \n",
    " These edges are of different kinds. By default all edges are of the type \"transaction\", except for those that express a relationship (familial or otherwise); those are marked as \"relation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = []\n",
    "for Pno in ids:                              # one document ID at a time\n",
    "    people = PNs.index[PNs.id_text == Pno]    # indexes of all the people in that document\n",
    "    for p in people:                       # iterate over those indexes\n",
    "        rest = PNs.loc[p+1 : max(people)].index # indexes of all other name instances, later in the document\n",
    "        source = ''\n",
    "        target = ''\n",
    "        edgetype = 'transaction'   # that is the default\n",
    "        role = PNs.loc[p]['role']\n",
    "        if role in ['intermediary', 'representative']:\n",
    "            if treasure.get(Pno) == 'expenditure':\n",
    "                target = PNs.loc[p]['lemma']\n",
    "                q = [n for n in rest if PNs.loc[n]['role'] in ['source', 'expender']]  # look for source after the target\n",
    "                if q:\n",
    "                    source = PNs.loc[q[0]]['lemma'] \n",
    "            else:\n",
    "                source = PNs.loc[p]['lemma']\n",
    "                q = [n for n in rest if PNs.loc[n]['role'] in ['recipient', 'sealer']]  # look for target after the source\n",
    "                if q:\n",
    "                    target = PNs.loc[q[0]]['lemma']  \n",
    "        elif role in ['sender', 'deliverer', 'source', 'producer', 'weigher', 'owner']:\n",
    "            source = PNs.loc[p]['lemma']\n",
    "            q = [n for n in rest if PNs.loc[n]['role'] in ['recipient', 'sealer', 'intermediary']] \n",
    "            if q:\n",
    "                target = PNs.loc[q[0]]['lemma']\n",
    "        elif role == 'relation':\n",
    "            source = PNs.loc[p]['lemma']\n",
    "            q = [n for n in people if n < p]\n",
    "            if q:\n",
    "                target = PNs.loc[q[-1]]['lemma']\n",
    "                edgetype = 'relation'\n",
    "        elif role in ['recipient', 'sealer']:\n",
    "            target = PNs.loc[p]['lemma']\n",
    "            q = [n for n in rest if PNs.loc[n]['role'] in ['representative', 'source', 'intermediary', 'expender']]\n",
    "            if q:\n",
    "                source = PNs.loc[q[0]]['lemma']\n",
    "        elif role in ['reason']:\n",
    "            target = PNs.loc[p]['lemma']\n",
    "            q = [n for n in rest if PNs.loc[n]['role'] in ['recipient']]\n",
    "            if q:\n",
    "                source = PNs.loc[q[0]]['lemma']\n",
    "        elif role == \"offerer\":\n",
    "            source = PNs.loc[p]['lemma']\n",
    "            q = [n for n in rest if PNs.loc[n]['role'] in ['source', 'expender']]\n",
    "            if q: \n",
    "                target = PNs.loc[q[0]]['lemma']\n",
    "        if source and target:\n",
    "            edges.append([source, target, Pno, edgetype])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show results in a table with links for checking\n",
    "The code in the following two cells does not add or change anything, but allows to check the results. In the second cell one may enter a condition to restrict the output to a particular document, or a particular type of edge for instance\n",
    "```python\n",
    "return edgs2.loc[edgs2.duplicated()][:rows].style\n",
    "```\n",
    "to check if there are edges between the same people from the same text. \n",
    "\n",
    "To check for the edges in one particular text, select with `edgs.id_text == 'P######'`, because the field `id_text` in edgs2 includes the full URL:\n",
    "```python\n",
    "return edgs2.loc[edgs.id_text == 'P112515'][:rows].style\n",
    "```\n",
    "To inspect all edges, simply write: \n",
    "```python\n",
    "return edgs2[:rows].style\n",
    "```\n",
    "\n",
    "The Pandas `style` method creates links out of properly formated HTML anchor (\\<a\\>) tags. The links are created by adding the field `id_text` to the URL for the ur3 database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgs = pd.DataFrame(edges)\n",
    "edgs.columns = ['source', 'target', 'id_text', 'edge_type']\n",
    "anchor = '<a href=\"http://build-oracc.museum.upenn.edu/treasury/{}\", target=\"_blank\">{}</a>'\n",
    "edgs2 = edgs.copy()\n",
    "edgs2['id_text'] = [anchor.format(val,val) for val in edgs['id_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(rows = (1, len(edgs2), 1))\n",
    "def showedges(rows = 25): \n",
    "    return edgs2.loc[edgs.id_text == 'P332256'][:rows].style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add edges by hand\n",
    "Complex administrative texts such as UTI 6 3800 ([P141796](http://oracc.org/epsd2/P141796)) and Ist PD 912 ([P332256](http://oracc.org/epsd2/P332256)), or documents that are badly damaged such AUCT 1 276 ([P103121](http://oracc.org/epsd2/P103121) are not parsed well with the routine above. These edges are collected by hand in the file /edges/add_edges.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('edges/add_edges.txt', 'r', encoding='utf8') as e:\n",
    "    add_edges = pd.read_csv(e, sep=',')\n",
    "replace = add_edges.id_text.unique()\n",
    "edgs = edgs.loc[~edgs.id_text.isin(replace)]\n",
    "edgs = edgs.append(add_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicate Edges\n",
    "For various reasons a single document may create duplicate edges. In large summary texts we may encounter the same two people who interact at different points in time and those are true duplicates that should increase the weight of the edge. More common, however, are two other situations. First, illegible or partly legible names may be represented as identical, resulting in multiple transaction between a Mr or Mrs X and a recipient or expender. Second, the same relationship may be expressed multiple times, for instance Me-ištaran dumu-munus lugal (Me-Ištaran the daughter of the king).\n",
    "\n",
    "For the time being, all duplicate edges are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgs = edgs.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Nodes\n",
    "Name instances that are known to refer to the same person should be merged. This may include nicknames, or professional titles such as *sukkalmah*.\n",
    "\n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Edges\n",
    "Edges that connect the same people, but originate in different documents are merged. The number of merged edges becomes the weight of the edge. The field `id_text` now becomes a sequence of P numbers separated by commas, as in: \"P125753,P126577\". Such comma-separated lists can be used in a URL to inspect multiple documents (for instance http://oracc.org/epsd2/admin/ur3/P125753,P126577).\n",
    "\n",
    "Edges are merged naively, without checking for the possibility of namesakes. This issue is not a big deal in this particular archive, but will need more attention in larger text groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgs2 = edgs.groupby(['source', 'target', 'edge_type']).agg({'id_text': list}).reset_index()\n",
    "edgs2['weight'] = [len(i) for i in edgs2['id_text']]\n",
    "edgs2['id_text'] = [','.join(i)for i in edgs2['id_text']]\n",
    "edgs2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Networkx\n",
    "Networkx is the main module in Python for network analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create initial Directed Graph\n",
    "Networkx may create a network directly from the edgelist that was created in Pandas. The node list is created from the edge list. Node attributes need to be added separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.convert_matrix.from_pandas_edgelist(edgs2, source = 'source', target = 'target', \n",
    "                                         edge_attr = ['id_text', 'weight', 'edge_type'],\n",
    "                                        create_using = nx.DiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_attr = {name : name for name in G.nodes}\n",
    "degree = {name : G.degree[name] for name in G.nodes}\n",
    "node_size = {name : G.degree[name]*3 for name in G.nodes}\n",
    "edge_size = [G[a][b]['weight'] for (a,b) in G.edges]\n",
    "nx.set_node_attributes(G, node_attr, \"name\")\n",
    "nx.set_node_attributes(G, degree, \"degree\")\n",
    "nx.set_node_attributes(G, node_size, \"node_size\")\n",
    "eigenvector = nx.eigenvector_centrality(G)\n",
    "#nx.set_edge_attributes(G, edge_size, \"edge_size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Graph for Single Text or Single Node (Ego)\n",
    "For checking the validity of the process above.\n",
    "# TODO\n",
    "* add publication to dropdown.\n",
    "* return list of text ids from Ego network to open editions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_single_text(change):\n",
    "    id_text = dropdown.value\n",
    "    if id_text == 'None':\n",
    "        return\n",
    "    dropdown5.value = id_text\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    s_edges = ((u, v, e) for u,v,e in G.edges(data=True) if id_text in e['id_text'])\n",
    "    graph = nx.Graph(s_edges, create_using = nx.DiGraph())\n",
    "    edge_color = ['blue' if graph[source][target]['edge_type'] == 'relation' \n",
    "                 else 'red' for source, target in graph.edges]\n",
    "    node_color = ['#00ffff' if node.endswith('DN') else '#cc0066' for node in graph.nodes]\n",
    "    nx.draw_circular(graph, with_labels=True,node_size=300, font_size = 12, edge_color=edge_color, \n",
    "                node_color=node_color, width=2.0, alpha=0.8);\n",
    "    with out:\n",
    "        clear_output()\n",
    "        print(f\"Nodes: {str(graph.number_of_nodes())}; Density: {str(nx.density(graph))}\")\n",
    "        print(f\"Edges: {str(graph.number_of_edges())}\")\n",
    "        plt.show();\n",
    "    return\n",
    "def display_ego(change):\n",
    "    Ego = dropdown3.value\n",
    "    if Ego == 'None':\n",
    "        return\n",
    "    radius = dropdown4.value\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    graph = nx.ego_graph(G, Ego, radius=radius, undirected=False)  \n",
    "    eigenv = eigenvector.get(Ego)\n",
    "    documents = list(set(nx.get_edge_attributes(graph, 'id_text').values()))\n",
    "    id_text = ','.join(documents)\n",
    "    dropdown5.value = id_text\n",
    "    edge_color = ['blue' if graph[source][target]['edge_type'] == 'relation' \n",
    "                 else 'red' for source, target in graph.edges]\n",
    "    node_color = ['#00ffff' if node.endswith('DN') else '#cc0066' for node in graph.nodes]\n",
    "    nx.draw_circular(graph, with_labels=True,node_size=300, font_size = 12, edge_color=edge_color, \n",
    "                node_color=node_color, width=2.0, alpha=0.8);\n",
    "    with out:\n",
    "        clear_output()\n",
    "        print(f\"Nodes: {str(graph.number_of_nodes())}; Density: {str(nx.density(graph))}\")\n",
    "        print(f\"Edges: {str(graph.number_of_edges())}; Eigenvector Centrality: {eigenv}\")\n",
    "        print(f\"Documents: {len(documents)}\")\n",
    "        plt.show();\n",
    "    return\n",
    "def open_edition_page(change):\n",
    "    id_text = dropdown5.value\n",
    "    if id_text != 'None':\n",
    "        url = f\"http://build-oracc.museum.upenn.edu/treasury/{id_text}\"\n",
    "        webbrowser.open(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortorder = \" ʾAaāâBbCcDdEeēêFfGgŊŋHhIiīîJjKkLlMmNnOoPpQqRrSsṢṣŠšTtṬṭUuūûVvWwXxYyZz0123456789₀₁₂₃₄₅₆₇₈₉{}[].-\"\n",
    "Egos = sorted(list(G.nodes), key=lambda word: [sortorder.index(c) for c in word]) # use custom sort order\n",
    "button = widgets.Button(description = 'Open Edition in ORACC', layout=Layout(width='180px'))\n",
    "button.style.button_color = \"lightblue\"\n",
    "dropdown = widgets.Dropdown(options = ['None'] + ids, value='None', description = 'Select Pno')\n",
    "dropdown3 = widgets.Dropdown(options = ['None'] + Egos, value='None', description = 'Ego')\n",
    "dropdown4 = widgets.Dropdown(options = range(2,10), value=2, description = 'Radius')\n",
    "dropdown5 = widgets.Text(value='None') # only used to hold id_text; not displayed\n",
    "out = widgets.Output()\n",
    "dropdown.observe(display_single_text, 'value')\n",
    "dropdown3.observe(display_ego, 'value')\n",
    "dropdown4.observe(display_ego, 'value')\n",
    "button.on_click(open_edition_page)\n",
    "row1 = widgets.HBox([dropdown, dropdown3, dropdown4])\n",
    "row2 = widgets.HBox([button, out])\n",
    "col = widgets.VBox([row1, row2])\n",
    "display(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute some node centrality measures\n",
    "We may compute a variety of characteristics of each node in our graph. There are different ways to conceptualize the centrality  of a node in a graph. The number of connections of a node to other nodes in the graph contributes to its *degree centrality*. *In degree centrality* refers to the number of incoming connections (where the node is target), whereas *out degree centrality* relates to the number of outgoing connections (where the node source). *Betweenness centrality* is based on the number of shortest paths between any two nodes in the graph that go through this node. *Eigenvector centrality* is a measure that takes into account the centrality of the nodes that the node under consideration connects to. A node that connects to many low-ranking nodes will receive a low eigenvector centrality score. A node that connects to a few well-connected nodes will receive a high eigenvector centrality score.\n",
    "\n",
    "*Hub* and *authority*, finally, are interrelated measures that were originally developed for ranking web pages. A node with high hub value points towards multiple nodes with high authority value. A node with high authority value has incoming connections from multiple nodes with high hub  value. Hub and authority are thus defined by each other and the process of computing hub and authority values is an iterative one.\n",
    "\n",
    "Each of these computations results in a dictionary in which the nodes are keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvector = nx.eigenvector_centrality(G)\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "in_degree_centrality = nx.in_degree_centrality(G)\n",
    "out_degree_centrality = nx.out_degree_centrality(G)\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "hub, authority = nx.hits(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality = [eigenvector, degree_centrality, in_degree_centrality, out_degree_centrality, closeness_centrality, betweenness_centrality, hub, authority]\n",
    "centrality_labels = [\"eigenvector\", \"degree centrality\", \"in degree centrality\",\n",
    "                     \"out degree centrality\", \"closeness centrality\",\n",
    "                     \"betweenness centrality\", \"hub\", \"authority\"]\n",
    "for idx, m in enumerate(centrality):\n",
    "    s = sorted(m, key=m.get, reverse=True)[:5]\n",
    "    print(f\"{centrality_labels[idx]}: {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cliques\n",
    "Cliques are sets of nodes that are all connected to each other. That is, if we have four nodes, A, B, C, and D, all possible connections exist: \n",
    "> A --- B</br>\n",
    "> A --- C</br>\n",
    "> A --- D</br>\n",
    "> B --- C</br>\n",
    "> B --- D</br>\n",
    "> C --- D</br>\n",
    "\n",
    "Cliques may represent powerful centers in a graph, with  information flowing freely within each clique. Cliques are defined only for undirected graphs, so we first have to transform our graph. In order to compute cliques we need to decide the minimum number of nodes - in this case 4.\n",
    "See https://orbifold.net/default/community-detection-using-networkx/\n",
    "\n",
    "A related concept is the k-clique community. A k-clique community consists of adjacent cliques of at least *k* members. Adjacent cliques share at least *k-1* nodes. As we will see, k-clique communities tend to overlap. That is, there are powerful actors who belong to multiple such communities and connect them to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = nx.DiGraph.to_undirected(G)\n",
    "cliques = sorted(nxcom.k_clique_communities(H, 4), key=len, reverse=True)\n",
    "# Count the communities\n",
    "print(f\"The graph has {len(cliques)} clique communities.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(H, 0, \"clique\")\n",
    "for c, v_c in enumerate(cliques):\n",
    "    for v in v_c:\n",
    "        # Add 1 to save 0 for nodes that are not in a clique\n",
    "        H.nodes[v]['clique'] = c + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_edge_attributes(H, 0, \"clique\")  # default is 0\n",
    "for v, w, in H.edges:\n",
    "    if H.nodes[v]['clique'] == H.nodes[w]['clique']:\n",
    "            # Internal edge, mark with community\n",
    "        H.edges[v, w]['clique'] = H.nodes[v]['clique']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [x for x, y in H.nodes(data=True) if y[\"clique\"] > 0 or y[\"degree\"] > 8] # select nodes that belong to a clique\n",
    "I = H.subgraph(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mult = set()  # Nodes in multiple cliques\n",
    "for v, c in enumerate(cliques):\n",
    "    for clique in cliques[v+1:]:\n",
    "        mult.update(c.intersection(clique))\n",
    "node_size = [1000 if n in mult else 100 for n in I.nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = list(nx.get_node_attributes(I, 'clique').values())\n",
    "#node_color = [colmap[I.nodes[v]['clique']] for v in I.nodes]\n",
    "#edge_color = [colmap[I.edges[v, w]['clique']] for v, w in I.edges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = nx.get_node_attributes(I, 'name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pos = {}\n",
    "pos = nx.spring_layout(I)\n",
    "for k, v in pos.items():\n",
    "    label_pos[k] = (v[0], v[1]+0.03)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.rcParams.update({'figure.figsize': (15, 10)})\n",
    "nx.draw(\n",
    "        I,\n",
    "        pos=pos,\n",
    "        node_color=colors,\n",
    "        #edge_color=edge_color, \n",
    "        node_size = node_size, \n",
    "        with_labels = False,\n",
    "        cmap= plt.cm.jet)\n",
    "nx.draw_networkx_labels(I, label_pos, labels, font_size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliques = nx.find_cliques(H)\n",
    "cliques4 = [clq for clq in cliques if len(clq) >= 5]\n",
    "nodes = set(n for clq in cliques4 for n in clq)\n",
    "print(len(cliques4))\n",
    "h = H.subgraph(nodes)\n",
    "nx.draw(h, with_labels=True)\n",
    "# from https://stackoverflow.com/questions/25222322/networkx-create-new-graph-of-all-nodes-that-are-a-part-of-4-node-clique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Bokeh\n",
    "The networkx module has facilities for displaying a network, but the options are limited. The Bokeh module has a much wider range of possibilities for drawing, inspecting, and exporting a network graph. The `from_networkx` function in Bokeh allows efficient import of nodes and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import output_file, show\n",
    "from bokeh.models import (BoxSelectTool, Circle, EdgesAndLinkedNodes, HoverTool,\n",
    "                          MultiLine, NodesAndLinkedEdges, Plot, Range1d, TapTool,\n",
    "                         BoxZoomTool, ResetTool, OpenURL, CustomJS, Column, SaveTool)\n",
    "from bokeh.palettes import Spectral4\n",
    "from bokeh.plotting import figure, output_notebook\n",
    "from bokeh.models.graphs import from_networkx\n",
    "from bokeh.models import TextInput, Button\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tooltips for Nodes\n",
    "This graph provides tooltips for each node, containing the name and the degree. By computing other characteristics of the node, one may easily display other attributes in the tooltip. \n",
    "\n",
    "The edges are not displayed by default, but light up when the mouse hovers over a node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = Plot(plot_width=900, plot_height=900,\n",
    "            x_range=Range1d(-1.1, 1.1), y_range=Range1d(-1.1, 1.1))\n",
    "plot.title.text = \"Treasure Archive Drehem\"\n",
    "\n",
    "node_hover_tool = HoverTool(tooltips=[(\"name\", \"@name\"), (\"degree\", \"@degree\")])\n",
    "plot.add_tools(node_hover_tool, BoxZoomTool(), ResetTool())\n",
    "\n",
    "graph_renderer = from_networkx(G, nx.spring_layout, scale=1, center=(0, 0))\n",
    "\n",
    "graph_renderer.node_renderer.glyph = Circle(size='node_size', fill_color=Spectral4[0])\n",
    "graph_renderer.edge_renderer.glyph = MultiLine(line_color=Spectral4[1], line_alpha=0.8, line_width=3)\n",
    "plot.renderers.append(graph_renderer)\n",
    "\n",
    "output_file(\"interactive_graphs.html\")\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = figure(plot_width=700, plot_height=700,\n",
    "            x_range=Range1d(-1.1,1.1), y_range=Range1d(-1.1,1.1))\n",
    "\n",
    "node_hover_tool = HoverTool(tooltips=[(\"name\", \"@name\"), (\"degree\", \"@degree\")])\n",
    "plot.add_tools(node_hover_tool)\n",
    "plot.title.text = \"Drehem Treasure Archive: the Nodes\"\n",
    "\n",
    "r = from_networkx(G, nx.circular_layout, scale=1, center=(0,0))\n",
    "\n",
    "r.node_renderer.glyph = Circle(size='node_size', fill_color='#2b83ba')\n",
    "r.node_renderer.hover_glyph = Circle(size='node_size', fill_color='#abdda4')\n",
    "\n",
    "r.edge_renderer.glyph = MultiLine(line_alpha=0, line_width='weight')  # zero line alpha\n",
    "r.edge_renderer.hover_glyph = MultiLine(line_color='#abdda4', line_width=5)\n",
    "\n",
    "r.inspection_policy = NodesAndLinkedEdges()\n",
    "plot.renderers.append(r)\n",
    "\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = Plot(plot_width=1000, plot_height=900,\n",
    "            x_range=Range1d(-2, 2), y_range=Range1d(-2, 2))\n",
    "plot.title.text = \"Drehem Treasure Archive\"\n",
    "\n",
    "graph_renderer = from_networkx(G, nx.circular_layout, scale=1.9, center=(0, 0))\n",
    "#graph_renderer.edge_renderer.data_source.data[\"line_width\"] = [G.get_edge_data(a,b)['weight'] for a, b in G.edges()]\n",
    "#graph_renderer.edge_renderer.glyph.line_width = {'field': 'line_width'}\n",
    "graph_renderer.edge_renderer.glyph.line_alpha = 0.8\n",
    "graph_renderer.node_renderer.selection_glyph = Circle(size='degree', fill_color=Spectral4[2])\n",
    "graph_renderer.node_renderer.hover_glyph = Circle(size='degree', fill_color=Spectral4[1])\n",
    "graph_renderer.node_renderer.glyph = Circle(size='degree', fill_color=Spectral4[0])\n",
    "graph_renderer.edge_renderer.glyph = MultiLine(line_color=\"#CCCCCC\", line_alpha=0.8, line_width = 'weight')\n",
    "graph_renderer.edge_renderer.selection_glyph = MultiLine(line_color=Spectral4[2], line_width = 'weight')\n",
    "graph_renderer.edge_renderer.hover_glyph = MultiLine(line_color=Spectral4[1], line_width =  'weight')\n",
    "\n",
    "graph_renderer.inspection_policy = NodesAndLinkedEdges()\n",
    "graph_renderer.selection_policy = EdgesAndLinkedNodes()\n",
    "\n",
    "node_hover_tool = HoverTool(tooltips=[(\"name\", \"@name\"), ('degree', '@value')])\n",
    "plot.renderers.append(graph_renderer)\n",
    "plot.add_tools(node_hover_tool, BoxZoomTool(), ResetTool())\n",
    "output_file(\"vis/interactive_graphs.html\")\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tooltips for Edges\n",
    "The following code draws a network of the Drehem Treasure Archive. By selecting a node (click on the node) the node and all its direct neighbors are highlighted and the button below the drawing links to the collection of texts in ORACC from which these edges come.\n",
    "# TODO\n",
    "The list of P numbers is currently not unique - a P number may provide more than one relevant edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = Plot(plot_width = 900, plot_height = 900,\n",
    "            x_range = Range1d(-1.1, 1.1), y_range = Range1d(-1.1, 1.1))\n",
    "plot.title.text = \"Drehem Treasure Archive\"\n",
    "\n",
    "plot.add_tools(HoverTool(tooltips = [('Text ID', '@id_text'), ('start', '@start'), ('end', '@end')]), \n",
    "               TapTool(), BoxSelectTool(), ResetTool(), BoxZoomTool(), SaveTool())\n",
    "\n",
    "graph_renderer = from_networkx(G, nx.circular_layout, scale = 1, center = (0, 0))\n",
    "\n",
    "graph_renderer.node_renderer.glyph = Circle(size = 'node_size', fill_color = Spectral4[0])\n",
    "graph_renderer.node_renderer.selection_glyph = Circle(size = 'node_size', fill_color = Spectral4[2])\n",
    "graph_renderer.node_renderer.hover_glyph = Circle(size = 'node_size', fill_color = Spectral4[1])\n",
    "\n",
    "graph_renderer.edge_renderer.glyph = MultiLine(line_color = \"#CCCCCC\", line_alpha = 0.8, line_width = 5)\n",
    "graph_renderer.edge_renderer.selection_glyph = MultiLine(line_color = Spectral4[2], line_width = 5)\n",
    "graph_renderer.edge_renderer.hover_glyph = MultiLine(line_color = Spectral4[1], line_width = 5)\n",
    "\n",
    "graph_renderer.selection_policy = NodesAndLinkedEdges()\n",
    "graph_renderer.inspection_policy = EdgesAndLinkedNodes()\n",
    "\n",
    "plot.renderers.append(graph_renderer)\n",
    "\n",
    "info_text_ids = TextInput(title = 'Text IDs:', value = '')\n",
    "info_start_end = TextInput(title = 'Start => End Values:', value = '')\n",
    "url = \"http://oracc.org/epsd2/admin/ur3/\"\n",
    "esource = graph_renderer.edge_renderer.data_source\n",
    "code = \"\"\"  start_end_values = []\n",
    "            text_ids = []\n",
    "            for(idx in esource.selected.indices)\n",
    "            {\n",
    "                index = esource.selected.indices[idx]\n",
    "                start_end_values.push(esource.data['start'][index] + '=>' + esource.data['end'][index] ); \n",
    "                text_ids.push(esource.data['id_text'][index]) \n",
    "            }\n",
    "            info_start_end.value = String(start_end_values);\n",
    "            info_text_ids.value = String(text_ids);\"\"\"\n",
    "code2 = \"\"\" urlnew = url.concat(info_text_ids.value);\n",
    "            window.open(urlnew)\"\"\"\n",
    "callback = CustomJS(args = dict(esource = esource, \n",
    "                                info_text_ids = info_text_ids, \n",
    "                                info_start_end = info_start_end), \n",
    "                    code = code)\n",
    "plot.select_one(TapTool).callback = callback\n",
    "button = Button(label=\"Click to open text editions\", button_type=\"success\")\n",
    "button.js_on_click(CustomJS(args = dict(url=url, info_text_ids=info_text_ids), \n",
    "                        code=code2))\n",
    "show(Column(plot, info_start_end, info_text_ids, button))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = ipyw.Dropdown(\n",
    "    options=['Addition', 'Multiplication', 'Subtraction'],\n",
    "    value='Addition',\n",
    "    description='Task:',\n",
    ")\n",
    "\n",
    "def on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        print (\"changed to %s\" % change['new'])\n",
    "\n",
    "w.observe(on_change)\n",
    "\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as wg\n",
    "widget_list = []\n",
    "def obs(change):\n",
    "    index = widget_list.index(change.owner)\n",
    "    print(index, change)\n",
    "def fun(n):\n",
    "    global widget_list\n",
    "    for i in range(n):\n",
    "        slider = wg.IntSlider()\n",
    "        widget_list.append(slider)\n",
    "        slider.observe(obs, names='value')\n",
    "    display(wg.VBox(widget_list))\n",
    "fun(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widget_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# create number for each group to allow use of colormap\n",
    "from itertools import count\n",
    "# get unique groups\n",
    "groups = set(nx.get_node_attributes(I,'clique').values())\n",
    "mapping = dict(zip(sorted(groups),count()))\n",
    "nodes = I.nodes()\n",
    "colors = [mapping[I.nodes[n]['clique']] for n in nodes]\n",
    "\n",
    "\n",
    "# drawing nodes and edges separately so we can capture collection for colobar\n",
    "pos = nx.spring_layout(I)\n",
    "ec = nx.draw_networkx_edges(I, pos, alpha=0.2)\n",
    "nc = nx.draw_networkx_nodes(I, pos, nodelist=nodes, node_color=colors, \n",
    "                            with_labels=True, node_size=100, cmap=plt.cm.jet)\n",
    "plt.colorbar(nc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
