{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+3\">4 The Treasure And Shoe Archive from Drehem</font>\n",
    "\n",
    "The Treasure and Shoe archive from Drehem (ancient Puzriš-Dagan) is a relatively small archive (< 300 documents) of administrative texts that date to the 21st century BCE and deal with valuable objects (made of metals and valuable stones), leather objects (shoes, boots, etc.) and weapons. The archive was studied in detail by Paola Paoletti in her book *Der König und sein Kreis* (2012). The present notebook essentially takes the texts edited and studied by Paoletti, assigning roles to personal names in each text and creating edges between actors in the documents. By identifying nodes (people) and edges (connections between people) this notebook creates a social network that may be analyzed with the tools of Social Network Analysis.\n",
    "\n",
    "The present notebook will create the social network and do some initial plotting. The next notebook will focus on the analysis of the network.\n",
    "\n",
    "This notebook goes through several steps: \n",
    "* Data acquisition\n",
    "* Data cleaning\n",
    "* Defining Nodes\n",
    "* Assigning roles\n",
    "* Defining edges\n",
    "* Creating a Network (using networkx)\n",
    "\n",
    "We will pay much attention to checking the validity of the results. At every stage (identifying nodes, identifying edges, and creating the network) there will be the possibility to see the outcome for one single text with a link to the edition of that text.\n",
    "\n",
    "Although the treasure archive uses very standardized terminology (keywords), it is not always possible to determine nodes and edges programmatically. We therefore include the option to list edges by hand in a Comma Separated Values file and add those to the edges that were found computationally.\n",
    "\n",
    "Many of the keywords that determine the roles of actors in Treasury texts are also used in other Puzriš-Dagan files. The structure of the code is such that other archives may be analyzed essentially the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\">4.1 Building the Network</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Necessary Modules\n",
    "Here we import the modules necessary for data acquisition and data cleaning and for assignment of roles and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # this suppresses a warning about pandas from tqdm\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import itertools\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "import statistics\n",
    "from collections import Counter\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout\n",
    "import webbrowser    # for opening web pages\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display, clear_output\n",
    "import re\n",
    "import pickle\n",
    "util_dir = os.path.abspath('../utils')\n",
    "sys.path.append(util_dir)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Corpus\n",
    "\n",
    "A list of P numbers (text ID numbers) of texts that belong to the Treasure/Shoe archive, adjusted from a file kindly made available by Paolo Paoletti, may be found in the directory `csv`. This list contains some additional information, for instance where a text was published, and to which sub-archive it belongs. The latter information may be used later on to separate the Treasure texts from the leather texts (or to color those differently in a plot).\n",
    "\n",
    "One of the texts studied in her book still remains unpublished and is therefore not included (this is U.30117). Several additional Treasury texts were published by [Nawalla Al-Mutawalli and Walther Sallaberger 2018](https://doi-org.libproxy.berkeley.edu/10.1515/za-2017-0101) and one additional leather text is found in [RSO 83 (2010), 345 text 14](http://oracc.org/epsd2/admin/ur3/P433582). Another leather text was identified among the photographs of texts from the Louvre ([AO 11393](http://oracc.org/epsd2/admin/ur3/P493329)) made available on [CDLI](http://cli.ucla.edu/P493329).\n",
    "\n",
    "The corpus used here is, therefore, not exactly identical to what was used in Paoletti's book - but it is very close. At the moment of writing the corpus that is being used includes 298 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'csv/treasury_shoes2.txt'\n",
    "tr_df = pd.read_csv(file, encoding='utf8')\n",
    "ids = list(tr_df['id_text'])\n",
    "ids.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the Corpus\n",
    "When running this notebook for the first time (or after a major update of the Ur III corpus on [epsd2])(http://oracc.org/epsd2/admin/ur3) it is necessary to download the entire Ur III corpus in JSON format. If you already have the file `epsd2-admin-ur3.zip` in the `jsonzip` directory you may comment out the command that does the download, by placing a hashmark (#) in front of it as follows: \n",
    "\n",
    "``` python\n",
    "#oracc_download([project])\n",
    "```\n",
    "\n",
    "Remove the hashmark and run the cell again to download a fresh copy (this is a large file and may take between one and ten minutes, depending on the speed of your computer and your connection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'epsd2/admin/ur3'\n",
    "#oracc_download([project]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Parsejson function\n",
    "\n",
    "The `parsejson()` function used here differs in one way from the ones demonstrated in Chapter 2.1. The code looks for a `key` called `ftype` (field type) among the lemmatization data. The only value that `ftype` may take in this corpus is `yn`, indicating that a word is part of a year name. The field `ftype` enables us to easily exclude year names from our analysis. We will, of course, consider the dating of each text (that is part of the metadata). The vocabulary of the year names, however, is irrelevant for understanding the transactions and connections described in this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsejson(text, meta_d):\n",
    "    l = []\n",
    "    for JSONobject in text[\"cdl\"]:\n",
    "        if \"cdl\" in JSONobject: \n",
    "            l.extend(parsejson(JSONobject, meta_d))\n",
    "        if \"label\" in JSONobject:\n",
    "            meta_d[\"label\"] = JSONobject['label']\n",
    "        if \"f\" in JSONobject:\n",
    "            lemma = JSONobject[\"f\"]\n",
    "            lemma['ftype'] = JSONobject.get('ftype','') # this picks up YN for year name\n",
    "            lemma[\"id_word\"] = JSONobject[\"ref\"]\n",
    "            lemma['label'] = meta_d[\"label\"]\n",
    "            lemma[\"id_text\"] = meta_d[\"id_text\"]\n",
    "            l.append(lemma)\n",
    "        if \"strict\" in JSONobject and JSONobject[\"strict\"] == \"1\":\n",
    "            lemma = dict()\n",
    "            lemma['extent'] = JSONobject['extent']\n",
    "            lemma['scope'] = JSONobject['scope']\n",
    "            lemma['state'] = JSONobject['state']\n",
    "            lemma[\"id_word\"] = JSONobject[\"ref\"]\n",
    "            lemma[\"id_text\"] = meta_d[\"id_text\"]\n",
    "            l.append(lemma)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the Parser on Treasury Texts\n",
    "\n",
    "The variable `ids`, created above, holds all the P numbers of the documents we are interested in. This list is used to define the list of files to be extracted from the `zip` file and to be sent to the parser. Otherwise, the code below is identical to the code in Chapter 2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_l = []\n",
    "meta_d = {\"label\": None, \"id_text\": None}\n",
    "file = f\"jsonzip/{project.replace('/', '-')}.zip\"\n",
    "try:\n",
    "    z = zipfile.ZipFile(file) \n",
    "except:\n",
    "    e = sys.exc_info() # get error information\n",
    "    print(file), print(e[0]), print(e[1]) # and print it\n",
    "    #print(f\"{file} does not exist or is not a proper ZIP file\")\n",
    "files = z.namelist() # list of all the files in the ZIP file\n",
    "files = [name for name in files if name[-12:-5] in ids] # select only those of the treasury/leather archive\n",
    "for filename in tqdm(files, desc = project):\n",
    "    id_text = project + filename[-13:-5] \n",
    "    meta_d[\"id_text\"] = id_text\n",
    "    try:\n",
    "        st = z.read(filename).decode('utf-8')\n",
    "        data_json = json.loads(st)           \n",
    "        lemm_l.extend(parsejson(data_json, meta_d))\n",
    "    except:\n",
    "        e = sys.exc_info() # get error information\n",
    "        print(filename), print(e[0]), print(e[1]) # and print it\n",
    "        #print(f'{id_text} is not available or not complete')\n",
    "st = z.read(f\"{project}/catalogue.json\").decode('utf-8')\n",
    "cat = json.loads(st)\n",
    "cat = cat['members']\n",
    "cat = {key : cat[key] for key in ids}\n",
    "pub = [cat[id_text].get(\"designation\", \"\") for id_text in cat]\n",
    "pub2p = dict(zip(pub, ids))\n",
    "p2pub = dict(zip(ids, pub))                        \n",
    "z.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize year dates\n",
    "\n",
    "In the [ORACC](http://oracc.org/epsd2/admin/ur3) metadata the field `date_of origin` is formatted as `Amar-Suen.05.10.03`, meaning \"5th regnal year of Amar-Suen; month 10, day 3\". The [ORACC](http://oracc.org/epsd2/admin/ur3) metadata are directly imported from [CDLI](http://cdli.ucla.edu). In general, [BDTNS](http://bdtns.filol.csic.es/) metadata tend to be more reliable than [CDLI](http://cdli.ucla.edu) metadata. In the following we will take 2 steps: 1. replace the field `date_of_origin` with the date from the [BDTNS](http://bdtns.filol.csic.es/) catalog, where available and 2. add a field `normdate` (year only) where the year is a numerical value between 1 (year 1 of Urnamma) and 108 (year 24 of Ibbi-Suen). This new field will allow selecting data from a range of years. For unknown years (year name broken or not given) we will use the median value.\n",
    "\n",
    "| king | normalized years | regnal years|\n",
    "| ----- | ---------------- | ---------- |\n",
    "| Ur-Namma    | 1-18  |  18 |\n",
    "| Shulgi      | 19-66 |  48 |\n",
    "| Amar-Suen   | 67-75  |  9 |\n",
    "| Shu-Suen    | 76-84  |  9 |\n",
    "| Ibbi-Suen   | 85-108 | 24 |\n",
    "\n",
    "For unknown years insert the median value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read BDTNS catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['bdtns_no', 'cdli_no', 'publication', 'museum_no', 'date', 'provenance']\n",
    "bdtns_catalogue_data = pd.read_csv('bdtns_metadata/QUERY_catalogue.txt', \n",
    "                                   names = columns, header = None, delimiter='\\t').fillna(\"\")\n",
    "bdtns_dates = dict(zip(bdtns_catalogue_data.cdli_no, bdtns_catalogue_data.date))\n",
    "bdtns_dates = {key : bdtns_dates[key] for key in bdtns_dates.keys() if key in ids}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat BDTNS dates to CDLI standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kings = {'UN' : 'Urnamma', \n",
    "         'SH' : 'Šulgi', \n",
    "         'AS' : 'Amar-Suen', \n",
    "         'SS' : 'Šū-Suen', \n",
    "         'IS' : 'Ibbi-Suen'}\n",
    "bdtns_dates_values = list(bdtns_dates.values())\n",
    "bdtns_dates_values = [f\"{kings.get(val[:2], 'XX')}.{val[2:]}\" for val in bdtns_dates_values]\n",
    "bdtns_dates_values = [val.replace(' - ', '.') for val in bdtns_dates_values]\n",
    "bdtns_dates = dict(zip(bdtns_dates.keys(), bdtns_dates_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace CDLI date by BDTNS date. \n",
    "If there is no corresponding bdtns entry, the CDLI date is left unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in cat: \n",
    "    if key in bdtns_dates: \n",
    "        cat[key][\"date_of_origin\"] = bdtns_dates[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create normalized year dates\n",
    "Define an offset for each king, and add that offset to the regnal year. `Amar-Suen.05` will become 71 (integer). Add the normalized year dates to the `cat` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reigns = {'Ur-Namma' : 0, \n",
    "              'Šulgi' : 18, \n",
    "              'Amar-Suen': 66, \n",
    "              'Šū-Suen' : 75, \n",
    "              'Ibbi-Suen' : 84}\n",
    "normdates = []\n",
    "for key in cat.keys():\n",
    "    date = cat[key]['date_of_origin'].split('.')\n",
    "    king = date[0]\n",
    "    year = date[1]\n",
    "    year = year[:2].strip()\n",
    "    offset = reigns.get(king, 999)\n",
    "    if year.isnumeric():\n",
    "        normdate = int(year) + offset\n",
    "        normdates.append(normdate)\n",
    "    else:\n",
    "        normdate = 999\n",
    "    cat[key]['normdate'] = normdate\n",
    "md = int(statistics.median(normdates))  # median of all known years\n",
    "for key in cat.keys():\n",
    "    if cat[key]['normdate'] >= 999:\n",
    "        cat[key]['normdate'] = md\n",
    "x = [cat[key]['normdate'] for key in cat.keys()]\n",
    "plt.hist([x], bins=5)\n",
    "plt.xlabel(\"year\")\n",
    "plt.ylabel(\"numer of tablets\")\n",
    "plt.title(\"number of tablets per year\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame\n",
    "\n",
    "The `parsejson()` function has filled the list of lists `lemm_l` with data. This list  is read into a DataFrame for further manipulation. We remove commas and spaces from Guide Words and simplify the field `id_text` to read \"P433582\" instead of \"epsd2/admin/ur3/P433582\" (all documents derive from [epsd2/admin/ur3](http://oracc.org/epsd2/admin/ur3)).\n",
    "\n",
    "Finally the code adds a field `id_line` (integer) in order to keep track of the lines in a document. Since lines are meaningful units in Sumerian administrative texts, this will be an important tool. \n",
    "\n",
    "All of these steps are identical (or very similar) to steps explained in Chapter 2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame(lemm_l).fillna('')\n",
    "keep = ['extent', 'scope', 'state', 'id_word', 'id_text', 'form', 'cf', 'gw', 'pos', 'ftype', 'label']\n",
    "words = words[keep]\n",
    "words['gw'] = words['gw'].replace([' ', ','], ['-', ''], regex=True)\n",
    "words['id_text'] = [i[-7:] for i in words['id_text']]\n",
    "words['id_line'] = [int(i.split('.')[1]) for i in words['id_word']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Lemma Field\n",
    "\n",
    "The `lemma` field strings Citation Form, Guide Word, and Part of Speech together and is dealt with in the same way as in Chapter 2.1 Words that remain unlemmatized (e.g. because of illegible or broken signs) are represented by their `form` (transliteration) and numbers receive the POS 'NU'. \n",
    "\n",
    "In the DataFrame (physical) breaks and text divisions (marked by horizontal rulings on the tablet, blank lines, and the like) are preserved in the field `state`. Such demarcations have their own row in the DataFrame and receive their own line ID, but they do not have data in fields such as `form`. In order to keep track of breaks we will define two different types: logical and physical breaks. The keywords break_physical and break_logical are included in the `lemma` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_break = ['illegible', 'traces', 'missing', 'effaced']\n",
    "logical_break = ['other', 'blank', 'ruling']\n",
    "words['lemma'] = words[\"cf\"] + '[' + words[\"gw\"] + ']' + words[\"pos\"]\n",
    "words.loc[words[\"cf\"] == \"\" , 'lemma'] = words['form'] + '[NA]NA'\n",
    "words.loc[words[\"pos\"] == \"n\" , 'lemma'] = words['form'] + '[]NU'\n",
    "words.loc[words[\"state\"].isin(logical_break), 'lemma'] = \"break_logical\"\n",
    "words.loc[words[\"state\"].isin(physical_break), 'lemma'] = \"break_physical\"\n",
    "words.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nodes and Roles\n",
    "## Document Typology\n",
    "\n",
    "Different types of documents will be treated differently. In the Treasure Archive the main document types are Intake, Expenditure, and Transfer.\n",
    "\n",
    "|  type           | keyword         | lemma(s)              | description |\n",
    "|----------------|------------------|-----------------------|-------------|\n",
    "| Intake      | mu-kuₓ(DU)      | mu.DU\\[delivery\\]N      | materials delivered to the administration |\n",
    "| Expenditure | ba-zi or zi-ga  | zig[rise]V/i          | materials expended by the administration |\n",
    "| Transfer    | šu ba-ti        | šu\\[hand\\]N teŋ\\[near\\]V/i | materials transfered from one office to another |\n",
    "\n",
    "In the great majority of cases they can be identified simply by looking  for the keyword lemmas.\n",
    "\n",
    "This typology is discussed in much detail in Paoletti's *Der König und sein Kreis* (2012), Chapters 2.1 and 3.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texttype = []\n",
    "for i in ids:\n",
    "    ttype = ''\n",
    "    text = list(words.loc[words.id_text == i, 'lemma'])\n",
    "    if 'mu.DU[delivery]N' in text:\n",
    "        ttype = 'intake'\n",
    "    elif 'zig[rise]V/i' in text:\n",
    "        ttype = 'expenditure'\n",
    "    elif 'teŋ[near]V/i' in text: \n",
    "        ttype = 'transfer'\n",
    "    texttype.append(ttype)\n",
    "treasure = dict(zip(ids, texttype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harvest Nodes\n",
    "\n",
    "For the node list, select Personal Names (PN) and Royal Names (RN) but skip those that appear in Year Names or Seals. Add lugal (king) sukkalmah (prime minister) and nin (queen), because these actors may be mentioned without personal name. Seals and year names are excluded here because the text of a seal or a year name does not directly reflect the transaction that is recorded in the tablet. Seals do include important information about the identity of the sealer (a father's name, a profession and/or loyalty to a king or another high official). Such information could be harvested for node attributes (but that is not done here). Year names include essential chronological information (that is taken care of elsewhere) - but the proper nouns in year names (the name of the king, or the name of a priestess being elected) are not part of the transaction proper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnamed = {'lugal[king]N', 'nin[queen]N', 'sukkalmah[official]N'}\n",
    "names = set(words.loc[words.pos.isin(['PN', 'RN', 'DN']), 'lemma'])\n",
    "actors = unnamed | names\n",
    "seal = list(words.loc[words.label.str.contains('seal'), 'id_word'])\n",
    "yn = list(words.loc[words.ftype == 'yn', 'id_word'])\n",
    "exclude = seal + yn\n",
    "PNs = words.loc[(words.lemma.isin(actors)) & (~words.id_word.isin(exclude))].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define keywords and the corresponding roles\n",
    "The accounts use key words to indicate the particular role of each person in a document. Some of these key words (or key phrases, in some instances) follow the Proper Noun, others precede it. Thus when the word **ŋiri\\[foot\\]N** immediately precedes a Personal Name, that person is an intermediary (or conveyor). A Personal Name followed by **šu\\[hand\\]N teŋ\\[near\\]V/i** marks the recipient of goods.\n",
    "\n",
    "The keywords are associated with a role in two separate dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_post = {'mu.DU[delivery]N': 'deliverer', 'maškim[administrator]N' : 'representative', \n",
    "               'zig[rise]V/i' : 'expender',\n",
    "               'šu[hand]N teŋ[near]V/i': 'recipient' , 'šu[hand]N us[follow]V/t': 'sender', \n",
    "           'la[hang]V/t' : 'weigher'\n",
    "            }\n",
    "key_pre =  {'ŋiri[foot]N' : 'conveyor', \n",
    "            'arua[offering]N' : 'offerer', 'kišib[seal]N' : 'sealer', \n",
    "            'gabari[copy]N kišib[seal]N' : 'sealer', 'mu.DU[delivery]N' : 'deliverer',\n",
    "            'mu[name]N' : 'reason', 'ki[place]N' : 'source', 'kiŋ[work]N ak[do]V/t' : 'producer', \n",
    "            'niŋgur[property]N' :'owner', 'šag[heart]N la[hang]V/t' : 'weigher'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the Role of each PN\n",
    "The role of a personal name in an administrative texts is determined by keywords before of after the name. The keywords and their corresponding roles are defined in the dictionaries `key_post` and `key_pre` in the preceding cell.\n",
    "\n",
    "The DataFrame `PNs`, created above, is an extract from the DataFrame `words` and therefore uses the same indexes. If the index of a Personal Name in `PNs` is `i`, then the word before that name is `words.loc[i-1]`.\n",
    "\n",
    "We iterate over the index numbers in `PNs` to determine which keywords are found before or after. Before doing so we determine the extent of a document by finding the lowest and highest index number with the same `id_text` as the name instance we are dealing with. Similarly, we can define the extent of the line on which the name is found by finding the lowest and highest index number of the words within the document that share the same `id_line`. With these index numbers we can easily see whether a name is in first position in a line (its index equals the lowest index of that line) or what the last word is in the line in which the name instance appears. We can also check to make sure that when we inspect the next line, this line still belongs to the same document. Working with indexes this way is very fast and flexible - we do not have to look up the P number (in `id_text`) every time, or check the `id_line` field.\n",
    "\n",
    "Because of the structure of Sumerian grammar, keywords that come *after* the name may be separated from their name instance by one or more positions, as in:\n",
    "> \tARAD₂-{d}nanna sukkal maškim\t\t\tAradNanna the prime minister was representative. \n",
    "\n",
    "In practice the keyword is always the last word on the line, or the first on the next line, as in:\n",
    "> \tlu₂-{d}nanna šagina nag-su{ki}-ke₄ \t\tLuNanna the military leader of Nagsu\n",
    "> \tšu ba-ti \t\t\t\t\t\t\t\treceived\n",
    "\n",
    "Therefore, if the PN is the first word on the line, we can check  to see if the last word on the line is a keyword and, if so, assign a role to the PN. We can also look at the first word (or two words) of the next line, to see if there is a keyword in that position. Any word following the PN that is not a keyword is considered an attribute (a father's name, a profession, etc.).\n",
    "\n",
    "If the PN is not the first word on the line (its index number is higher than the lowest index number of that line), one possibility is that it is preceded by a keyword, as in:\n",
    "> \tki lu₂-dingir-ra-ta\t\t\t\t\tfrom Ludingira\n",
    "\n",
    "If there is a preceding keyword, we use the `key_pre` dictionary to assign a role. But a PN later in the line may also be the name of a parent (PN1 dumu PN2) or some other relative, or, occasionally, the name of an employer, as in:\n",
    "> \tgiri₃ a-bu-du₁₀ lu₂ a-bu-ni-ka\t\tConveyor: Abuṭāb the man of Abuni\n",
    "\n",
    "In the case of Abuni, his name is not preceded by a keyword, but there is another PN earlier in the line. Abuni does not have a role in the document directly, but he is involved indirectly by his relation of authority to Abuṭāb. In such cases the role is defined as 'relation'. \n",
    "\n",
    "If none of these methods results in a role assignment, we fall back to default roles. Default roles are different for different document types. Transfer texts (keywords šu ba-ti) and Intake (mu.DU\\[delivery\\]N) may record deliveries by multiple people who are not specifically marked with a keyword. The default in those texts is \"source\". Expenditure texts (zi-ga or ba-zi) may record expenditures to various people, not marked specifically. Here the default is \"recipient\".\n",
    "\n",
    "The code takes care of a few special cases. If a name is preceded by u\\[and\\]CNJ the role of that name is the same as the role of the preceding name. The expression ki PN-še₃ (\"to the place of PN\") appears a few times in this archive, but is otherwise very rare in Ur III. It uses the same keyword ki\\[place\\]N as the very common expression ki PN-ta (\"from the place of PN\") but has the opposite meaning. Since the morphology (-ta vs. -še₃) is not available in the `lemma` column, we have to use the `form` column to check for such instances. The expression kin ak, which precedes the name of a craftsman (finished work of PN) may be found on the line preceding the name.\n",
    "\n",
    "Breaks in the text and horizontal rulings have their own entries in the DataFrame `words`, with separate index numbers and a separate `id_line`. This means that while looking for keywords in a preceding or following line the code will never \"jump\" over such a break. If, for instance, a document has a name instance right before a break and the line right after the break has šu ba-ti, this will not result in the role \"recipient\" for the name instance because the break is the next line. It is possible that a name instance right before or right after a break does not receive a role because key words are missing - and is then assigned a default role. This is a possible source of errors.\n",
    "\n",
    "Much of the code here might be used in the same way for other Ur III archives, but it is likely that there will be other keywords and other special cases that one would need to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_nos(PN_index):\n",
    "    \"\"\"get_index_nos() has one argument, the index number of a personal name. It returns a dictionary ('indexes') \n",
    "    that contains indexes for the first and last word in the line in which the PN appears, the first and last word\n",
    "    in the text, etc. These indexes are used to find key words before or after the PN\"\"\"\n",
    "    indexes = {}\n",
    "    Pno = PNs.loc[i]['id_text']                    # the text ID (P number)\n",
    "    lineno = PNs.loc[i]['id_line']                 # the line number in which the name instance appears\n",
    "    text = words.loc[words.id_text == Pno]                # the entire text\n",
    "    line = text.loc[text.id_line == lineno] #  the line with the PN\n",
    "    indexes[\"Pno\"] = Pno\n",
    "    indexes[\"line\"] = line\n",
    "    indexes[\"first_word_in_line\"] = min(line.index)                 # index no. of first word in line\n",
    "    indexes[\"last_word_in_line\"] = max(line.index)                   # index no. of last word in line\n",
    "    indexes[\"first_word_in_text\"] = min(text.index)                    # index no. of first word in text\n",
    "    indexes[\"last_word_in_text\"] = max(text.index)                    # index no. of last word in text\n",
    "    indexes[\"last_line_in_text\"] = text.loc[max(line.index)]['id_line']          # highest line number in text\n",
    "    if lineno < indexes[\"last_line_in_text\"]:                       # check that next line is still in the same text\n",
    "        next_line_id = text.loc[mxw + 1, 'id_line']  # line id of the word after \n",
    "                                           # the last word of the line\n",
    "        indexes[\"nextline\"] = text.loc[text.id_line == next_line_id] # words in nextline\n",
    "    else:\n",
    "        indexes[\"nextline\"] = pd.DataFrame(columns = words.columns) #empty dataframe\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_keywords_after(i, role, indexes):\n",
    "    first_word_in_text = indexes[\"first_word_in_text\"]\n",
    "    last_word_in_line = indexes[\"last_word_in_line\"]\n",
    "    first_word_in_line = indexes[\"first_word_in_line\"]\n",
    "    line = indexes[\"line\"]\n",
    "    nextline = indexes[\"nextline\"]\n",
    "    role = key_post.get(words.loc[last_word_in_line]['lemma'], role)    # keyword appears in same line in last position\n",
    "    role = key_post.get(words.loc[last_word_in_line+1]['lemma'], role) # or in the next line in first position\n",
    "    if last_word_in_line - first_word_in_line > 1: \n",
    "        lastwords = ' '.join(list(line[-2:]['lemma'])) # two-word keywords (as in šu ba-ti)\n",
    "        role = key_post.get(lastwords, role)\n",
    "    if len(nextline) > 1:               # two-word keyword appearing in first position in next line\n",
    "        firstwords = ' '.join(list(nextline[:2]['lemma']))\n",
    "        role = key_post.get(firstwords, role) \n",
    "    if i > first_word_in_text + 1:  # special case: if kin ak or ša₃ i₃-la₂ is found on preceding line \n",
    "        preceding2words = ' '.join(list(words.loc[i-2:i-1]['lemma']))\n",
    "        role = key_pre.get(preceding2words, role)\n",
    "    return role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_keywords_before(i, role, indexes):\n",
    "    line = indexes[\"line\"]\n",
    "    first_word_in_line = indexes[\"first_word_in_line\"]\n",
    "    last_word_in_line = indexes[\"last_word_in_line\"]\n",
    "    Pno = indexes[\"Pno\"]\n",
    "    firstwords = ' '.join(list(line.loc[first_word_in_line:i-1]['lemma']))  # join all words before PN\n",
    "    role = key_pre.get(firstwords, role)        # ŋiri₃ PN; ki PN-ta; kin ak PN; etc\n",
    "    if line.loc[first_word_in_line]['lemma'] == 'ki[place]N' and line.loc[last_word_in_line]['form'].endswith('-še₃'): \n",
    "        role = 'destination'              # special case: ki PN-še₃         \n",
    "    if role == '':                # role has not been filled yet\n",
    "        PN = [w for w in line.loc[first_word_in_line:i-2]['lemma'] if w in list(PNs['lemma'])] # is there another PN previously in the line?\n",
    "        if PN:                                                                  \n",
    "            if words.loc[i-1]['lemma'] == 'u[and]CNJ':   # PN1 u PN2\n",
    "                if roles:\n",
    "                    role = roles[-1]               # same role as previous PN\n",
    "            else:\n",
    "                role = 'relation'             # as in PN dumu PN\n",
    "        elif treasure.get(Pno) == 'expenditure': # commodities + PN: recipient\n",
    "            role = 'recipient'\n",
    "        else:                              # if there is no preceding PN: drop the name\n",
    "            role = 'none'                     # this may  need refinement\n",
    "        if PNs.loc[i]['lemma'] in unnamed and words.loc[i-1]['lemma'] in list(PNs['lemma']):  \n",
    "            role = 'none'       # 'unnamed' person (e.g. sukkalmah) is preceded by name\n",
    "    return role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roles = []\n",
    "attribute = []\n",
    "for i in PNs.index:                           # the index of PNs is identical to the one of words\n",
    "    indexes = get_index_nos(i)\n",
    "    attr = []\n",
    "    role = ''\n",
    "    # first get attributes\n",
    "    if i < indexes[\"last_word_in_line\"]:  # PN is not the last word of the line\n",
    "        attr = list(words.loc[i+1:indexes[\"last_word_in_line\"]]['lemma'])   # all words following a PN are attributes\n",
    "        attr = [w for w in attr if not w in key_post]  # except for keywords\n",
    "        if len(attr) > 1: \n",
    "            if ' '.join(attr[-2:]) in key_post: # or multi-word keywords\n",
    "                attr = attr[:-2]\n",
    "                \n",
    "    # now assign roles\n",
    "    \n",
    "    if indexes[\"first_word_in_line\"] == i:               # PN in first position\n",
    "        role = check_keywords_after(i, role, indexes)\n",
    "                \n",
    "    elif indexes[\"first_word_in_line\"] < i:             # PN appears further in the line with keyword(s) preceding\n",
    "        role = check_keywords_before(i, role, indexes)\n",
    "    if role == '':                                         # role still empty\n",
    "        if treasure.get(indexes[\"Pno\"]) == 'expenditure': # default role for zi-ga/ba-zi texts\n",
    "            role = 'recipient'\n",
    "        elif treasure.get(indexes[\"Pno\"]) in ['intake', 'transfer']:    # default role for mu-DU and šu ba-ti texts\n",
    "            role = 'source'\n",
    "    roles.append(role)\n",
    "    attribute.append(' '.join(attr))\n",
    "PNs['role'] = roles\n",
    "PNs['attribute'] = attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show results in a table with links for checking\n",
    "The code in the following two cells does not add or change anything, but allows to check the results. One may inspect the entire output, or restrict it to a certain type of role, or one particular text.\n",
    "\n",
    "The Pandas `style` method creates links out of properly formated HTML anchor (\\<a\\>) tags. The links are made by adding the `id_word` field to the URL for the treasury database. This will highlight the word in question. Note that `id_word` is fairly stable, but not absolutely so. If the text has been edited in such a way that there are more (or less) lines on the object or there are more (or less) words in the line, the highlighting may be off.\n",
    "\n",
    "Move the slide to see more (or less) results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = '<a href=\"http://oracc.museum.upenn.edu/epsd2/admin/ur3/{}\", target=\"_blank\">{}</a>'\n",
    "PNs2 = PNs[['id_word', 'form', 'pos', 'lemma', 'role', 'attribute']].copy()\n",
    "PNs2['id_word'] = [anchor.format(val,val) for val in PNs['id_word']]\n",
    "roles = PNs.role.unique()\n",
    "roles.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(rows = (1, len(PNs2), 1), textid = [''] + ids,role=roles)\n",
    "def showpns(rows = 25, textid = '', role=''):\n",
    "    if textid == '' and role == '': \n",
    "        return PNs2[:rows].style\n",
    "    elif textid == '': \n",
    "        return PNs2[PNs.role == role][:rows].style\n",
    "    elif role == '': \n",
    "        return PNs2[PNs.id_text == textid][:rows].style\n",
    "    else: \n",
    "        return PNs2[(PNs.role == role) & (PNs.id_text == textid)][:rows].style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Case: replace `lugal`(king) by the king's name\n",
    "The king is never referred to by name, but alwas as *lugal* (king). Use the filed `date_of_origin` to determine the identity of *lugal* and replace the lemma *lugal\\[king\\]N* by the appropriate Royal Name (in ORACC style)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kings_oracc = {'Ur-Namma' : 'Urnamma[1]RN', \n",
    "              'Šulgi'     : 'Šulgir[1]RN', \n",
    "              'Amar-Suen' : 'Amarsuenak[1]RN', \n",
    "              'Šū-Suen'   : 'Šusuen[1]RN', \n",
    "              'Ibbi-Suen' : 'Ibbisuen[1]RN'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PNs[\"date\"] = [cat[P_no][\"date_of_origin\"] for P_no in PNs.id_text]\n",
    "PNs[\"king\"] = [cat[P_no][\"date_of_origin\"].split('.')[0] for P_no in PNs.id_text]\n",
    "PNs[\"king\"] = [kings_oracc.get(king, 'lugal[king]N') for king in PNs.king]\n",
    "PNs.loc[PNs.lemma == 'lugal[king]N', \"lemma\"] = PNs.king"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Names\n",
    "The following names/titles, all refer to the same person: AradNannak\\[0\\]PN, Aradnanna\\[0\\]PN, Aradŋu\\[0\\]PN, and sukkalmah\\[official\\]N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aradnannak = ['Aradnanna[0]PN', 'Aradŋu[0]PN', 'sukkalmah[official]N']\n",
    "PNs.loc[PNs.lemma.isin(Aradnannak), 'lemma'] = 'AradNannak[0]PN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Edges\n",
    "In order to create edges we need to decide what the direction is of the movement. For instance, goods go from a 'source' to a 'conveyor' and from there to a 'recipient'. This example creates two edges: \n",
    "> source => conveyor </br>\n",
    "> conveyor => recipient\n",
    "\n",
    "Each edge is a separate row in a new dataframe, whith four columns: `source`, `target`, `id_text`, and `edgetype`.\n",
    "\n",
    "In order to create an edge we find a name, check its role, and then look for a name with an appropriate role further on in the text. In an Intake document, if we find an `conveyor` (keyword ŋiri₃) we want to find a matching `recipient` (šu ba-ti) - this will be the first PN with role `recipient` after our `conveyor`. If we find a `recipient` we want to find a matching `expender` later in the text. In an Intake document we will not find one, because the `recipient`is found at the end with the keywords šu ba-ti (that edge has already been made when the `source` or the `conveyor` was encountered earlier in the text). In Expenditure documents, however, a list of recipients comes first, each one potentially followed by a `repesentative` or `conveyor` with an `expender` at the end. By  searching only forwards we avoid making double edges.\n",
    "\n",
    "This way we can take care of multiple transaction texts, where we may have multiple pairs of \"source\" and \"conveyor\" (or \"representative\") and one \"recipient\" at the end, or multiple \"recipients\" followed by one \"expender\".\n",
    "\n",
    "The one exception is for the role \"relation\" in a line like: \n",
    "> \tgiri₃ a-bu-du₁₀ lu₂ a-bu-ni-ka\t\tTransporter: Abuṭāb the man of Abuni\n",
    "\n",
    "Here, Abuni has the role 'relation', which always points backwards in the document. The \"relation\" (here: Abuni) is the Source, and the first name encountered before this one is the Target (here Abuṭāb), whatever its role. Since, in this case, Abuṭāb has the role \"conveyor\" he will have three edges ([Trouvaille 86](http://oracc.org/epsd2/admin/ur3/P134759)):\n",
    "\n",
    " | source | target |\n",
    " |--------|-----------|\n",
    " | Abuni  | Abuṭāb |\n",
    " |Šu-Enlil | Abuṭāb |\n",
    " |Abuṭāb | PuzurErra |\n",
    " \n",
    " Šu-Enlil, who sends his goods through Abuṭāb to PuzurErra, is qualified himself as \"son\"of the king\", so that he, in turn, will appear in  another edge in which the king is Source, and Šu-Enlil Target.\n",
    " \n",
    " These edges are of different kinds. By default all edges are of the type \"transaction\", except for those that express a relationship (familial or otherwise); those are marked as \"relation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = []\n",
    "for Pno in ids:                              # one document ID at a time\n",
    "    people = PNs.index[PNs.id_text == Pno]    # indexes of all the people in that document\n",
    "    for p in people:                       # iterate over those indexes\n",
    "        rest = PNs.loc[p+1 : max(people)].index # indexes of all other name instances, later in the document\n",
    "        source = ''\n",
    "        target = ''\n",
    "        edgetype = 'transaction'   # that is the default\n",
    "        role = PNs.loc[p]['role']\n",
    "        if role in ['conveyor', 'representative']:\n",
    "            if treasure.get(Pno) == 'expenditure':\n",
    "                target = PNs.loc[p]['lemma']\n",
    "                q = [n for n in rest if PNs.loc[n]['role'] in ['source', 'expender']]  # look for source after the target\n",
    "                if q:\n",
    "                    source = PNs.loc[q[0]]['lemma'] \n",
    "            else:\n",
    "                source = PNs.loc[p]['lemma']\n",
    "                q = [n for n in rest if PNs.loc[n]['role'] in ['recipient', 'sealer']]  # look for target after the source\n",
    "                if q:\n",
    "                    target = PNs.loc[q[0]]['lemma']  \n",
    "        elif role in ['sender', 'deliverer', 'source', 'producer', 'weigher', 'owner']:\n",
    "            source = PNs.loc[p]['lemma']\n",
    "            q = [n for n in rest if PNs.loc[n]['role'] in ['recipient', 'sealer', 'conveyor']] \n",
    "            if q:\n",
    "                target = PNs.loc[q[0]]['lemma']\n",
    "        elif role == 'relation':\n",
    "            source = PNs.loc[p]['lemma']\n",
    "            q = [n for n in people if n < p]\n",
    "            if q:\n",
    "                target = PNs.loc[q[-1]]['lemma']\n",
    "                edgetype = 'relation'\n",
    "        elif role in ['recipient', 'sealer']:\n",
    "            target = PNs.loc[p]['lemma']\n",
    "            q = [n for n in rest if PNs.loc[n]['role'] in ['representative', 'source', 'conveyor', 'expender']]\n",
    "            if q:\n",
    "                source = PNs.loc[q[0]]['lemma']\n",
    "        elif role in ['reason']:\n",
    "            target = PNs.loc[p]['lemma']\n",
    "            q = [n for n in rest if PNs.loc[n]['role'] in ['recipient']]\n",
    "            if q:\n",
    "                source = PNs.loc[q[0]]['lemma']\n",
    "        elif role == \"offerer\":\n",
    "            source = PNs.loc[p]['lemma']\n",
    "            q = [n for n in rest if PNs.loc[n]['role'] in ['source', 'expender']]\n",
    "            if q: \n",
    "                target = PNs.loc[q[0]]['lemma']\n",
    "        if source and target:\n",
    "            edges.append([source, target, Pno, edgetype])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show results with links for checking\n",
    "The code in the following two cells does not add or change anything, but allows to check the results. In the second cell one may enter a condition to restrict the output to a particular document, or a particular type of edge for instance\n",
    "```python\n",
    "return edgs2.loc[edgs2.duplicated()][:rows].style\n",
    "```\n",
    "to check if there are edges between the same people from the same text. \n",
    "\n",
    "To check for the edges in one particular text, select with `edgs.id_text == 'P######'`, because the field `id_text` in edgs2 includes the full URL:\n",
    "```python\n",
    "return edgs2.loc[edgs.id_text == 'P112515'][:rows].style\n",
    "```\n",
    "To inspect all edges, simply write: \n",
    "```python\n",
    "return edgs2[:rows].style\n",
    "```\n",
    "\n",
    "The Pandas `style` method creates links out of properly formated HTML anchor (\\<a\\>) tags. The links are created by adding the field `id_text` to the URL for the ur3 database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgs = pd.DataFrame(edges)\n",
    "edgs.columns = ['source', 'target', 'id_text', 'edge_type']\n",
    "anchor = '<a href=\"http://oracc.museum.upenn.edu/epsd2/admin/ur3/{}\", target=\"_blank\">{}</a>'\n",
    "edgs2 = edgs.copy()\n",
    "edgs2['id_text'] = [anchor.format(val,val) for val in edgs['id_text']]\n",
    "target = sorted(list(edgs.target.unique()))\n",
    "source = sorted(list(edgs.source.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(rows = (1, len(edgs2), 1), source = [''] + source, target = [''] + target, textid = [''] + ids)\n",
    "def showedges(rows = 25, source = '', target = '', textid = ''):\n",
    "    if textid == '' and source == '' and target == '': \n",
    "        return edgs2[:rows].style\n",
    "    elif source == '' and target == '': \n",
    "        return edgs2[edgs.id_text == textid][:rows].style\n",
    "    elif source =='' and textid == '': \n",
    "        return edgs2[edgs.target == target][:rows].style\n",
    "    elif target == '' and textid == '': \n",
    "        return edgs2[edgs.source == source][:rows].style\n",
    "    elif target == '': \n",
    "        return edgs2[(edgs.id_text == textid) & (edgs.source == source)][:rows].style\n",
    "    elif source == '': \n",
    "        return edgs2[(edgs.id_text == textid) & (edgs.target == target)][:rows].style\n",
    "    elif textid == '': \n",
    "        return edgs2[(edgs.target == target) & (edgs.source == source)][:rows].style\n",
    "    else: \n",
    "        return edgs2[(edgs.target == target) & (edgs.source == source) &edgs.id_text == textid][:rows].style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add edges by hand\n",
    "Complex administrative texts such as UTI 6 3800 ([P141796](http://oracc.org/epsd2/P141796)) and Ist PD 912 ([P332256](http://oracc.org/epsd2/P332256)), or documents that are badly damaged such AUCT 1 276 ([P103121](http://oracc.org/epsd2/P103121)) are not parsed well with the routine above. These edges are collected by hand in the file /edges/add_edges.txt. The routine will note if a node is not currently in the PNs dataframe. If so, this may indicate an error somewhere (in the `add-edges.txt` or in the name authority), or it could simply be a name that had not been recognized so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('edges/add_edges.txt', 'r', encoding='utf8') as e:\n",
    "    add_edges = pd.read_csv(e, sep=',')\n",
    "add_source = list(add_edges.source.unique())\n",
    "add_target = list(add_edges.target.unique())\n",
    "add = list(set(add_source + add_target))\n",
    "for a in add: \n",
    "    if not a in PNs.lemma.unique(): \n",
    "        print(f\"{a} is not recognized\")\n",
    "replace = add_edges.id_text.unique()\n",
    "edgs = edgs.loc[~edgs.id_text.isin(replace)]\n",
    "edgs = edgs.append(add_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Duplicate Edges\n",
    "For various reasons a single document may create duplicate edges. In large summary texts we may encounter the same two people who interact at different points in time and those are true duplicates that should increase the weight of the edge. More common, however, are two other situations. First, illegible or partly legible names may be represented as identical, resulting in multiple transaction between a Mr or Mrs X and a recipient or expender. Second, the same relationship may be expressed multiple times, for instance Me-ištaran dumu-munus lugal (Me-Ištaran the daughter of the king).\n",
    "\n",
    "For the time being, all duplicate edges are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgs = edgs.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Edges\n",
    "\n",
    "Edges that connect the same people, but originate in different documents are merged. The number of merged edges becomes the weight of the edge. The field `id_text` now becomes a list of P numbers.\n",
    "\n",
    "Edges are merged naively, without checking for the possibility of namesakes. This issue is not a big deal in this particular archive, but will need more attention in larger text groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgs['normdate'] = [cat[i]['normdate'] for i in edgs['id_text']]\n",
    "edgs2 = edgs.groupby(['source', 'target', 'edge_type']).agg({'id_text' : list, 'normdate' : list}).reset_index()\n",
    "edgs2['weight'] = [len(i) for i in edgs2['id_text']]\n",
    "edgs2['min_date'] = [min(date) for date in edgs2['normdate']]\n",
    "edgs2['max_date'] = [max(date) for date in edgs2['normdate']]\n",
    "edgs2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create initial Directed Graph\n",
    "Networkx may create a network directly from the edgelist that was created in Pandas. The node list is created from the edge list. Node attributes need to be added separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.convert_matrix.from_pandas_edgelist(edgs2, source = 'source', target = 'target', \n",
    "                                         edge_attr = ['id_text', 'weight', 'edge_type', 'min_date', 'max_date'],\n",
    "                                        create_using = nx.DiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {'PN' : '#525252', 'DN' :'#00ffff', 'RN' : '#8469e7'}\n",
    "node_attr = {name : name for name in G.nodes}\n",
    "degree = {name : G.degree[name] for name in G.nodes}\n",
    "node_size = {name : G.degree[name]*3 for name in G.nodes}\n",
    "node_color = {name: colors.get(name[-2:], '#cc0066') for name in G.nodes}\n",
    "edge_size = {(a, b) : G[a][b]['weight'] for (a,b) in G.edges}\n",
    "edge_color = {(source, target): 'blue' if G[source][target]['edge_type'] == 'relation' \n",
    "                 else 'red' for source, target in G.edges}\n",
    "nx.set_node_attributes(G, node_attr, \"name\")\n",
    "nx.set_node_attributes(G, degree, \"degree\")\n",
    "nx.set_node_attributes(G, node_size, \"node_size\")\n",
    "nx.set_node_attributes(G, node_color, \"node_color\")\n",
    "nx.set_edge_attributes(G, edge_color, \"edge_color\")\n",
    "nx.set_edge_attributes(G, edge_size, \"edge_size\")\n",
    "eigenvector = nx.eigenvector_centrality(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Graph for Single Text or Single Node (Ego)\n",
    "In order to check the validity of the process above the code below allows the user to display the network for a single text, or to display the ego-network of a single node. When displaying these sub-networks, one may open the edition or editions of the document(s) involved in [ORACC](http://oracc.org) by clicking on a button.\n",
    "\n",
    "In order to do so we connect a set of widgets with three functions: `display_single_text()`, `display_ego()`, and `open_edition_page()`. \n",
    "\n",
    "### display_single_text()\n",
    "\n",
    "The `display_single_text()` function is called when a dropdown menu is changed. There are two such dropdown menus, one where the user may select a P number (the text ID) and one with publication numbers. Publication numbers and ID numbers may be mapped to one another by two dictionaries (`p2pub` and `pub2p`) so that the two dropdowns change in unison.\n",
    "\n",
    "The function reads the new value of the dropdown in the variable `id_text` and uses that value to filter the edges in the main graph G, as follows:\n",
    "```python\n",
    "s_edges = ((s, t, a) for s,t,a in G.edges(data=True) if id_text in a['id_text'])\n",
    "```\n",
    "The expression `G.edges(data=True)` (or `G.edges.data()`) returns a list of tuples, where each tuple has the format `(source, target, edge-attributes)`. The third element of the tuple (the edge attributes) has the form of a dictionary. As a result, `s_edges` is a subset of the edges in G, that only contains the edges that are available in that one single document. Note that the edge attribute \"id_text\" is a string that may contain more than one P number (because the same source, target combination may be attested in multiple documents). That is why the consition is formed with an `in` expression (`if id_text in a['id_text']`).\n",
    "\n",
    "This sub-set of edges is than used to crreate a new (directed) graph. To make the graph more informative we define edge colors (blue for relations and red for transactions) and node colors (purple for human beings and cyan for divine beings). The graph is then drawn as a circular graph (`nx.draw_circular()`; many other layout options are available), specifying labels, node colors, etc. Behind the scenes, the actual drawing is done by matplotlib, so we initiate the figure with `plt.figure()`.\n",
    "\n",
    "The graph of a single text (or any graph in NetworkX) has a number of characteristics that we can call and print as additional information, including the number of nodes, the number of edges, and the density of the graph (which percentage of the potential number of edges is realized?).\n",
    "\n",
    "### display_ego()\n",
    "\n",
    "This function is called when one of the dropdown menus \"Ego\" or \"radius\" is changed. In order to display an Ego network we may use the `ego_graph()` function in NetworkX. The value of Ego (the name of the actor at the center of the Ego network) is read from the dropdown menu (`dropdown3`). The radius defaults to 2, but may get any value between 1 (only direct neighbors) to 10. The P numbers of all the texts that contain this name in an edge are listed in the variable `textids` (a list) and this list is transformed to a single string and assigned to the key \"id_text\" in the dictionary `p_nos`. This dictionary is used in the function `open_edition_page()` to open one or more texts in [ORACC](http://oracc.org). In the graph, nodes and edges are colored in the same way as in `display_single_text()`. The graph G is a directed graph, but the function `ego_graph()` needs the option `undirected = True`, otherwise only outgoing edges are included in the Ego network. This does not change the network itself into an undirected network.\n",
    "\n",
    "### open_edition_page()\n",
    "This function is called when one clicks the button \"Open Edition in ORACC\". The function reads the key `id_text` in the dictionary `pnos`. The key is initialized with the value \"None\" (a string). When the functions `display_ego()` or `display_single_text()` are called the key will receive a value that either consists of a single P number, or a sequence of P numbers, separated by commas. This value is used in the URL that is the sole argument of `webbrowser.open()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_single_text(change):\n",
    "    if change.get(\"owner\") == dropdown: # if the change came from the first dropdown menu\n",
    "        id_text = dropdown.value\n",
    "        dropdown2.unobserve(display_single_text, 'value')\n",
    "        dropdown2.value = p2pub.get(dropdown.value, \"None\") \n",
    "        dropdown2.observe(display_single_text, 'value')\n",
    "    elif change.get(\"owner\") == dropdown2: # if the change came from the 2nd dropdown menu\n",
    "        id_text = pub2p.get(dropdown2.value, \"None\")\n",
    "        dropdown.unobserve(display_single_text, 'value')\n",
    "        dropdown.value = id_text\n",
    "        dropdown.observe(display_single_text, 'value')\n",
    "    pnos[\"id_text\"] = id_text\n",
    "    if id_text == 'None':    \n",
    "        with out:\n",
    "            clear_output()\n",
    "        return\n",
    "    s_edges = [(s, t) for s,t,a in G.edges(data=True) if id_text in a['id_text']]\n",
    "    graph = G.edge_subgraph(s_edges)\n",
    "    node_c = nx.get_node_attributes(graph, 'node_color').values()\n",
    "    edge_c = nx.get_edge_attributes(graph, 'edge_color').values()\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.gca().margins(0.15, 0.15) # provide margins to prevent labels from going over the edge\n",
    "    nx.draw_circular(graph, with_labels=True,node_size=300, font_size = 12, edge_color=edge_c, \n",
    "                node_color=node_c, width=2.0, alpha=0.8, arrows=True);\n",
    "    with out:\n",
    "        clear_output()\n",
    "        print(f\"Nodes: {graph.number_of_nodes()}; Density: {nx.density(graph): .4f}\")\n",
    "        print(f\"Edges: {graph.number_of_edges()}\")\n",
    "        plt.show();\n",
    "    return\n",
    "def display_ego(change):\n",
    "    Ego = dropdown3.value\n",
    "    radius = dropdown4.value\n",
    "    if Ego == 'None':\n",
    "        with out:\n",
    "            clear_output()\n",
    "        return\n",
    "    graph = nx.ego_graph(G, Ego, radius=radius, undirected=True)  # undirected = False results in an Ego network that\n",
    "                                                                  # only includes outgoing edges.\n",
    "    eigenv = eigenvector.get(Ego)\n",
    "    textids = nx.get_edge_attributes(graph, 'id_text').values()   # id_text is a list, textids is a list of lists\n",
    "    textids = set(itertools.chain.from_iterable(textids))         # from_iterable pulls all items from a list of lists\n",
    "    pnos[\"id_text\"] = ','.join(textids)\n",
    "    node_c = nx.get_node_attributes(graph, 'node_color').values()\n",
    "    edge_c = nx.get_edge_attributes(graph, 'edge_color').values()\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.gca().margins(0.15, 0.15) # provide margins to prevent labels from going over the edge\n",
    "    nx.draw_circular(graph, with_labels=True,node_size=300, font_size = 12, edge_color=edge_c, \n",
    "                node_color=node_c, width=2.0, alpha=0.8, arrows=True);\n",
    "    with out:\n",
    "        clear_output()\n",
    "        print(f\"Nodes: {graph.number_of_nodes()}; Density: {nx.density(graph): .4f}\")\n",
    "        print(f\"Edges: {graph.number_of_edges()}; Eigenvector Centrality: {eigenv: .4f}\")\n",
    "        print(f\"Documents: {len(textids)}\")\n",
    "        plt.show();\n",
    "    return\n",
    "def open_edition_page(change):\n",
    "    if not pnos.get(\"id_text\") == \"None\":\n",
    "        url = f\"http://oracc.museum.upenn.edu/epsd2/admin/ur3/{pnos['id_text']}\"\n",
    "        webbrowser.open(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Display the Widgets\n",
    "The code in the following cell creates the widgets (the dropdown menus etc.), links those to the functions created in the previous cell, and displays the results. The dropdown menus for \"Publication\" and \"Ego\" use a custom sort order that ignores case and places the special characters (such as š and ṭ) in the correct position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortorder = \" []'ʾaāâbcdeēêfgŋhiīîjklmnopqrsṣštṭuūûvwxyz0123456789₀₁₂₃₄₅₆₇₈₉ₓ{}[]().-+/~?@×|&'<>,§\"\n",
    "Egos = sorted(list(G.nodes), key=lambda word: [sortorder.index(c.casefold()) for c in word]) # use custom sort order\n",
    "publications = sorted(list(pub2p.keys()), key=lambda word: [sortorder.index(c.casefold()) for c in word])\n",
    "button = widgets.Button(description = 'Open Edition in ORACC', layout=Layout(width='180px'))\n",
    "button.style.button_color = \"lightblue\"\n",
    "dropdown = widgets.Dropdown(options = ['None'] + ids, value='None', description = 'Select Pno')\n",
    "dropdown2 = widgets.Dropdown(options = ['None'] + publications, value = 'None', description = \"Publication\")\n",
    "dropdown3 = widgets.Dropdown(options = ['None'] + Egos, value='None', description = 'Ego')\n",
    "dropdown4 = widgets.Dropdown(options = range(1,10), value=2, description = 'Radius')\n",
    "pnos = {\"id_text\" : \"None\"} # dictionary for storing P numbers; to be filled by the functions attached to the widgets.\n",
    "out = widgets.Output()\n",
    "dropdown.observe(display_single_text, 'value')\n",
    "dropdown2.observe(display_single_text, 'value')\n",
    "dropdown3.observe(display_ego, 'value')\n",
    "dropdown4.observe(display_ego, 'value')\n",
    "button.on_click(open_edition_page)\n",
    "col1 = widgets.VBox([dropdown, dropdown2, button])\n",
    "col2 = widgets.VBox([dropdown3, dropdown4])\n",
    "row = widgets.HBox([col1, col2])\n",
    "col = widgets.VBox([row, out])\n",
    "display(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Graph\n",
    "Save the graph for use in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(G, open('pickle/treasury.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
