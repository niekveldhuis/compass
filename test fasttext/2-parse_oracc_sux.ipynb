{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Represent ORACC Texts in UTF-8 Cuneiform\n",
    "The code in this notebook will parse [ORACC](http://oracc.org) `JSON` files to extract signs from the Sumerian texts of one or more projects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import json\n",
    "import tqdm\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import re\n",
    "util_dir = os.path.abspath('../utils')\n",
    "sys.path.append(util_dir)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Create Directories, if Necessary\n",
    "The two directories needed for this script are `jsonzip` and `output`. If they do not exist they are created, else: do nothing.\n",
    "\n",
    "For the code, see [Stack Overflow](http://stackoverflow.com/questions/18973418/os-mkdirpath-returns-oserror-when-directory-does-not-exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('jsonzip', exist_ok = True)\n",
    "os.makedirs('output', exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Input Project Names\n",
    "Provide a list of one or more project names, separated by commas. Note that subprojects must be listed separately, they are not included in the main project. For instance:\n",
    "\n",
    "`epsd2/admin/ed3a, epsd2/admin/ed3b, epsd2/admin/ebla, epsd2/admin/oakk, epsd2/admin/lagash2, epsd2/admin/u3adm, epsd2/admin/u3let, epsd2/admin/u3leg, epsd2/admin/oldbab, epsd2/literary, epsd2/emesal, epsd2/earlylit, epsd2/royal, epsd2/praxis, epsd2/praxis/liturgy, dcclt, dcclt/nineveh, dcclt/signlists, obmc, ckst, blms`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = input('Project(s): ').lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Split the List of Projects and Download the ZIP files.\n",
    "Split the list of projects and create a list of project names, using the `format_project_list()` and `oracc_download()` functions in the `utils` module. The code of these functions is discussed in more detail in 2.1.0. Download ORACC JSON Files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving http://oracc.org/epsd2/literary/json/epsd2-literary.zip as jsonzip/epsd2-literary.zip.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df52eb5a67174639a030eb00206b5f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epsd2/literary: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['epsd2/literary']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = [\"epsd2/literary\"]\n",
    "#p = format_project_list(projects)\n",
    "oracc_download(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"head21\"></a>2.1 The `parsejson()` function\n",
    "The `parsejson()` function will \"dig into\" the `json` file (transformed into a dictionary) until it finds the relevant data. The `json` file consists of a hierarchy of `cdl` nodes; only the lowest nodes contain lemmatization data. The function goes down this hierarchy by calling itself when another `cdl` node is encountered. For nore information about the data hierarchy in the [ORACC](http://oracc.org) `json` files, see [ORACC Open Data](http://oracc.museum.upenn.edu/doc/opendata/index.html).\n",
    "\n",
    "The argument of the `parsejson()` function is a `JSON` object, a dictionary that initially contains the entire contents of the original JSON file. The code takes the key `cdl` which itself contains an array (a list) of `JSON` objects. Iterating through these objects, if an object contains another `cdl` node, the function calls itself with this object as first argument. This way the function digs deeper and deeper into the `JSON` tree, until it does not encounter a `cdl` key anymore. Here we are at the level of individual words. The code checks for a key `f`, if it exists the signs are in the node `gdl` within the `f` node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsejson_signs(text):\n",
    "    for JSONobject in text[\"cdl\"]:\n",
    "        field = ''\n",
    "        if \"cdl\" in JSONobject: \n",
    "            parsejson_signs(JSONobject)\n",
    "        if \"type\" in JSONobject and JSONobject[\"type\"] == \"field-start\":\n",
    "            field = JSONobject[\"subtype\"]\n",
    "        if \"f\" in JSONobject and not field in ['sg', 'pr']: # skip the fields \"sign\" and \"pronunciation\"\n",
    "                                # in lexical texts\n",
    "            if JSONobject[\"f\"][\"lang\"][:3] == \"sux\": #only Sumerian and Emesal\n",
    "                word = JSONobject[\"f\"]\n",
    "                f = word[\"form\"]\n",
    "                if \"sexified\" in word[\"gdl\"][0]:\n",
    "                    f = word[\"gdl\"][0][\"sexified\"]\n",
    "                if \"cf\" in word:\n",
    "                    if 'pos' in word:  #for some reason some words appear without pos. Provisionally treated as Noun\n",
    "                        lemm = word[\"cf\"] + '[' + word[\"gw\"] + \"]\" + word[\"pos\"]\n",
    "                    else:\n",
    "                        lemm = word[\"cf\"] + '[' + word[\"gw\"] + \"]N\"\n",
    "                    lemm = lemm.replace(' ', '-') # remove commas and spaces from lemm\n",
    "                    lemm = lemm.replace(',', '')\n",
    "                else:\n",
    "                    lemm = word[\"form\"] # if word is unlemmatized\n",
    "                all_.append(f)\n",
    "                lemm_.append(lemm)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Call the `parsejson()` function for every `JSON` file\n",
    "The code in this cell will iterate through the list of projects entered above (1.1). For each project the `JSON` zip file is located in the directory `jsonzip`, named PROJECT.zip. \n",
    "\n",
    "Each of these files is extracted from the `zip` file and read with the command `json.loads()`, which reads the json data and transforms it into a Python dictionary (a sequence of keys and values).\n",
    "\n",
    "This dictionary, which is called `text` is now sent to the `parsejson()` function. The function adds signs to the `sign_l` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02dec8c277ed45898de380fcae65e500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epsd2/literary:   0%|          | 0/915 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_ = []\n",
    "lemm_ = []\n",
    "ids_ = []\n",
    "for project in p:\n",
    "    file = \"jsonzip/\" + project.replace(\"/\", \"-\") + \".zip\"\n",
    "    try:\n",
    "        z = zipfile.ZipFile(file)       # create a Zipfile object\n",
    "    except:\n",
    "        print(file + \" does not exist or is not a proper ZIP file\")\n",
    "        continue\n",
    "    files = z.namelist()     # list of all the files in the ZIP\n",
    "    files = [name for name in files if \"corpusjson\" in name and name[-5:] == '.json']                                                                                                  #that holds all the P, Q, and X numbers.\n",
    "    for filename in tqdm(files, desc = project):                            #iterate over the file names\n",
    "        id_no = filename[-13:-5]\n",
    "        if id_no in ids_ and not \"X\" in id_no: # Check if P/Q number is already in there\n",
    "            continue        # a text may appear in multiple projects\n",
    "        id_text = project + id_no # id_text is, for instance, blms/P414332\n",
    "        try:\n",
    "            text = z.read(filename).decode('utf-8')         #read and decode the json file of one particular text\n",
    "            data_json = json.loads(text)                # make it into a json object (essentially a dictionary)\n",
    "            all_.append('Start'+id_text)\n",
    "            lemm_.append('Start'+id_text)   # to keep all_ and lemm_ same length\n",
    "            parsejson_signs(data_json)\n",
    "            ids_.append(id_no)\n",
    "            #print(filename)\n",
    "        except:\n",
    "            print(id_text + ' is not available or not complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Data Structuring\n",
    "### 3.1 Transform the Data into a DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "294dd52adedd479ba269625a3f123c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/195689 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words_l = []\n",
    "separators = ['{', '}', '-']\n",
    "separators2 = ['.', '+', '|']\n",
    "operators = ['&', '%', '@', 'Ã—']\n",
    "for e in tqdm(all_):\n",
    "    word = []\n",
    "    if '1(Å¡arâ‚‚{gal})' in e: # this cheating but it seems to work (appears in SKL 38)\n",
    "            e = e.replace('1(Å¡arâ‚‚{gal})', '1(Å¡arâ‚‚)-gal')\n",
    "    for s in separators: # first split word into signs   \n",
    "        e = e.replace(s, ' ').strip()\n",
    "    s_l = e.split()\n",
    "    for sign in s_l:\n",
    "        if sign[0].isdigit(): # 1(geÅ¡â‚‚), 2(DIÅ ), etc.\n",
    "            sign = sign.lower()\n",
    "        elif sign[-1] == ')': # qualified sign - get only the qualifier\n",
    "            stack = []  # |GIÅ Ã—(GIÅ %GIÅ )|(LAK277) becomes LAK277\n",
    "            ind = {}    # LAK277(|GIÅ Ã—(GIÅ %GIÅ )|) becomes |GIÅ Ã—(GIÅ %GIÅ )|\n",
    "            for i, c in reversed(list(enumerate(sign))):\n",
    "                if c == ')':\n",
    "                    stack.append(i)\n",
    "                if c == '(':\n",
    "                    ind[stack.pop()] = i   # find the opening parens that belongs to the closing parens at position -1    \n",
    "            start = ind[len(sign)-1]   # this line fails on 1(Å¡arâ‚‚{gal}) in SKL.\n",
    "            t = sign[start+1:-1]\n",
    "            if t.isupper(): #leave 1(diÅ¡) etc. alone\n",
    "                sign = t\n",
    "            \n",
    "        if '|' in sign:  # separate |DU.DU| and |DU+DU| into its components but not |DU&DU|\n",
    "                        # and also not |DU.DU&DU|\n",
    "            flag = False\n",
    "            for o in operators:\n",
    "                if o in sign:\n",
    "                    flag = True\n",
    "            if not flag:\n",
    "                for s in separators2:\n",
    "                    sign = sign.replace(s, ' ').strip() \n",
    "                sign_l = sign.split()\n",
    "                word.extend(sign_l)\n",
    "                continue\n",
    "        elif \"+\" in sign:  # + as marker of gloss\n",
    "            sign = sign.replace('+', ' ').strip()\n",
    "            sign_l = sign.split()\n",
    "            word.extend(sign_l)\n",
    "            continue\n",
    "        word.append(sign)\n",
    "    words_l.append(word)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/ogsl.p\", \"rb\") as f:\n",
    "    o = pd.read_pickle(f, compression = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = list(o[\"value\"])\n",
    "utf = list(o[\"utf8\"])\n",
    "names = list(o[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict(zip(names, utf))\n",
    "d2 = dict(zip(val,names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32042685877340f3b71f9fd8542618f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/195689 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "names_l = []\n",
    "utf8_l = []\n",
    "for w in tqdm(words_l):\n",
    "    seq = [d2[s.lower()] if s.lower() in d2 else s for s in w]\n",
    "    names_l.append(seq)\n",
    "    utf8 = [d[n] if n in d else n for n in seq]\n",
    "    utf8_l.append(''.join(utf8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transliteration</th>\n",
       "      <th>words</th>\n",
       "      <th>names</th>\n",
       "      <th>utf-8</th>\n",
       "      <th>lemm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Startepsd2/literary/Q000802</td>\n",
       "      <td>[Startepsd2/literary/Q000802]</td>\n",
       "      <td>[Startepsd2/literary/Q000802]</td>\n",
       "      <td>Startepsd2/literary/Q000802</td>\n",
       "      <td>Startepsd2/literary/Q000802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nir-Å‹alâ‚‚-e</td>\n",
       "      <td>[nir, Å‹alâ‚‚, e]</td>\n",
       "      <td>[|NUN&amp;NUN|, IG, E]</td>\n",
       "      <td>ð’‰ªð’……ð’‚Š</td>\n",
       "      <td>nirÅ‹al[authoritative]AJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a-na</td>\n",
       "      <td>[a, na]</td>\n",
       "      <td>[A, NA]</td>\n",
       "      <td>ð’€€ð’ˆ¾</td>\n",
       "      <td>ana[what?]QP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>biâ‚‚-in-dugâ‚„</td>\n",
       "      <td>[biâ‚‚, in, dugâ‚„]</td>\n",
       "      <td>[NE, IN, KA]</td>\n",
       "      <td>ð’‰ˆð’…”ð’…—</td>\n",
       "      <td>dug[speak]V/t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nu-sagâ‚‰</td>\n",
       "      <td>[nu, sagâ‚‰]</td>\n",
       "      <td>[NU, Å Aâ‚†]</td>\n",
       "      <td>ð’‰¡ð’Š·</td>\n",
       "      <td>sag[good]V/i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195684</th>\n",
       "      <td>rib-x</td>\n",
       "      <td>[rib, x]</td>\n",
       "      <td>[KAL, X]</td>\n",
       "      <td>ð’†—X</td>\n",
       "      <td>rib-x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195685</th>\n",
       "      <td>Å¡i-in-x</td>\n",
       "      <td>[Å¡i, in, x]</td>\n",
       "      <td>[IGI, IN, X]</td>\n",
       "      <td>ð’…†ð’…”X</td>\n",
       "      <td>Å¡i-in-x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195686</th>\n",
       "      <td>x</td>\n",
       "      <td>[x]</td>\n",
       "      <td>[X]</td>\n",
       "      <td>X</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195687</th>\n",
       "      <td>x</td>\n",
       "      <td>[x]</td>\n",
       "      <td>[X]</td>\n",
       "      <td>X</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195688</th>\n",
       "      <td>x</td>\n",
       "      <td>[x]</td>\n",
       "      <td>[X]</td>\n",
       "      <td>X</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195689 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    transliteration                          words  \\\n",
       "0       Startepsd2/literary/Q000802  [Startepsd2/literary/Q000802]   \n",
       "1                        nir-Å‹alâ‚‚-e                 [nir, Å‹alâ‚‚, e]   \n",
       "2                              a-na                        [a, na]   \n",
       "3                       biâ‚‚-in-dugâ‚„                [biâ‚‚, in, dugâ‚„]   \n",
       "4                           nu-sagâ‚‰                     [nu, sagâ‚‰]   \n",
       "...                             ...                            ...   \n",
       "195684                        rib-x                       [rib, x]   \n",
       "195685                      Å¡i-in-x                    [Å¡i, in, x]   \n",
       "195686                            x                            [x]   \n",
       "195687                            x                            [x]   \n",
       "195688                            x                            [x]   \n",
       "\n",
       "                                names                        utf-8  \\\n",
       "0       [Startepsd2/literary/Q000802]  Startepsd2/literary/Q000802   \n",
       "1                  [|NUN&NUN|, IG, E]                          ð’‰ªð’……ð’‚Š   \n",
       "2                             [A, NA]                           ð’€€ð’ˆ¾   \n",
       "3                        [NE, IN, KA]                          ð’‰ˆð’…”ð’…—   \n",
       "4                           [NU, Å Aâ‚†]                           ð’‰¡ð’Š·   \n",
       "...                               ...                          ...   \n",
       "195684                       [KAL, X]                           ð’†—X   \n",
       "195685                   [IGI, IN, X]                          ð’…†ð’…”X   \n",
       "195686                            [X]                            X   \n",
       "195687                            [X]                            X   \n",
       "195688                            [X]                            X   \n",
       "\n",
       "                               lemm  \n",
       "0       Startepsd2/literary/Q000802  \n",
       "1           nirÅ‹al[authoritative]AJ  \n",
       "2                      ana[what?]QP  \n",
       "3                     dug[speak]V/t  \n",
       "4                      sag[good]V/i  \n",
       "...                             ...  \n",
       "195684                        rib-x  \n",
       "195685                      Å¡i-in-x  \n",
       "195686                            x  \n",
       "195687                            x  \n",
       "195688                            x  \n",
       "\n",
       "[195689 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"transliteration\":all_, \"words\":words_l, \"names\":names_l, \"utf-8\":utf8_l, \"lemm\" : lemm_})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/sux_df.p\", \"wb\") as w:\n",
    "    pickle.dump(df, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save as Text\n",
    "Save three different representations of the Sumerian text. Each representation is saved in a separate text file:\n",
    "- in transliteration        ===> sux_tl.txt\n",
    "- in lemmatized format   ===> sux_lemm.txt\n",
    "- in cuneiform (utf-8)    ===> sux_utf8.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_d = {\"sux_utf8\": \"utf-8\", \"sux_lemm\": \"lemm\", \"sux_tl\" : \"transliteration\"}\n",
    "for rep in rep_d:\n",
    "    text = ' '.join(df[rep_d[rep]]).strip()\n",
    "    text = text.replace(' Start', '\\n').strip()\n",
    "    text = text.replace('Start', '')\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    file = \"corpus/\" + rep + \".txt\"\n",
    "    with open(file, 'w', encoding=\"utf-8\") as w:\n",
    "        w.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
