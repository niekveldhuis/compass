{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Overlap in Lexical and Admin Vocabulary\n",
    "This notebook uses the techniques and the coding developed in 3.1 - 3.3 in order to compare the vocabulary of the Ur III administrative, legal, and epistolary documents with the vocabulary of the Old Babylonian lexical texts. Since there is nothing new about the code, is presented here with minimal comment.\n",
    "\n",
    "N.B. Since the Ur III corpus is large, running this notebook may take a long time on older computers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # this suppresses a warning about pandas from tqdm\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from ipywidgets import interact\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas() # initiate pandas support in tqdm, allowing progress_apply() and progress_map()\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import zipfile\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "util_dir = os.path.abspath('../utils')\n",
    "sys.path.append(util_dir)\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download and Parse\n",
    "Download the JSON file of the Ur3 corpus (in [epsd2/admin/ur3](http://oracc.org/epsd2/admin/ur3)) and parse the JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = \"epsd2/admin/ur3\"\n",
    "words = utils.get_data(projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Select for Sumerian and Create Lemma field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = words.loc[words[\"lang\"].str.contains(\"sux\")] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[\"lemma\"] = words.progress_apply(lambda r: f\"{r['cf']}[{r['gw']}]{r['pos']}\" \n",
    "                            if r[\"cf\"] != '' else f\"{r['form']}[NA]NA\", axis=1)\n",
    "words[\"lemma\"] = words[\"lemma\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Represent the Administrative Corpus in Line by Line Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm_lines = words.groupby([words['id_text'], words['id_line']]).agg({\n",
    "        'lemma': ' '.join\n",
    "    }).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Mark MWEs in Administrative Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Open the List of Lexical Vocabulary\n",
    "This list was created in 3.1. It contains all individual words (lemmas), plus all Multiple Word Expressions (= lexical entries). In the MWEs the lemmas are connect by underscores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/lex_vocab.txt', 'r', encoding = 'utf8') as r:\n",
    "    lex_vocab = r.read().splitlines()\n",
    "lex_vocab.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 MWEtokenizer\n",
    "Split the MWEs at the underscores and transform the resulting lists into tuples. The result is a list of tuples. Remove from the list tuples that contain only a single word. The list of tuples is now fed into the MWEtokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = [tuple(item.split(\"_\")) for item in lex_vocab]\n",
    "lex = [item for item in lex if len(item) > 1]\n",
    "tokenizer = MWETokenizer(lex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Use Tokenizer to Mark MWEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_list = [lemma.split() for lemma in adm_lines[\"lemma\"]]\n",
    "lemma_mwe = tokenizer.tokenize_sents(tqdm(lemma_list))\n",
    "adm_lines[\"lemma_mwe\"] = [' '.join(line) for line in lemma_mwe]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Create the Set of Shared Administrative and Lexical Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm_lines.to_pickle('output/ur3_lines.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Create the Set of Administrative Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm_words1 = words[\"lemma\"] # individual lemmas\n",
    "adm_words_s1 = {lemma for lemma in adm_words1 if not '[na]na' in lemma}\n",
    "adm_words2 = ' '.join(adm_lines['lemma_mwe']).split() # MWEs\n",
    "adm_words_s2 = {lemma for lemma in adm_words2 if not '[na]na' in lemma}\n",
    "adm_words_s2 = adm_words_s1 | adm_words_s2 # Union of individual lemmas and MWEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Intersection of Lexical and Administrative Lemmas and MWEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_words_s2 = set(lex_vocab)\n",
    "adm_lex = list(lexical_words_s2.intersection(adm_words_s2))\n",
    "adm_lex = [item.replace('_', ' ') for item in adm_lex]\n",
    "adm_lex.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Open Lexical Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_lines = pd.read_pickle('output/lexlines.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ura6 = [\"dcclt/P227657\",\n",
    "\"dcclt/P227743\",\n",
    "\"dcclt/P227791\",\n",
    "\"dcclt/P227799\",\n",
    "\"dcclt/P227925\",\n",
    "\"dcclt/P227927\",\n",
    "\"dcclt/P227958\",\n",
    "\"dcclt/P227967\",\n",
    "\"dcclt/P227979\",\n",
    "\"dcclt/P228005\",\n",
    "\"dcclt/P228008\",\n",
    "\"dcclt/P228200\",\n",
    "\"dcclt/P228359\",\n",
    "\"dcclt/P228368\",\n",
    "\"dcclt/P228488\",\n",
    "\"dcclt/P228553\",\n",
    "\"dcclt/P228562\",\n",
    "\"dcclt/P228663\",\n",
    "\"dcclt/P228726\",\n",
    "\"dcclt/P228831\",\n",
    "\"dcclt/P228928\",\n",
    "\"dcclt/P229015\",\n",
    "\"dcclt/P229093\",\n",
    "\"dcclt/P229119\",\n",
    "\"dcclt/P229304\",\n",
    "\"dcclt/P229332\",\n",
    "\"dcclt/P229350\",\n",
    "\"dcclt/P229351\",\n",
    "\"dcclt/P229352\",\n",
    "\"dcclt/P229353\",\n",
    "\"dcclt/P229354\",\n",
    "\"dcclt/P229356\",\n",
    "\"dcclt/P229357\",\n",
    "\"dcclt/P229358\",\n",
    "\"dcclt/P229359\",\n",
    "\"dcclt/P229360\",\n",
    "\"dcclt/P229361\",\n",
    "\"dcclt/P229362\",\n",
    "\"dcclt/P229365\",\n",
    "\"dcclt/P229366\",\n",
    "\"dcclt/P229367\",\n",
    "\"dcclt/P229890\",\n",
    "\"dcclt/P229925\",\n",
    "\"dcclt/P230066\",\n",
    "\"dcclt/P230208\",\n",
    "\"dcclt/P230230\",\n",
    "\"dcclt/P230530\",\n",
    "\"dcclt/P230586\",\n",
    "\"dcclt/P231095\",\n",
    "\"dcclt/P231128\",\n",
    "\"dcclt/P231424\",\n",
    "\"dcclt/P231446\",\n",
    "\"dcclt/P231453\",\n",
    "\"dcclt/P231458\",\n",
    "\"dcclt/P231742\",\n",
    "\"dcclt/P266520\"]\n",
    "lex_lines.loc[lex_lines[\"id_text\"].isin(Ura6), \"id_text\"] = \"dcclt/Q000043\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Make DTM out of Lexical Corpus\n",
    "Use the shared administrative/lexical vocabulary as vocabulary and ngrams from 1 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(preprocessor = lambda x: x, tokenizer = lambda x: x.split(), vocabulary = adm_lex, ngram_range=(1, 5))\n",
    "dtm = cv.fit_transform((lex_lines['lemma']))\n",
    "lex_lines_dtm = pd.DataFrame(dtm.toarray(), columns= cv.get_feature_names(), index=lex_lines[\"id_text\"])\n",
    "lex_comp_dtm = lex_lines_dtm.groupby('id_text').agg(sum).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Compute n_matches for each lexical document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_comp_dtm[\"n_matches\"] = lex_comp_dtm[adm_lex].astype(bool).sum(axis = 1)\n",
    "lex_comp_dtm[\"id_text\"] = [i[-7:] for i in lex_comp_dtm[\"id_text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Add Lexical Metadata\n",
    "From a pickled file, produced in 3.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = pd.read_pickle('output/lexdtm.p')\n",
    "lex = pd.merge(lex, lex_comp_dtm[['n_matches', 'id_text']], on='id_text', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Add Normalized Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex['norm'] = lex['n_matches'] / lex['length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 Explore Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex.to_pickle('output/adm_lex.p')\n",
    "anchor = '<a href=\"http://oracc.org/dcclt/{}\", target=\"_blank\">{}</a>'\n",
    "lex2 = lex.copy()\n",
    "lex2['id_text'] = [anchor.format(val,val) for val in lex['id_text']]\n",
    "lex2['PQ'] = ['Composite' if i[0] == 'Q' else 'Exemplar' for i in lex['id_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(sort_by = lex2.columns, rows = (1, len(lex2), 1), min_length = (0,500,5), show = [\"Exemplars\", \"Composites\", \"All\"])\n",
    "def sort_df(sort_by = \"norm\", ascending = False, rows = 25, min_length = 200, show = 'All'):\n",
    "    if not show == 'All':\n",
    "        l = lex2.loc[lex2['PQ'] == show[:-1]]\n",
    "    else:\n",
    "        l = lex2\n",
    "    l = l.drop('PQ', axis = 1)\n",
    "    l = l.loc[l.length >= min_length].sort_values(by = sort_by, ascending = ascending).reset_index(drop=True)[:rows].style\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "state": {
    "0a5ada570441422b81feab4f46df6e85": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "455a67b4345a4b2ca810ae304ddf61de": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "8b1b885646de476c868fb0583cd3fff2": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "ab4e279cb03647e984b99cd78f91b68f": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
