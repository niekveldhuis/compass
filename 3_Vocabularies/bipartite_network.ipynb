{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407a6f45-ecf3-42b0-9dd2-f407c5aab195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hvplot\n",
    "import hvplot.pandas\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "import matplotlib.pyplot as plt\n",
    "import hvplot.networkx as hvnx\n",
    "import zipfile\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bd5c57-9251-46a9-9d6e-3a2f420d38ca",
   "metadata": {},
   "source": [
    "Read the catalogs of dcclt and dcclt/signlists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860f6617-0ead-405f-8f78-027036a7d1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"jsonzip/dcclt.zip\", \"jsonzip/dcclt-signlists.zip\"]\n",
    "cat_lex_df = pd.DataFrame()\n",
    "for file in files:\n",
    "    z = zipfile.ZipFile(file) \n",
    "    namelist = z.namelist()\n",
    "    cat = [name for name in namelist if \"catalogue.json\" in name][0]\n",
    "    st = z.read(cat).decode(\"utf-8\")\n",
    "    j = json.loads(st)\n",
    "    df = pd.DataFrame(j[\"members\"]).T\n",
    "    df[\"id_text\"] = df[\"id_text\"].fillna(df[\"id_composite\"])\n",
    "    df = df[[\"id_text\", \"period\", \"designation\"]]\n",
    "    cat_lex_df = cat_lex_df.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf8b44f-7c4b-4fa1-bd84-60501a67a5cb",
   "metadata": {},
   "source": [
    "From the catalog, keep only the composite texts (those with Q numbers) dating to ED IIIa, ED IIIb or the Old Babylonian period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2349f5ef-d9ce-4493-93f9-3cfe4ab07962",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_lex_df = cat_lex_df.loc[cat_lex_df['id_text'].str.contains(\"Q\")]\n",
    "periods = [\"Early Dynastic IIIa\", \"Early Dynastic IIIb\", \"Old Babylonian\"]\n",
    "cat_lex_df = cat_lex_df.loc[cat_lex_df[\"period\"].isin(periods)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e336f6-7ad0-440d-89ed-1b05b73501c9",
   "metadata": {},
   "source": [
    "Read catalog of the literary texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37143cf-b957-4ac3-a734-a7ae9d8a15c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"jsonzip/epsd2-literary.zip\"\n",
    "z = zipfile.ZipFile(file) \n",
    "st = z.read(\"epsd2/literary/catalogue.json\").decode(\"utf-8\")\n",
    "j = json.loads(st)\n",
    "cat_lit_df = pd.DataFrame(j[\"members\"]).T\n",
    "cat_lit_df[\"id_text\"] = cat_lit_df[\"id_text\"].fillna(cat_lit_df[\"id_composite\"])\n",
    "cat_lit_df = cat_lit_df[[\"id_text\", \"period\", \"designation\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad48e3c-486f-4d90-aef4-df898caf2543",
   "metadata": {},
   "source": [
    "Read pickled version of lexical lists and OB literary texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5f0ac4-b283-4aec-96d2-399ba23fb366",
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = pd.read_pickle(\"output/lex_words.p\")\n",
    "lit = pd.read_pickle(\"output/lit_words.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f41405-41ec-42ea-8d5b-fc807b5c006c",
   "metadata": {},
   "source": [
    "Remove unlemmatized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d601e975-b9df-41cf-9595-013737817829",
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = lex.loc[~lex.lemma.str.endswith(\"[na]na\")]\n",
    "lit = lit.loc[~lit.lemma.str.endswith(\"[na]na\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac359ba3-f562-4086-8336-d8edb14f3ed9",
   "metadata": {},
   "source": [
    "From the lexical texts, keep the entries that belong to composite texts from the ED IIIa, ED IIIb, or Old Babylonian periods, by using the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aacb9d9-4678-4b23-9fd0-95f10d1d4f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "lex[\"id_text\"] = [textid.split('/')[-1] for textid in lex[\"id_text\"]]\n",
    "keep = cat_lex_df.index.values\n",
    "lexQ = lex.loc[lex[\"id_text\"].isin(keep)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79abdb53-2e69-449e-a5b5-c03f829d8351",
   "metadata": {},
   "source": [
    "From the literary texts, keep only composites (those with Q numbers). This removes, for instance, all the exemplars from Ur in UET 6. This may be a bit too rough. There are some longer texts that only exists in a single exemplar and have no Q number - e.g. P357170 (Ludwig/Metcalf in ZA 107), and several texts in CUSAS 37. For now, those texts are added by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0133169-65c7-45c0-af2b-bd951112a4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit[\"id_text\"] = [textid.split(\"/\")[-1] for textid in lit[\"id_text\"]]\n",
    "litQ = lit.loc[lit['id_text'].str.contains(\"Q\")]\n",
    "added = [\"P357170\", \"P254171\", \"P252333\", \"P251713\", \"P251427\", \"P252296\", \"P254175\", \"X010001\"]\n",
    "lit_add = lit.loc[lit['id_text'].isin(added)]\n",
    "litQ = litQ.append(added)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bfa934-8c15-45fc-8b17-80f27fe298fc",
   "metadata": {},
   "source": [
    "Group lemmas by composition; create one long string of lemmas for each composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d35304-ddd4-4c56-921b-fd5ae9c5514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexQ2 = lexQ.groupby(by = \"id_text\").aggregate({\"lemma\" : \" \".join}).reset_index()\n",
    "litQ2 = litQ.groupby(by = \"id_text\").aggregate({\"lemma\" : \" \".join}).reset_index()\n",
    "lexQ2[\"text_length\"] = [len(set(lemlist.split())) for lemlist in lexQ2[\"lemma\"]]\n",
    "litQ2[\"text_length\"] = [len(set(lemlist.split())) for lemlist in litQ2[\"lemma\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dae071-afd3-4cd0-94df-48395d7e783f",
   "metadata": {},
   "source": [
    "Create dictionary of texids/text names (designation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed0d8b-18e1-452e-a5e2-2337c839ba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dict = dict(zip(cat_lit_df[\"id_text\"], cat_lit_df[\"designation\"]))\n",
    "lex_dict = dict(zip(cat_lex_df[\"id_text\"], cat_lex_df[\"designation\"]))\n",
    "comp_dict.update(lex_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f802869-ca62-4d82-8e93-2e1988831411",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_lexical = lexQ2[\"id_text\"]\n",
    "nodes_literary = litQ2[\"id_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4496c9b7-3654-487b-90f8-1c120e79f22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = nx.Graph()\n",
    "B.add_nodes_from(nodes_lexical, bipartite=0)\n",
    "B.add_nodes_from(nodes_literary, bipartite=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c242115-0687-4ac0-b792-1dc6087748bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = []\n",
    "for i in range(len(lexQ2)):\n",
    "    lexwords = set(lexQ2.iloc[i][\"lemma\"].split())\n",
    "    id_lex = lexQ2.iloc[i][\"id_text\"]\n",
    "    for j in range(len(litQ2)):\n",
    "        litwords = set(litQ2.iloc[j][\"lemma\"].split())\n",
    "        id_lit = litQ2.iloc[j][\"id_text\"]\n",
    "        weight = len(lexwords.intersection(litwords))\n",
    "        if weight > 0:\n",
    "            edge = (id_lex, id_lit, weight)\n",
    "            edges.append(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d805b1-0fe6-4a3b-9c06-5a3786f0ed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "B.add_weighted_edges_from(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d13ba7-a64d-4395-b1f6-0d75126bbcf2",
   "metadata": {},
   "source": [
    "Remove edges with low weights. Remove nodes that no longer connect to anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18e4c31-1314-49b7-ba7d-5d47857496ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_network(G, T):\n",
    "    \"\"\"\n",
    "    Remove all edges with weight<T from G or its copy.\n",
    "    \"\"\"\n",
    "    F = G.copy()\n",
    "    F.remove_edges_from((n1, n2) for n1, n2, w in G.edges(data=\"weight\") if w < T)\n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea89c081-ca88-4384-b9b3-a02d53a1bc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = slice_network(B, 50)\n",
    "C.remove_nodes_from(list(nx.isolates(C)))\n",
    "nodes_lexical = [node for node in nodes_lexical if node in C.nodes]\n",
    "nodes_literary = [node for node in nodes_literary if node in C.nodes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57eed87-0c9d-45e4-9933-63fd4316ba4a",
   "metadata": {},
   "source": [
    "compute total weights for all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5a9a54-8a83-4207-a9d0-942335f509f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [(s, w) for s, t, w in C.edges(data=\"weight\")]\n",
    "weights_total = pd.DataFrame(w, columns=['id_text', 'weight']).groupby(by='id_text').aggregate({\"weight\" : sum}).reset_index()\n",
    "weights_total_d = dict(zip(weights_total['id_text'], weights_total[\"weight\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd45ec94-9422-4395-8ade-a8369b966aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = [(t, w) for s, t, w in B.edges(data=\"weight\")]\n",
    "weights_total2 = pd.DataFrame(w2, columns = ['id_text', 'weight']).groupby(by='id_text').aggregate({'weight' : sum}).reset_index()\n",
    "weights_total_d2 = dict(zip(weights_total2['id_text'], weights_total2[\"weight\"]))\n",
    "weights_total_d.update(weights_total_d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5062ab50-502c-4a59-91e9-26d63f0ce74e",
   "metadata": {},
   "source": [
    "Relative weight. Since all computations are done with unique lemmas, text length is also be expressed in terms of number of unique lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8ea435-672a-4c1b-a397-96dbd69f55da",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length_d = dict(zip(lexQ2[\"id_text\"], lexQ2[\"text_length\"]))\n",
    "text_length_lit = dict(zip(litQ2[\"id_text\"], litQ2[\"text_length\"]))\n",
    "text_length_d.update(text_length_lit)\n",
    "relative_weight_d = {textid : weight/text_length_d.get(textid) for (textid, weight) in weights_total_d.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf83bb52-940a-4a2f-b757-b37949a40af6",
   "metadata": {},
   "source": [
    "Add total weights, relative weights, and text name as node attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220725d1-3c5f-4878-8106-de5b35b381fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in C.nodes:\n",
    "    C.nodes[node]['total_weight'] = weights_total_d.get(node, 0)\n",
    "    C.nodes[node]['title'] = comp_dict.get(node, \"None\")\n",
    "    C.nodes[node]['rel_weight'] = relative_weight_d.get(node, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64adeb3f-a5b4-4af6-ba8b-c7fb83a31986",
   "metadata": {},
   "source": [
    "order nodes by relative weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366aa2eb-5744-4a4d-84c2-5ceb74fc31b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_lexical = sorted(nodes_lexical, key= lambda t: C.nodes[t]['rel_weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e3bc3-60c5-4b5f-a1db-2fb10e2d733c",
   "metadata": {},
   "source": [
    "compute positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159d2d32-d281-4c3d-8b91-4e1940c8bb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.drawing.layout.bipartite_layout(C, nodes_lexical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be960d6-b44d-4061-bd45-29b383c1de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "hvnx.draw(C,\n",
    "          pos,\n",
    "          height=1800,\n",
    "          width=1000,\n",
    "          #with_labels=True,\n",
    "          #labels=label_dict,\n",
    "          #edge_width='weight',\n",
    "          edge_alpha= 0.7,\n",
    "          edge_color='weight',\n",
    "          node_color='rel_weight',\n",
    "          node_cmap=plt.cm.plasma,\n",
    "          edge_cmap=plt.cm.Blues,\n",
    "          label=\"lexical and literary bipartite graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0402a86c-0b02-458a-bfc0-4d934b243ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
