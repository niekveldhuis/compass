{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is being replaced by 3_3_Alternative\n",
    "\n",
    "# 3.3 Looking at the Lexical Vocabulary from the Perspective of the Literary Material\n",
    "\n",
    "In section 3.2 we asked whether we can see differences between Old Babylonian literary compositions in their usage of vocabulary (lemmas and MWEs) attested in the lexical corpus. In this notebook we will change perspective and ask: are there particular lexical texts (or groups of lexical texts) that show a greater engagement with literary vocabulary than others?\n",
    "\n",
    "In large part, this notebook uses the same techniques and the same code as section 3.2 did, and the reader is referred there for further explanation. In some aspects, however, the process is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # this suppresses a warning about pandas from tqdm\n",
    "import pandas as pd\n",
    "from ipywidgets import interact\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas() # initiate pandas support in tqdm, allowing progress_apply() and progress_map()\n",
    "import zipfile\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the file `litlines.p` which was produced in [3_1_Lit_Lex_Vocab.ipynb](./3_1_Lit_Lex_Vocab.ipynb). The file contains the pickled version of the DataFrame `lit_lines` in which the literary ([epsd2/literary](http://oracc.org/epsd2/literary)) corpus is represented in line-by-line format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_lines = pd.read_pickle('output/litlines.p')\n",
    "lit_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the file `lit_lex_vocab` which was produced in [3_2_Lit_Lex.ipynb](./3_2_Lit_Lex.ipynb) and which contains the vocabulary (lemmas and MWEs) shared by lexical and literary texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/lit_lex_vocab.txt', 'r', encoding = 'utf8') as l:\n",
    "    lit_lex_vocab = l.read().splitlines()\n",
    "lit_lex_vocab[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Lexical Corpus\n",
    "Open the file `lexlines.p` which contains the (Old Babylonian) lexical corpus in line-by-line format. Replace all spaces by underscores, in order to turn lexical entries into Multiple Word Expressions. Remove all lines that contain unlemmatized words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_lines = pd.read_pickle('output/lexlines.p')\n",
    "lex_lines['lemma'] = [lemma.replace(' ', '_') for lemma in lex_lines['lemma']]\n",
    "lex_lines = lex_lines.loc[~lex_lines.lemma.str.contains('\\[na\\]na')]\n",
    "lex_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Case: OB Nippur Ura 6\n",
    "The sixth chapter of the Old Babylonian Nippur version of the thematic list Ura deals with foodstuffs and drinks. This chapter was not standardized (each exemplar has its own order of items and sections) and therefore no composite text has been created in [DCCLT](http://oracc.org/dcclt). Instead, the \"composite\" of [OB Nippur Ura 6](http://oracc.org/dcclt/Q000043) consists of the concatenation of all known Nippur exemplars of the list of foodstuffs. In our current dataframe, therefore, there are no lines where the field `id_text` equals \"dcclt/Q000043\".\n",
    "\n",
    "We create a \"composite\" by changing the field `id_text` in all exemplars of [OB Nippur Ura 6](http://oracc.org/dcclt/Q000043) to \"dcclt/Q000043\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ura6 = [\"dcclt/P227657\",\n",
    "\"dcclt/P227743\",\n",
    "\"dcclt/P227791\",\n",
    "\"dcclt/P227799\",\n",
    "\"dcclt/P227925\",\n",
    "\"dcclt/P227927\",\n",
    "\"dcclt/P227958\",\n",
    "\"dcclt/P227967\",\n",
    "\"dcclt/P227979\",\n",
    "\"dcclt/P228005\",\n",
    "\"dcclt/P228008\",\n",
    "\"dcclt/P228200\",\n",
    "\"dcclt/P228359\",\n",
    "\"dcclt/P228368\",\n",
    "\"dcclt/P228488\",\n",
    "\"dcclt/P228553\",\n",
    "\"dcclt/P228562\",\n",
    "\"dcclt/P228663\",\n",
    "\"dcclt/P228726\",\n",
    "\"dcclt/P228831\",\n",
    "\"dcclt/P228928\",\n",
    "\"dcclt/P229015\",\n",
    "\"dcclt/P229093\",\n",
    "\"dcclt/P229119\",\n",
    "\"dcclt/P229304\",\n",
    "\"dcclt/P229332\",\n",
    "\"dcclt/P229350\",\n",
    "\"dcclt/P229351\",\n",
    "\"dcclt/P229352\",\n",
    "\"dcclt/P229353\",\n",
    "\"dcclt/P229354\",\n",
    "\"dcclt/P229356\",\n",
    "\"dcclt/P229357\",\n",
    "\"dcclt/P229358\",\n",
    "\"dcclt/P229359\",\n",
    "\"dcclt/P229360\",\n",
    "\"dcclt/P229361\",\n",
    "\"dcclt/P229362\",\n",
    "\"dcclt/P229365\",\n",
    "\"dcclt/P229366\",\n",
    "\"dcclt/P229367\",\n",
    "\"dcclt/P229890\",\n",
    "\"dcclt/P229925\",\n",
    "\"dcclt/P230066\",\n",
    "\"dcclt/P230208\",\n",
    "\"dcclt/P230230\",\n",
    "\"dcclt/P230530\",\n",
    "\"dcclt/P230586\",\n",
    "\"dcclt/P231095\",\n",
    "\"dcclt/P231128\",\n",
    "\"dcclt/P231424\",\n",
    "\"dcclt/P231446\",\n",
    "\"dcclt/P231453\",\n",
    "\"dcclt/P231458\",\n",
    "\"dcclt/P231742\",\n",
    "\"dcclt/P266520\"]\n",
    "lex_lines.loc[lex_lines[\"id_text\"].isin(Ura6), \"id_text\"] = \"dcclt/Q000043\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_comp = lex_lines.groupby(['id_text']).agg({'lemma': ' '.join}).reset_index()\n",
    "lex_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Duplicates\n",
    "Since the lexical data are drawn from multiple (sub)projects, it is possible that there are duplicates. Duplicates have the same P, Q, or X number. We select the version with the largest number of (lemmatized) words and drop others.\n",
    "\n",
    "First the field `id_text` is reduced to only the last 7 positions (P, Q, or X, followed by six digits). The new field `length` represents the length of a document in terms of lemmatized words or entries (unlemmatized words and entries have already been dropped above). The DataFrame is then ordered by length (from large to small) and, if duplicate `text_id`s are found, only the first one is kept with the Pandas method `drop_duplicates()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_comp['id_text'] = [i[-7:] for i in lex_comp['id_text']]\n",
    "lex_comp['length'] = [len(lem.split()) for lem in lex_comp['lemma']]\n",
    "lex_comp = lex_comp.sort_values(by = 'length', ascending = False)\n",
    "lex_comp = lex_comp.drop_duplicates(subset = 'id_text', keep = 'first')\n",
    "lex_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(preprocessor = lambda x: x, tokenizer = lambda x: x.split(), vocabulary = lit_lex_vocab)\n",
    "dtm = cv.fit_transform(lex_comp['lemma'])\n",
    "lex_df = pd.DataFrame(dtm.toarray(), columns= cv.get_feature_names(), index=lex_comp[\"id_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion: lit_lex_vocab, saved form notebook 3.2 includes all lemmas and all MWEs shared by lit and lex. In the data, countvectorizer only sees the lexical *entries* because they have been connected by underscores. Perhaps: do not connect, CountVectorizer() with ngram_range = (1,4) and use CV on lexical *lines* then combine lines to compositions in DTM. No that didn't work.\n",
    "Other solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_df = lex_df.loc[: , lex_df.sum(axis=0) > 0].copy()\n",
    "vocab = lex_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_df[\"n_matches\"] = lex_df[vocab].astype(bool).sum(axis = 1, numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get the metadata. \n",
    "cat = {}\n",
    "for proj in ['dcclt', 'dcclt/signlists', 'dcclt/nineveh', 'dcclt/ebla']:\n",
    "    f = proj.replace('/', '-')\n",
    "    file = f\"jsonzip/{f}.zip\" # The ZIP file was downloaded in notebook 3_1\n",
    "    z = zipfile.ZipFile(file) \n",
    "    st = z.read(f\"{proj}/catalogue.json\").decode(\"utf-8\")\n",
    "    j = (json.loads(st))\n",
    "    cat.update(j[\"members\"])\n",
    "cat_df = pd.DataFrame(cat).T\n",
    "cat_df[\"id_text\"] = cat_df[\"id_text\"].fillna(cat_df[\"id_composite\"])\n",
    "cat_df = cat_df.fillna('')\n",
    "cat_df = cat_df[[\"id_text\", \"designation\", \"subgenre\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = pd.merge(cat_df, lex_df['n_matches'], on = 'id_text', how = 'inner')\n",
    "lex = pd.merge(lex, lex_comp[['length', 'id_text']], on = 'id_text', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex['norm'] = lex['n_matches'] / lex['length']\n",
    "lex = lex.sort_values(by = 'norm', ascending = False)\n",
    "lex.loc[lex.length > 250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = '<a href=\"http://oracc.org/dcclt/{}\", target=\"_blank\">{}</a>'\n",
    "lex2 = lex.copy()\n",
    "lex2['id_text'] = [anchor.format(val,val) for val in lex['id_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(sort_by = lex2.columns, rows = (1, len(lex2), 1), min_length = (1,500,5))\n",
    "def sort_df(sort_by = \"norm\", ascending = False, rows = 25, min_length = 250):\n",
    "    return lex2.loc[lex2.length >= min_length].sort_values(by = sort_by, ascending = ascending).reset_index(drop=True)[:rows].style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step: look at important words with tfidf.\n",
    "\n",
    "Note: first make ngrams (as above) then TfidfVectorizer() with vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_comp2 = lit_lines.groupby(['id_text']).agg({'lemma' : ' '.join}).reset_index()\n",
    "lit_comp2['id_text'] = [i[-7:] for i in lit_comp2['id_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = TfidfVectorizer(token_pattern = r'[^ ]+', ngram_range = (1,5), vocabulary = vocab)\n",
    "dtm = tv.fit_transform(lit_comp2['lemma'])\n",
    "lit_df = pd.DataFrame(dtm.toarray(), columns= tv.get_feature_names(), index=lit_comp2[\"id_text\"])\n",
    "#cols = [col for col in lit_df.columns if not '[na]na' in col]\n",
    "#lit_df = lit_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_df.columns = [voc.replace('_', ' ') for voc in lex_df.columns]\n",
    "vocab = [v.replace('_', ' ') for v in vocab]\n",
    "lit_lex_df = lit_df[vocab].copy() #select columns with terms in lexical vocabulary\n",
    "lit_lex_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = lit_lex_df.sum(axis=0) / lit_lex_df.astype(bool).sum(axis=0)\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_lex_tfidf = lex_df[vocab].mul(mean, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_lex_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_lex_tfidf['weighted'] = lit_lex_tfidf[vocab].sum(axis=1, numeric_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_lex_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_lex_tfidf = lit_lex_tfidf.loc[lit_lex_tfidf.sum(axis=1) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex2 = pd.merge(cat_df, lit_lex_tfidf['weighted'], on = 'id_text', how = 'inner')\n",
    "lex2 = pd.merge(lex2, lex[['length', 'n_matches', 'id_text']], on = 'id_text', how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of dividing by length look at mean value of weighted\n",
    "```python\n",
    "lex2['norm'] = lex2['weigthed'] / lex2[vocab].astype(bool).sum(axis = 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lex2['norm'] = lex2['weighted'] / lex2['n_matches']\n",
    "lex2['norm'] = lex2['weighted'] / lex2['length']\n",
    "lex2.sort_values(by = 'norm', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = '<a href=\"http://oracc.org/dcclt/{}\", target=\"_blank\">{}</a>'\n",
    "lex3 = lex2.copy()\n",
    "lex3['id_text'] = [anchor.format(val,val) for val in lex2['id_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(sort_by = lex3.columns, rows = (1, len(lex3), 1), min_length = (1,500,5))\n",
    "def sort_df(sort_by = \"weighted\", ascending = False, rows = 25, min_length = 200):\n",
    "    return lex3.loc[lex3.length >= min_length].sort_values(by = sort_by, ascending = ascending).reset_index(drop=True)[:rows].style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ngrams(lemmas):\n",
    "    lemmas = lemmas.split()\n",
    "    lemmas_bi = bigrams(lemmas)\n",
    "    lemmas_tri = trigrams(lemmas)\n",
    "    lemmas_n = list(lemmas_bi) + list(lemmas_tri)\n",
    "    lemmas_n = ['_'.join(lem) for lem in lemmas_n]\n",
    "    lemmas = set(lemmas + lemmas_n)\n",
    "    lemmas = [lem for lem in lemmas if not '[na]na' in lem]\n",
    "    lit_vocab.extend(lemmas)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_vocab = []\n",
    "lit_lines['lemma'].progress_apply(make_ngrams)\n",
    "lit_vocab = list(set(lit_vocab))\n",
    "lit_vocab.sort()\n",
    "lit_vocab[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make ngrams: unigrams, bigrams, and trigrams. Represent bigrams and trigrams as MWEs, connected by underscores. Create a full list of all lemmas and ngrams, omitting all non-lemmatized words (or ngrams that include non-lemmatized words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: This step can be done with Countvectorizer, with setting ngrams = (1,3). Disadvantages of that approach:\n",
    "> - we don not need a full DTM for the literary corpus\n",
    "> - the DTM should be made on *lines* instead of *documents* to prevent words from consecutive lines to form bigrams or trigrams. Afterwards use groupby and agg to make DTM on document level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
