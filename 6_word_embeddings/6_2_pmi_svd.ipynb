{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Computing Word Similarities with PMI and SVD\n",
    "\n",
    "The following is inspired by the blog post [simple word vectors with co-occurrence pmi and svd](https://www.kaggle.com/alexklibisz/simple-word-vectors-with-co-occurrence-pmi-and-svd) by Alex Klibisz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from math import log\n",
    "from pprint import pformat\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn import preprocessing\n",
    "from nltk import ngrams\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First read in the list of list that was created in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/data_for_pmi.p\", \"rb\") as r:\n",
    "    lemm_l = pickle.load(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute PMI we need a sliding window going over the data. There are various ways of creating such windows. The approach taken here is to use the `ngrams` function from the `nltk` package.\n",
    "```python\n",
    "from nltk import ngrams\n",
    "n = 7\n",
    "windows = ngrams(text, n)\n",
    "```\n",
    "This will create windows of the size `n`. Increasing `n` will increase the number of collocates and the time it takes to produce them. The minimum size of `n` is 2, which will only consider words that are immediately adjacent. Larger window sizes may capture the general theme of the passage from which the window is taken.\n",
    "\n",
    "For each window the function `combinations()` (from the `itertool` library) is used to produce all possible combinations of 2 words in that window. With a 7-word window length, a sequence such as **en-lil₂ kur gal-ra šulgi lugal-e e₂ mu-na-du₃** (\"Šulgi the king build a temple for Great Mountain Enlil\") will yield the following 21 word pairs as tuples:  \n",
    "* (Enlil\\[1\\]DN, kur\\[mountain\\]N)\n",
    "* (Enlil\\[1\\]DN, gal\\[big\\]V/i)\n",
    "* (Enlil\\[1\\]DN, Šulgi\\[1\\]RN)\n",
    "* (Enlil\\[1\\]DN, lugal\\[king\\]N)\n",
    "* (Enlil\\[1\\]DN, e\\[house\\]N)\n",
    "* (Enlil\\[1\\]DN, du\\[build\\]V/t)\n",
    "* (kur\\[mountain\\]N, gal\\[big\\]V/i)\n",
    "* (kur\\[mountain\\]N, Šulgi\\[1\\]RN)\n",
    "* (kur\\[mountain\\]N, lugal\\[king\\]N)\n",
    "* (kur\\[mountain\\]N, e\\[house\\]N)\n",
    "* (kur\\[mountain\\]N, du\\[build\\]V/t)\n",
    "* (gal\\[big\\]V/i, Šulgi\\[1\\]RN)\n",
    "* (gal\\[big\\]V/i, lugal\\[king\\]N)\n",
    "* (gal\\[big\\]V/i, e\\[house\\]N)\n",
    "* (gal\\[big\\]V/i, du\\[build\\]V/t)\n",
    "* (Šulgi\\[1\\]RN, lugal\\[king\\]N)\n",
    "* (Šulgi\\[1\\]RN, e\\[house\\]N)\n",
    "* (Šulgi\\[1\\]RN, du\\[build\\]V/t)\n",
    "* (lugal\\[king\\]N, e\\[house\\]N)\n",
    "* (lugal\\[king\\]N, du\\[build\\V/t])\n",
    "* (e\\[house\\]N, du\\[build\\]V/t)\n",
    "\n",
    "Each of these word pairs is considered a collocate. By reducing the window size one may concentrate on words that occur closer together, and may be part of a more or less fixed combination (such as **kur gal** \"great mountain\" as an epithet of Enlil). Larger window sizes will consider broader thematic issues, recognizing words that are used when discussing particular themes or issues.\n",
    "\n",
    "The `Counter()` function (from the `collections` library) is used to establish the counts of unique words (variable `cx`) as well as the counts of unique collocates (variable `cxy`). The `Counter()` is first initialized as an empty variable and then updated while iterating over the list of lists `lemm_l`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cx = Counter()\n",
    "cxy = Counter()\n",
    "windowsize = widgets.IntSlider(min=2, max=25, value=7, description=\"window size\")\n",
    "windowsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in tqdm(lemm_l):\n",
    "    cx.update(text)\n",
    "    windows = ngrams(text, windowsize.value)\n",
    "    for w in windows:\n",
    "        z = [tuple(l) for l in map(sorted, combinations(w, 2))]\n",
    "        cxy.update(z)  # count all collocates (word pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stop words and rare words\n",
    "# TODO: edit the text below and make decisions about stop word list\n",
    "The following cell is used to remove very frequent and/or very rare lemmas, based on `min_count` and `max_count` thresholds. The variable `min_count` is currently set to 15. For low-frequency words the current approach is not reliable; the best setting of this parameter is still subject to further research and may well depend on the corpus that is being analyzed. The variable `max_count` currently equals the total number of tokens in the corpus, which means that no [stop words](https://en.wikipedia.org/wiki/Stop_word) are discarded. For Sumerian, it is not obvious which words should count as stop words since Sumerian barely uses pronouns (where used they are highly significant) and has few true function words. For the Ur III corpus, one could designate metrological units such as **gur**, **giŋ**, and **sila**, or time units such as **ud** (day) and **itud** (month) as stop words, since they add little indeed to the content of the documents. However,  the association of **gur**, **sila**, etc. with some commodities (counted by volume) and not with others (counted by piece or weight, etc.) is exactly the kind of information that is captured well with PMI. (For an excellent discussion of stop words see Julia Silge's [Supervised Machine Learning for Text Analysis in R](https://smltar.com/stopwords.html)).\n",
    "\n",
    "==================\n",
    "\n",
    "(one might argue that **ud** is in many cases a function word meaning \"when\"). (Add here **ki** (place) **mu** (name), **ŋiri** (foot) and **šuniŋin** (total); perhaps **kišib** (seal) and **dumu** (child)). Very frequent items such as **udu** sheep, **še** (barley), **ninda** (bread) or **lugal** (king) should not be removed, because those are the actual subjects of these texts.\n",
    "\n",
    "=====================\n",
    "\n",
    "Importantly, we do remove all underscores here. In our data, underscores are place holders that represent unlemmatized words (broken or unknown). They have been included in the process so far for the benefit of the sliding window in the previous section. Our list of collocates, therefore, has many entries such as (lugal\\[king\\]N, _ ) meaning that there are 7-word windows in which the word for king co-occurs with an unlemmatized word. These collocates are removed in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('%d unique tokens before' % len(cx))\n",
    "print('%d total of tokens before' % sum(cx.values()))\n",
    "sx = sum(cx.values())\n",
    "min_count = 5\n",
    "max_count = sx\n",
    "for x in list(cx.keys()):\n",
    "    if cx[x] < min_count or cx[x] > max_count:\n",
    "        del cx[x]\n",
    "del cx['_']  # underscores are place holders and may be removed now.\n",
    "print('%d unique tokens after' % len(cx))\n",
    "print('%d total of tokens after' % sum(cx.values()))\n",
    "print('Most common:', cx.most_common()[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the Collocates\n",
    "Remove all bigrams that contain a lemma that has been removed in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in list(cxy.keys()):\n",
    "    if x not in cx or y not in cx:\n",
    "        del cxy[(x, y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lookup Dictionaries\n",
    "The following step creates two dictionaries. The dictionary `x2i` has the lemmas as keys and a counter as value. The dictionary `i2x` has that same counter as keys, and the lemmas as values. The counters will be used as indexes for the columns and rows of the matrix constructed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2i, i2x = {}, {}\n",
    "for i, x in enumerate(cx.keys()):\n",
    "    x2i[x] = i\n",
    "    i2x[i] = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Token and Collocate Totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sx = sum(cx.values())\n",
    "sxy = sum(cxy.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create PMI matrix\n",
    "\n",
    "In the next cell the data is transformed into a (sparse) matrix, with rows and columns representing unique words, and the data in each cell representing the PMI score for the co-occurence of the two words. The variable `cxy` contains a list of all collocates in the format `('Unug[1]SN', 'šag[heart]N'): 1653`; meaning that the collocation of these two terms appears 1653 times. The dictionary `x2i` is used to translate each lemma into an index number and append the numbers to the lists `rows` and `cols`. The third element that the matrix function need is `data`. Instead of simply entering the frequency in `data` we enter the PPMI score. The formula for PMI is  $$log\\frac{p(x,y)}{p(x)p(y)}$$; that is: the logarithm of the probability of encountering x *and* y, divided by the probability of x times the probability of y. The probabilities are computed by dividing the frequency of x and y by the total number of tokens and dividing the frequency of (x, y) by the total number of collocates.\n",
    "\n",
    "One problem with PMI is that it favors low frequency terms. If term A only appears 10 times in our dataset, each collocate with any other term will appear significant, because it occurs in 10% of all the occurences of term A. We will see this effect, for instance, in the appearance of rare names. One solution is using PMI2, where the probability of the collocate is squared: PMI2 = $$log\\frac{p(x,y)^2}{p(x)p(y)}$$\n",
    "\n",
    "Still other possibility: PMIα: raise p(y) to the power of α, usually 0.75. PMIα = $$log\\frac{p(x,y)}{p(x)p(y)^α}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flavors = [\"PPMI\", \"PMI2\", \"PMIα\"]\n",
    "pmi_flavor = widgets.Dropdown(options=flavors, value=\"PMIα\")\n",
    "pmi_flavor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_samples = Counter()\n",
    "data, rows, cols = [], [], []\n",
    "for (x, y), n in tqdm(cxy.items()):\n",
    "    rows.append(x2i[x])\n",
    "    cols.append(x2i[y])\n",
    "    if pmi_flavor.value == \"PPMI\":\n",
    "        data.append(max(log((n / sxy) / (cx[x] / sx) / (cx[y] / sx)), 0)) # PPMI\n",
    "    elif pmi_flavor.value == \"PMI2\":\n",
    "        data.append(abs(log((n / sxy)**2 / (cx[x] / sx) / (cx[y] / sx)))) # PMI2\n",
    "    else:\n",
    "        data.append(max(log((n / sxy) / (cx[x] / sx) / (cx[y] / sx)**.75), 0)) # PMIα \n",
    "    pmi_samples[(x, y)] = data[-1]\n",
    "PMI = csc_matrix((data, (rows, cols)))\n",
    "print('%d non-zero elements' % PMI.count_nonzero())\n",
    "print('Sample values\\n', pformat(pmi_samples.most_common(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD\n",
    "The sparse PMI matrix, which has as many rows and columns as there are unique lemmas, is factorized by using Single Value Decomposition. The number of columns is reduced by computing the first *k* factors that best explain the variance in the matrix. We end up with a vector for each word (a row in the new matrix) of length *k*.\n",
    "\n",
    "SVD decomposes a matrix into three smaller matrices that, together, can reconstruct the original matrix. For our purposes, we only need the first of those. The underscores in the command line\n",
    "```python\n",
    "U, _, _ = svds(PMI, k)\n",
    "```\n",
    "represent the other two matrices, which are discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 300\n",
    "U, _, _ = svds(PMI, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing\n",
    "The resulting vectors (the rows in the reduced matrix) are normalized, so that each vector has length 1 (L2 normalization). The `preprocessing.normalize()`, part of the `sklearn` package, does that job. Normalization is necessary for computing cosine similarity in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = preprocessing.normalize(U, norm='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Vectors in Gensim Keyed Vectors Format\n",
    "The Gensim Keyed Vectors format has a header that represents the dimensions of the data set (number of vectors and number of places in each vector). Each subsequent line begins with a key (the word or lemma), followed by the vector values, all separated by spaces. In order to do so, the matrix U is transformed into a Pandas DataFrame, and the lemmas are used as an index to that DataFrame. The Pandas method `to_csv` will now take care of creating the file.\n",
    "When working in Windows, it is necessary to add the option `line_terminator='\\n'`, or else there will be an additional line feed after each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = pd.DataFrame(U, index=x2i.keys())\n",
    "vec = vec.loc[(vec!=0).any(axis=1)]  # remove rows with only zeroes\n",
    "header = f\"{str(len(vec))} {str(len(U[0]))}\\n\"\n",
    "with open(\"output/vec_file.txt\", \"w\", encoding = \"utf8\") as w:\n",
    "    w.write(header)\n",
    "    vec.to_csv(w, sep=\" \", line_terminator=\"\\n\", header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity\n",
    "The similarity between two vectors, representing lemmas, is expressed in cosine similarity. Cosine similarity is the cosine of the angle between two vectors and ranges from 0 (no similarity at all; orthogonal vectors) to 1 (full identity). For normalized vectors cosine similarity may be computed with the dot product of the two vectors. For our purpose, we need to compute the cosine similarity between our target word and all other words in our collection. The dot product of the SVD matrix with the vector of the target word will provide the full collection of cosine similarity values. This set of values, called `dd` is in the same order (using the same indexes) as our original index, so that when `U[i]` (the *i-th* row of the matrix) represents the vector of **kugsig\\[gold\\]N**, `i2x[i]` = **kugsig\\[gold\\]N** and `dd[i]` represents the cosine similarity of **kugsig\\[gold\\]N** with the target word.\n",
    "\n",
    "To find the highest scoring words in our vocabulary we use the `argsort()` function from the `numpy` library. This function will yield the indexes of a `numpy` array from the lowest value to the highest. Since we need the highest values, we use `[::-1]` to go through the array step by step backwards and select the last `k` indexes in the array. Among the highest scoring words will always be the target word itself (which should be represented by the value 1 in `dd`). Therefore, we will retrieve `k = n+1` values, while skipping the target word in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearestwords(word, n=5):\n",
    "    k = n + 1\n",
    "    s = ''\n",
    "    dd = np.dot(U, U[x2i[word]])\n",
    "    for i in np.argsort(dd)[::-1][:k]:        # sort the array by value from low to high. Select the highest k values.\n",
    "        if i2x[i] == word: continue           # omit the target word itself\n",
    "        s += '(%s, %.3lf, %s) ' % (i2x[i], dd[i], cx[i2x[i]])\n",
    "    print('PMI %s, %d\\n %s' % (word, cx[word], s))\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortorder = \" ʾ'[]aāâbcdeēêfgŋhiīîjklmnopqrsṣštṭuūûvwxyz0123456789₀₁₂₃₄₅₆₇₈₉ₓ{}().-/~?!@×|&<>\"\n",
    "# what follows is safety loop to make sure all characters in cx.keys()\n",
    "# are represented in the sortorder (if not, an error will occur)\n",
    "# add additional characters to the end of the sortorder\n",
    "for k in cx.keys(): \n",
    "    for c in k: \n",
    "        if c not in sortorder and c != '\"': \n",
    "            sortorder = sortorder + c\n",
    "word = sorted(cx.keys(), key=lambda w: [sortorder.index(c.casefold()) for c in w]) # use custom sort order\n",
    "interact(nearestwords, word = word, n = (1, 25));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select output by POS\n",
    "\n",
    "The following cells provide an interactive tool for analyzing the word vectors that were produced with PMI and SVD. The output is a Pandas DataFrame with nearby neighbors, that is, words that are represented by vectors that are relatively close to the vector of target word. One may restrict the output by Part of Speech and the output includes links to the [ePSD2](http://oracc.org/epsd2) pages for these words. The links to [ePSD2](http://oracc.org/epsd2) depend on the stable OID numbers (Oracc Identifiers). All Sumerian words have OIDs, but many proper nouns still have temporary Citation Forms and Guide Words and therefore have no OID. Such words/names do have dictionary pages, but those pages must be found by searching in [ePSD2/admin/ur3/qpn](http://oracc.org/epsd2/admin/ur3/qpn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = '<a href=\"http://oracc.org/epsd2/{}\", target=\"_blank\">{}</a>'\n",
    "columns = [\"word\", \"sim.\", \"freq.\"]\n",
    "with open(\"output/x2oid.p\", \"rb\") as r:\n",
    "    x2oid = pickle.load(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearestwordsselect(words, POSfilter = [], n=5):\n",
    "    n = n+len(words)\n",
    "    l = []\n",
    "    vecs = [U[x2i[word]] for word in words]\n",
    "    vecm = np.mean(vecs, axis = 0)\n",
    "    dd = np.dot(U, vecm)\n",
    "    ds = np.argsort(dd)           # sort the indexes of the dd array\n",
    "    if POSfilter: \n",
    "        F = [i for i in i2x.keys() if i2x[i].split(']')[1] in POSfilter]\n",
    "        ds = ds[np.isin(ds, F)]       # select the indexes that appear in F\n",
    "    #if len(words) == 1: \n",
    "    #    ds = ds[ds != x2i[words[0]]]      # omit target word\n",
    "    for i in ds[::-1][:n]:        # Select the highest n values\n",
    "        e = [i2x[i], dd[i], cx[i2x[i]]]\n",
    "        if x2oid.get(e[0]): \n",
    "            e[0] = anchor.format(x2oid.get(e[0]), e[0])\n",
    "        l.append(e)\n",
    "    df = pd.DataFrame(l, columns=columns)\n",
    "    print(words)\n",
    "    return df.style.format({\"sim.\" : '{0:,.3f}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS = ['N', 'V/i', 'V/t', 'AJ', 'PN', 'DN', 'SN', 'WN', 'FN', 'ON', 'TN', 'NU']\n",
    "interact(nearestwordsselect, \n",
    "         words = widgets.SelectMultiple(options = word, \n",
    "                                  description = \"Word\", \n",
    "                                  value = [word[0]]),\n",
    "         POSfilter=widgets.SelectMultiple(options = POS, \n",
    "                                  value = [], \n",
    "                                  description = \"POS\"), \n",
    "         n=(1,25)\n",
    "                                      );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_N = [w for w in word if w.split(']')[1] == 'N']  #select nouns only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearestwordsselect2(word, POSfilter = [], n=10):\n",
    "    n = n+1\n",
    "    l = []\n",
    "    vec = U[x2i[word]]\n",
    "    dd = np.dot(U, vec)\n",
    "    ds = np.argsort(dd)           # sort the indexes of the dd array\n",
    "    if POSfilter: \n",
    "        F = [i for i in i2x.keys() if i2x[i].split(']')[1] in POSfilter]\n",
    "        ds = ds[np.isin(ds, F)]       # select the indexes that appear in F\n",
    "    #if len(words) == 1: \n",
    "    #    ds = ds[ds != x2i[words[0]]]      # omit target word\n",
    "    for i in ds[::-1][:n]:        # Select the highest n values\n",
    "        if word == i2x[i]: \n",
    "            continue\n",
    "        e = [word, i2x[i], dd[i], cx[i2x[i]]]\n",
    "        l.append(e)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for noun in word_N: \n",
    "    closest_l = nearestwordsselect2(noun, ['N'], 15)\n",
    "    closest_l = [w for w in closest_l if w[2] > .9] # that is a very high bar\n",
    "    all_words.extend(closest_l)\n",
    "allwords_df = pd.DataFrame(all_words, columns = ['Source', 'Target', 'weight', 'target_freq'])\n",
    "allwords_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use allwords_df in creating a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allwords_df.to_csv('output/allwords.csv', sep=',', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
