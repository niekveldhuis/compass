{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition from CDLI\n",
    "The downloadable [CDLI](http://cdli.ucla.edu) files are found on the download page http://cdli.ucla.edu/bulk_data. The data available are a set of transliterations and a catalog file with meta-data. Because of its size the catalog file is currently split in two, it is possible that in the future there will be either more or fewer such files. The script identifies the file names and downloads those to a directory `cdlidata`. Once downloaded the catalog is reconstituted as a single file and is loaded into a `pandas` DataFrame. The DataFrame is used, by way of example, to select the transliterations from the Early Dynastic IIIa period.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm_notebook\n",
    "import errno\n",
    "import pandas as pd\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import sys\n",
    "util_dir = os.path.abspath('../utils')\n",
    "sys.path.append(util_dir)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create Download Directory\n",
    "Create a directory called `cdlidata`. If the directory already exists, do nothing. The directory is created with the function `make_dirs()` from the `utils` module. For the code see section 2.1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = ['cdlidata']\n",
    "make_dirs(directories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Retrieve File Names\n",
    "We first need to retrieve the names of the files that are offered for download on the CDLI [download](http://cdli.ucla.edu/bulk_data/) page. The script requests the HTML of the download page and uses BeautifulSoup (a package for web scraping) to  retrieve all the links from the page. This includes the file names, but also links used to order the files in different ways (by name, by size, or by date). These latter links, which all start with \"?\", are filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_page = \"http://cdli.ucla.edu/bulk_data/\"\n",
    "r = requests.get(download_page)\n",
    "html = r.text\n",
    "soup = BeautifulSoup(html)\n",
    "links = soup.find_all('a')       # retrieve all html anchors, which define links\n",
    "files = set()\n",
    "for link in links:\n",
    "    f = link.get('href')        # from the anchors, retrieve the URLs\n",
    "    files.add(f)\n",
    "files = {f for f in files if not f[0] == \"?\"}  # filter out URLs that start with \"?\"\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Download\n",
    "The download code in this cell is essentially identical with the code in notebook 2_1_0_download_ORACC-JSON.ipynb. Depending on the speed of your computer and internet connection the downloading process can take some time because of the size of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK = 16 * 1024\n",
    "for f in files:\n",
    "    url = download_page + f\n",
    "    target = 'cdlidata/' + f\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        if r.status_code == 200:\n",
    "            print(\"Downloading \" + url + \" saving as \" + target)\n",
    "            with open(target, 'wb') as t:\n",
    "                for c in tqdm_notebook(r.iter_content(chunk_size=CHUNK)):\n",
    "                    t.write(c)\n",
    "        else:\n",
    "            print(url + \" does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Concatenate the Catalogue Files\n",
    "The catalogue files are concatenated by reading them line by line into the file `catalogue.csv` which is placed in the directory `cdlidata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [f for f in files if \"cdli_catalogue\" in f]\n",
    "filenames.sort()  # to make sure we read cdli_catalogue_1of2.csv first.\n",
    "with open('cdlidata/catalogue.csv', 'w', encoding=\"utf8\") as outfile:\n",
    "    for fname in tqdm_notebook(filenames):\n",
    "        with open('cdlidata/' + fname, encoding=\"utf8\") as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Load in Pandas DataFrame\n",
    "## 5.1 Adjust Shape of the File\n",
    "At the time of writing this notebook, the file cdli_catalogue_1of2.csv had at least one instance of a a double record in a single line, resulting in more than the standard 63 fields. This prevents `pandas` from loading the file directly with the `from_csv()` function. For that reason the file is read with the `csv` library; each line is placed in the list `catalogue` but all columns higher than 63 are discarded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue = []\n",
    "with open('cdlidata/catalogue.csv', 'r', encoding=\"utf8\") as f:\n",
    "    csv_reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "    for row in csv_reader:\n",
    "        catalogue.append(row[:63])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 DataFrame\n",
    "The list `catalogue` is now loaded into `pandas`. The first row (row 0) is holding the column names. This row is used to rename the columns and then discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = pd.DataFrame(catalogue)\n",
    "cat.columns = cat.iloc[0]\n",
    "cat = cat[1:]\n",
    "cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Use Catalog to Select Transliterations\n",
    "In the example code in the following cell the catalog is used to select from the transliteration file all texts from the Early Dynastic IIIa period. The field \"period\" is used to select those catalog entries that have \"ED IIIa\" in that field. P numbers are stored in the catalog without the initial 'P' and without leading zeros (that is '1183' corresponds to 'P001183'). The function `zfill()` is used to created a 6-digit number with leading zeros, if necessary. The P-numbers of our catalog selection are stored in the variable `pnos` (but note that the numbers do not have the initial 'P'!).\n",
    "\n",
    "The code then iterates through the list of lines. The flag `keep` (which initially is set to `FALSE`) is set to `TRUE` if the code encounters a P number that is present in the list `pnos`. As long as `keep = TRUE` subsequent lines are added to the list `ed3a_atf`. When the script encounters a P-number that is not in `pnos`, the flag `keep` is set to `FALSE`.\n",
    "\n",
    "The result is a list lines with all the transliteration data of the Early Dynastic IIIa texts in [CDLI](http://cdli.ucla.edu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed3a = cat.loc[cat[\"period\"].str[:7] == \"ED IIIa\"]\n",
    "pnos = list(ed3a[\"id_text\"].str.zfill(6))\n",
    "with open(\"cdlidata/cdliatf_unblocked.atf\", encoding=\"utf8\") as c: \n",
    "    lines = c.readlines()\n",
    "keep = False\n",
    "ed3a_atf = []\n",
    "for line in tqdm_notebook(lines):\n",
    "    if line[0] == \"&\": \n",
    "        if line[2:8] in pnos: \n",
    "            keep = True\n",
    "        else: \n",
    "            keep = False\n",
    "    if keep: \n",
    "        ed3a_atf.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Place in DataFrame\n",
    "Place the ED IIIa texts in a DataFrame, where each row represents one document (line numbers are omitted). This is, of course, just one example of how the data may be selected and formatted.\n",
    "\n",
    "The lines are read in reverse order, so that when the script encounters an '&P' line (as in '&P212416 = AAICAB 1/1, pl. 008, 19282-439'), this signals that all the lines of a text have been read and that the document can be added to the list `docs`. (When reading the lines in regular order - taking the '&P' line as signaling the end of the previous document - one needs to separately save the last document, because there is no '&P' line anymore to indicate that the text is complete)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "d = ''\n",
    "id_text = ''\n",
    "ed3a_atf = [line for line in ed3a_atf if line.strip()]  # remove empty lines, which cause trouble\n",
    "for line in tqdm_notebook(reversed(ed3a_atf)):\n",
    "    if line[0] == \"&\":  # line beginning with & marks the beginning of a document\n",
    "        id_text = line[1:8] # retrieve the P number\n",
    "        docs.append([id_text, d])\n",
    "        d = ''   # after appending the data to docs, reset d for a new document.\n",
    "        continue\n",
    "    elif line [0] in [\"#\", \"$\", \"<\", \">\", \"@\"]:  # skip all non-transliteration lines\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            line = line.split(' ', 1)[1].strip() # split line at first space (after the line number)\n",
    "            d = line + ' ' + d # add the new line in front\n",
    "        except:\n",
    "            continue   # malformed lines (no proper separation between line number and text) are skipped\n",
    "ed3a_df = pd.DataFrame(docs)\n",
    "ed3a_df.columns = [\"id_text\", \"transliteration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed3a_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
